{
  "personality": null,
  "timestamp": "2025-10-23T04:39:00.116914",
  "category": "Technology",
  "news_summary": "Today's technology news highlights breakthroughs in sustainable energy harnessing, advanced reinforcement learning algorithms, novel insights into gravity-induced entanglement, and cutting-edge quantum optimization techniques.",
  "news_summary_fr": "L'actualité technologique d'aujourd'hui met en lumière des percées dans le domaine de l'exploitation durable de l'énergie, des algorithmes avancés d'apprentissage par renforcement, de nouvelles connaissances sur l'enchevêtrement induit par la gravité et des techniques d'optimisation quantique de pointe.",
  "news_summary_es": "Las noticias tecnológicas de hoy destacan avances en el aprovechamiento sostenible de la energía, algoritmos avanzados de aprendizaje por refuerzo, nuevos conocimientos sobre el entrelazamiento inducido por la gravedad y técnicas punteras de optimización cuántica.",
  "articles": [
    {
      "title": "How algae learned to harness the Sun without getting burned",
      "summary": "Under the sea, green algae have evolved a clever way to handle too much sunlight. Scientists found that a special pigment called siphonein acts like a natural sun shield, protecting the algae’s delicate photosynthetic machinery from burning out. Using advanced imaging and simulations, researchers showed how siphonein helps algae safely manage excess light energy. The discovery could inspire new solar technologies that mimic nature’s built-in protection systems.",
      "content": "A day of strong sunlight can spoil more than just a beach outing -- it can also harm the process of photosynthesis, the way plants and other organisms convert sunlight into energy. Underwater, however, certain algae have evolved a unique way to stay protected. Researchers from Osaka Metropolitan University and their collaborators discovered that a pigment known as siphonein helps marine green algae continue photosynthesizing efficiently, even under intense light.\n\nProtecting the Machinery of Photosynthesis\n\nPhotosynthetic organisms use complex molecular systems called light-harvesting complexes (LHCs) to absorb sunlight and convert it into usable energy. When chlorophyll, the green pigment central to photosynthesis, absorbs light, it becomes excited and passes that energy to reaction centers that fuel chemical processes. Under too much light, though, chlorophyll can enter a dangerous \"triplet\" state, producing reactive oxygen molecules that can damage cells.\n\n\"Organisms use carotenoids to quickly dissipate excess energy, or quench these triplet states, through a process called triplet-triplet energy transfer (TTET),\" said Ritsuko Fujii, lead author and associate professor at the Graduate School of Science and Research Center for Artificial Photosynthesis at Osaka Metropolitan University.\n\nUntil recently, the exact details of how this protective process works were not well understood.\n\nA Closer Look at Codium fragile\n\nTo investigate, the research team turned to Codium fragile, a type of marine green alga. Like land plants, it has a light-harvesting antenna complex called LHCII, but it also contains rare carotenoids such as siphonein and siphonaxanthin. These pigments allow the algae to use green light -- common in underwater environments -- for photosynthesis.\n\n\"The key to the quenching mechanism lies in how quickly and efficiently the triplet states can be deactivated,\" said Alessandro Agostini, researcher at the University of Padua, Italy and co-lead author of the study.\n\nThe researchers used electron paramagnetic resonance (EPR) spectroscopy, a technique that directly measures triplet excited states, to compare spinach with Codium fragile. In spinach, traces of harmful chlorophyll triplet states remained. But in Codium fragile, those signals disappeared entirely, showing that its carotenoids successfully neutralize the damaging energy.\n\n\"Our research has revealed that the antenna structure of photosynthetic green algae has an excellent photoprotective function,\" Agostini said.\n\nHow Siphonein Shields Algae From Sun Damage\n\nBy combining EPR data with quantum chemical simulations, the researchers identified siphonein, located at a critical binding site in the LHCII complex, as the key pigment responsible for this defense. They also revealed how its molecular structure and positioning make it especially effective at dispersing excess energy.\n\nThese findings show that marine algae have evolved specialized pigments not only to absorb the blue-green light available underwater but also to withstand the damaging effects of intense sunlight.\n\nFrom Ocean Discovery to Solar Innovation\n\nBeyond improving our understanding of photosynthesis, this research could influence the design of bio-inspired solar technologies that protect themselves from light damage. Such systems might lead to more durable and efficient renewable energy solutions.\n\n\"We hope to further clarify the structural characteristics of carotenoids that increase quenching efficiency, ultimately enabling the molecular design of pigments that optimize photosynthetic antennae,\" Fujii said.\n\nThe study was published in Cell Reports Physical Science.",
      "url": "https://www.sciencedaily.com/releases/2025/10/251022023110.htm",
      "source": "Latest Science News -- ScienceDaily",
      "published": "2025-10-22",
      "sentiment_score": 0.85,
      "reasoning": "The article reports a scientific discovery about how marine green algae protect themselves from sunlight damage using a special pigment. This finding has broad significance as it advances understanding of photosynthesis and has potential real-world applications in developing more durable and efficient solar energy technologies, benefiting society and the environment. The article provides detailed context about the discovery, the methods used, and its implications, fulfilling criteria for inspiring good news.",
      "category": "Technology",
      "personality_title": "Scientists uncover how algae protect themselves from sunlight damage",
      "personality_presentation": "**Context** – Photosynthesis is how plants and algae use sunlight to make energy. But too much sunlight can harm this process by creating damaging molecules. Underwater algae face strong sunlight too, and scientists wanted to understand how they avoid damage.\n\n**What happened** – Researchers studied a green alga called Codium fragile. They found it uses a special pigment named siphonein to protect its photosynthesis system. Using advanced tools like electron paramagnetic resonance (EPR) and computer simulations, they showed siphonein safely removes harmful energy caused by intense light.\n\n**Impact** – This discovery explains how marine algae survive strong sunlight without damage. It also reveals a natural “sun shield” that could inspire better solar panels or energy devices that protect themselves from light damage, making them last longer and work more efficiently.\n\n**What's next step** – Scientists plan to study more about how pigments like siphonein work at the molecular level. This could help design new materials for solar energy that copy algae’s natural protection methods.\n\n**One-sentence takeaway** – Marine algae use a special pigment to safely handle strong sunlight, offering clues for improving solar energy technology.",
      "personality_title_fr": "Des scientifiques découvrent comment les algues se protègent des dommages causés par le soleil",
      "personality_presentation_fr": "**Contexte** – La photosynthèse est le processus par lequel les plantes et les algues utilisent la lumière du soleil pour produire de l'énergie. Mais trop de lumière peut endommager ce processus en créant des molécules nocives. Les algues sous-marines sont aussi exposées à une forte lumière, et les scientifiques voulaient comprendre comment elles évitent ces dégâts.\n\n**Ce qui s'est passé** – Des chercheurs ont étudié une algue verte appelée Codium fragile. Ils ont découvert qu’elle utilise un pigment spécial nommé siphoneïne pour protéger son système de photosynthèse. Grâce à des outils avancés comme la résonance paramagnétique électronique (EPR) et des simulations informatiques, ils ont montré que la siphoneïne élimine en toute sécurité l’énergie nocive causée par une lumière intense.\n\n**Impact** – Cette découverte explique comment les algues marines survivent à une forte lumière solaire sans dommage. Elle révèle aussi un « bouclier solaire » naturel qui pourrait inspirer des panneaux solaires ou des dispositifs énergétiques plus résistants et efficaces.\n\n**Prochaine étape** – Les scientifiques comptent étudier plus en détail le fonctionnement moléculaire des pigments comme la siphoneïne. Cela pourrait aider à concevoir de nouveaux matériaux pour l’énergie solaire qui imitent la protection naturelle des algues.\n\n**Phrase clé** – Les algues marines utilisent un pigment spécial pour gérer en toute sécurité une forte lumière solaire, offrant des pistes pour améliorer la technologie solaire.",
      "personality_title_es": "Científicos descubren cómo las algas se protegen del daño solar",
      "personality_presentation_es": "**Contexto** – La fotosíntesis es el proceso mediante el cual las plantas y algas usan la luz del sol para crear energía. Pero demasiada luz puede dañar este proceso al crear moléculas dañinas. Las algas submarinas también reciben mucha luz, y los científicos querían entender cómo evitan el daño.\n\n**Qué pasó** – Investigadores estudiaron una alga verde llamada Codium fragile. Encontraron que usa un pigmento especial llamado sifoneína para proteger su sistema de fotosíntesis. Usando herramientas avanzadas como la resonancia paramagnética electrónica (EPR) y simulaciones por computadora, mostraron que la sifoneína elimina con seguridad la energía dañina causada por la luz intensa.\n\n**Impacto** – Este descubrimiento explica cómo las algas marinas sobreviven a la luz solar fuerte sin dañarse. También revela un “escudo solar” natural que podría inspirar mejores paneles solares o dispositivos energéticos que se protejan del daño por luz, haciéndolos más duraderos y eficientes.\n\n**Próximo paso** – Los científicos planean estudiar más sobre cómo funcionan los pigmentos como la sifoneína a nivel molecular. Esto podría ayudar a diseñar nuevos materiales para energía solar que imiten la protección natural de las algas.\n\n**Frase clave** – Las algas marinas usan un pigmento especial para manejar de forma segura la luz solar intensa, ofreciendo ideas para mejorar la tecnología solar.",
      "image_url": "public/images/news_image_How-algae-learned-to-harness-the-Sun-without-getti.png",
      "image_prompt": "A detailed, warm-toned painting of vibrant marine green algae underwater, with glowing, intricately structured light-harvesting antenna complexes depicted as delicate, translucent networks infused with soft green and golden hues, surrounded by abstract swirling energy flows symbolizing sunlight being safely absorbed and diffused by rare carotenoid pigments like siphonein, all rendered in natural, simple colors to evoke a serene, protective underwater environment."
    },
    {
      "title": "Discovering state-of-the-art reinforcement learning algorithms",
      "summary": "Nature, Published online: 22 October 2025; doi:10.1038/s41586-025-09761-xDiscovering state-of-the-art reinforcement learning algorithms",
      "content": "Humans and other animals use powerful reinforcement learning (RL) mechanisms that have been discovered by evolution over many generations of trial and error. By contrast, artificial agents typically learn using hand-crafted learning rules. Despite decades of interest, the goal of autonomously discovering powerful RL algorithms has proven elusive7-12. In this work, we show that it is possible for machines to discover a state-of-the-art RL rule that outperforms manually-designed rules. This was achieved by meta-learning from the cumulative experiences of a population of agents across a large number of complex environments. Specifically, our method discovers the RL rule by which the agent's policy and predictions are updated. In our large-scale experiments, the discovered rule surpassed all existing rules on the well-established Atari benchmark and outperformed a number of state-of-the-art RL algorithms on challenging benchmarks that it had not seen during discovery. Our findings suggest that the RL algorithms required for advanced artificial intelligence may soon be automatically discovered from the experiences of agents, rather than manually designed.",
      "url": "https://www.nature.com/articles/s41586-025-09761-x",
      "source": "Nature",
      "published": "2025-10-23",
      "sentiment_score": 0.85,
      "reasoning": "The article reports a significant breakthrough in artificial intelligence where machines autonomously discover state-of-the-art reinforcement learning algorithms that outperform manually designed ones. This advancement has broad implications for AI development and could accelerate progress in many fields reliant on machine learning, representing a meaningful and substantive technological achievement.",
      "category": "Technology",
      "personality_title": "Machines learn to create better learning rules on their own",
      "personality_presentation": "**Context** – Reinforcement learning is a way that both humans and animals learn by trying things and learning from success or failure. So far, computer programs that use reinforcement learning have relied on rules created by people.\n\n**What happened** – Scientists have now developed a method that lets machines discover their own reinforcement learning rules without human help. They taught a group of computer agents to learn from many different tasks and environments. The new rules they found worked better than all existing rules, including those designed by experts, especially on popular tests like Atari games.\n\n**Impact** – This discovery is important because it shows machines can improve how they learn by themselves. It means future artificial intelligence could develop smarter ways to learn faster and handle new problems better, without needing humans to write new instructions.\n\n**What's next step** – Researchers will likely test these new learning rules in other areas beyond games, such as robotics or language understanding. This method might help create more advanced AI systems that adapt quickly to different challenges.\n\n**One-sentence takeaway** – Machines have now invented their own advanced learning rules, which work better than those made by humans and could speed up AI progress.",
      "personality_title_fr": "Des machines apprennent à créer leurs propres règles d’apprentissage meilleures",
      "personality_presentation_fr": "**Contexte** – L’apprentissage par renforcement est une méthode utilisée par les humains et les animaux pour apprendre en essayant et en tirant des leçons de leurs succès ou échecs. Jusqu’à présent, les programmes informatiques utilisaient des règles créées par des humains.\n\n**Ce qui s’est passé** – Des scientifiques ont développé une méthode qui permet aux machines de découvrir elles-mêmes des règles d’apprentissage par renforcement, sans aide humaine. Ils ont entraîné un groupe d’agents informatiques à apprendre à partir de nombreuses tâches et environnements différents. Les nouvelles règles découvertes ont mieux fonctionné que toutes les règles existantes, y compris celles conçues par des experts, notamment dans des tests populaires comme les jeux Atari.\n\n**Impact** – Cette découverte est importante car elle montre que les machines peuvent améliorer leur façon d’apprendre toutes seules. Cela signifie que l’intelligence artificielle future pourrait apprendre plus vite et mieux s’adapter à de nouveaux problèmes, sans que les humains aient besoin d’écrire de nouvelles instructions.\n\n**Prochaine étape** – Les chercheurs vont probablement tester ces nouvelles règles d’apprentissage dans d’autres domaines, comme la robotique ou la compréhension du langage. Cette méthode pourrait aider à créer des systèmes d’IA plus avancés, capables de s’adapter rapidement à différents défis.\n\n**Conclusion en une phrase** – Les machines ont maintenant inventé leurs propres règles d’apprentissage avancées, qui fonctionnent mieux que celles créées par les humains et pourraient accélérer les progrès de l’IA.",
      "personality_title_es": "Las máquinas aprenden a crear mejores reglas de aprendizaje por sí solas",
      "personality_presentation_es": "**Contexto** – El aprendizaje por refuerzo es una forma en que los humanos y animales aprenden probando y aprendiendo de sus aciertos y errores. Hasta ahora, los programas de computadora usaban reglas hechas por personas.\n\n**Qué pasó** – Científicos desarrollaron un método que permite a las máquinas descubrir sus propias reglas de aprendizaje por refuerzo sin ayuda humana. Entrenaron a un grupo de agentes informáticos para aprender de muchas tareas y entornos diferentes. Las nuevas reglas descubiertas funcionaron mejor que todas las existentes, incluyendo las diseñadas por expertos, especialmente en pruebas populares como juegos de Atari.\n\n**Impacto** – Este hallazgo es importante porque muestra que las máquinas pueden mejorar cómo aprenden por sí mismas. Esto significa que la inteligencia artificial futura podría aprender más rápido y adaptarse mejor a nuevos problemas sin que los humanos tengan que crear nuevas instrucciones.\n\n**Próximo paso** – Los investigadores probablemente probarán estas nuevas reglas de aprendizaje en otras áreas, como la robótica o la comprensión del lenguaje. Este método podría ayudar a crear sistemas de IA más avanzados que se adapten rápidamente a diferentes desafíos.\n\n**Conclusión en una frase** – Las máquinas han inventado sus propias reglas avanzadas de aprendizaje, que funcionan mejor que las hechas por humanos y podrían acelerar el progreso de la IA.",
      "image_url": "public/images/news_image_Discovering-state-of-the-art-reinforcement-learnin.png",
      "image_prompt": "A warm, detailed painting of a diverse group of abstract animal silhouettes and geometric robotic figures gathered around a glowing, evolving neural network pattern that radiates light, symbolizing the discovery of advanced reinforcement learning algorithms through shared experience and trial, rendered in soft, natural earth tones and gentle blues."
    },
    {
      "title": "Classical theories of gravity produce entanglement",
      "summary": "Nature, Published online: 22 October 2025; doi:10.1038/s41586-025-09595-7By applying the full framework of quantum field theory, it is shown that local classical theories of gravity can transmit quantum information and, thus, generate entanglement through physical, local processes.",
      "content": "To illustrate this further, we now demonstrate how equation (4) leads to entanglement in a version of Feynman’s experiment. Two spherical mass distributions, each with total mass M and radius R, are prepared in a quantum superposition of two locations. This could be achieved by, for example, implementing matter-wave beam splitters6, manipulating potentials32 or exploiting internal degrees of freedom, such as quantum spins in Stern–Gerlach experiments5 (Fig. 3). Gravity is assumed to be the only interaction between the particles, and in the non-relativistic limit and describing matter within first quantization, it just acts as a quantum phase φ ij ≔ GM2t/(ħ d ij ) on each superposition branch5,6, where d ij is the distance between the matter distributions in the branch labelled by i, j ∈ {L, R}, and GM2/d ij is the Newtonian potential energy. With the superposition size Δx much greater than the smallest distance d RL , only the quantum phase φ ≔ φ RL is significant, so that the systems are clearly entangled, with the entanglement depending solely on φ (refs. 5,6). By contrast, when Δx ≪ d RL , the relevant parameter for entanglement becomes essentially \\(\\overline{\\varphi }:= \\varphi \\,\\Delta {x}^{2}/{d}_{{\\rm{RL}}}^{2}\\) (ref. 33). To measure the entanglement, the superposed paths could be recombined and correlations sought between the interferometer outputs6 or internal degrees of freedom5.\n\nFig. 3: Visualization of a version of Feynman’s experiment. Two spherical mass distributions (1 and 2) of radius R are placed in quantum superpositions at two locations as N00N states, with blue and red denoting the components separated by Δx. After gravitationally interacting for a short time, the paths are recombined and entanglement is sought5,6. Although Stern–Gerlach interferometry with internal spins is illustrated5, alternative set-ups, such as parallel Mach–Zehnders, are also possible6. Here, Δx is depicted larger than the minimum separation d RL , but a general configuration can be implemented, including Δx ≪ d RL . Full size image\n\nQuantum gravity\n\nWe now analyse this experiment using perturbative quantum gravity with a QFT description of matter. With electromagnetic interactions ignored, we take the initial state of the objects immediately after being placed in a quantum superposition as a product of N00N states:\n\n$$\\begin{array}{l}|\\varPsi \\rangle =\\frac{1}{2}({|N\\rangle }_{{\\rm{1L}}}{|0\\rangle }_{{\\rm{1R}}}{|\\uparrow \\rangle }_{1}+{|0\\rangle }_{{\\rm{1L}}}{|N\\rangle }_{{\\rm{1R}}}{|\\downarrow \\rangle }_{1})\\\\ \\qquad \\otimes ({|N\\rangle }_{{\\rm{2L}}}{|0\\rangle }_{{\\rm{2R}}}{|\\uparrow \\rangle }_{2}+{|0\\rangle }_{{\\rm{2L}}}{|N\\rangle }_{{\\rm{2R}}}{|\\downarrow \\rangle }_{2}),\\end{array}$$ (5)\n\nwhere |N⟩ κi is a product of N independent position states of matter particles obeying a complex scalar field34, with κ ∈ {1, 2} and i ∈ {L, R} labelling the position of the spherical objects (matching Fig. 3). N is the number of particles in the objects, such that M = mN, with m the mass of the particles. We also include possible internal spin states {|↑⟩, |↓⟩}, which could be used to generate the quantum superpositions.\n\nAfter a time t, the state of the matter system in the Schrödinger picture is\n\n$$| \\varPsi (t)\\rangle ={{\\rm{e}}}^{{\\rm{i}}{\\widehat{H}}_{0}t/\\hbar }\\widehat{T}{{\\rm{e}}}^{-({\\rm{i}}/\\hbar ){\\int }_{0}^{t}{\\rm{d}}\\tau {\\widehat{H}}_{{\\rm{I}}}(\\tau )}| \\varPsi \\rangle ,$$ (6)\n\nwhere \\(\\widehat{T}\\) is the time-ordering operator, τ is a dummy time variable, \\({\\widehat{H}}_{0}\\) is the Hamiltonian of perturbative quantum gravity in the absence of matter–gravity interactions and \\({\\widehat{H}}_{{\\rm{I}}}:= \\exp ({\\rm{i}}{\\widehat{H}}_{0}t/\\hbar ){\\widehat{H}}_{{\\rm{int}}}\\,\\exp (-{\\rm{i}}{\\widehat{H}}_{0}t/\\hbar )\\). Just before the superposition paths are bought back together, for example through reverse Stern–Gerlachs1,5, we can write\n\n$$\\begin{array}{l}|\\,\\varPsi (t)\\rangle \\propto {\\alpha }_{{\\rm{LL}}}{|N\\rangle }_{{\\rm{1L}}}{|N\\rangle }_{{\\rm{2L}}}+{\\alpha }_{{\\rm{LR}}}{|N\\rangle }_{{\\rm{1L}}}{|N\\rangle }_{{\\rm{2R}}}\\\\ \\qquad \\,+\\,{\\alpha }_{{\\rm{RL}}}{|N\\rangle }_{{\\rm{1R}}}{|N\\rangle }_{{\\rm{2L}}}+{\\alpha }_{{\\rm{RR}}}{|N\\rangle }_{{\\rm{1R}}}{|N\\rangle }_{{\\rm{2R}}},\\end{array}$$ (7)\n\nwhere \\({\\alpha }_{ij}\\in {\\mathbb{C}}\\). We have now neglected any internal spin states and the vacuum states for simplicity. We can calculate the amplitudes α ij by taking the inner product of equation (7) (and also equation (6)), with the basis states |N⟩ 1i |N⟩ 2j and expanding the time-ordered unitary operation in equation (6) as the Dyson series26,35. The amplitudes can then be written as a perturbative series with each term corresponding to that of the Dyson series: \\({\\alpha }_{ij}={\\alpha }_{ij}^{(0)}+{\\alpha }_{ij}^{(1)}+{\\alpha }_{ij}^{(2)}+\\cdots \\,\\). The first process where a virtual graviton is exchanged between the matter objects occurs at second order in the series and corresponds to the Feynman diagram in Fig. 1 (top left). The amplitude for this Feynman diagram, within the very good approximation ct ≫ d ij (Methods), is\n\n$$\\frac{G{M}^{2}t}{\\hbar {V}^{2}}\\iint {{\\rm{d}}}^{3}{\\bf{x}}\\,{{\\rm{d}}}^{3}{\\bf{y}}\\frac{{\\theta }_{1i}({\\bf{x}}){\\theta }_{2j}({\\bf{y}})}{| {\\bf{x}}-{\\bf{y}}| }\\equiv {\\varphi }_{ij},$$ (8)\n\nwhere θ κi (x) ≔ θ(R − ∣x − X κi ∣) is the unit-step function defining the spherical shape of the matter distribution κ in branch i, X κi is the coordinate for the centre of mass for the distributions and V ≔ 4πR3/3. The above amplitude directly contributes to \\({\\alpha }_{ij}^{(2)}\\) such that, when Δx ≫ d RL , the \\({\\alpha }_{{\\rm{RL}}}^{(2)}\\) amplitude dominates over all others and equals iφ. Then, given that \\({\\alpha }_{ij}^{(0)}=1\\) from the Dyson series and that \\({\\alpha }_{ij}^{(1)}\\) contains no contractions for the gravitational field, we are left with α ij ≈ 1 except for α RL ≈ 1 + iφ, which matches the first-quantized result to first order in the quantum phase \\(\\exp ({\\rm{i}}\\varphi )\\). The full non-perturbative result is straightforwardly obtained by considering the form of the corresponding higher-order Feynman diagrams and extrapolating the result26.\n\nClassical gravity\n\nWe now consider the above experiment within the context of classical gravity. The calculation follows the above but with the interaction Hamiltonian of equation (4) rather than equation (2). At second order in the Dyson series there are no non-vanishing Wick contractions corresponding to Feynman diagrams that contain quantum communication between the matter objects, and the diagram responsible for entanglement in quantum gravity, Fig. 1 (top left), becomes Fig. 2 (top middle). This diagram represents the two matter objects sitting in their combined classical gravitational field, with the amplitude just contributing a local relative quantum phase between the branches of each matter object, which does not lead to entanglement13. However, at fourth order in the series, a diagram appears where the matter distributions are connected quantum mechanically through virtual matter particles (Fig. 2, top right). Within the same approximations as in the quantum gravity case, the amplitude for the Feynman diagram is (Methods)\n\n$${{\\vartheta }}_{ij}:= \\frac{{m}^{6}{t}^{2}{N}^{2}}{4{{\\rm{\\pi }}}^{2}{\\hbar }^{6}{V}^{2}}{\\left(i\\int {{\\rm{d}}}^{3}{\\bf{x}}{{\\rm{d}}}^{3}{\\bf{y}}\\frac{\\varPhi ({\\bf{x}})\\varPhi ({\\bf{y}}){\\theta }_{1i}({\\bf{x}}){\\theta }_{2j}({\\bf{y}})}{| {\\bf{x}}-{\\bf{y}}| }\\right)}^{2},$$ (9)\n\nwhere Φ(x) ≔ −c2h00(x)/2 is the total gravitational potential of the matter objects, and ϑ ij directly contributes to \\({\\alpha }_{ij}^{(4)}\\) in equation (7). As we have a classical theory of gravity, Φ(x) is the same in each superposition branch, otherwise the Newtonian force would be in a quantum superposition. Certain works have considered gravity to be classical but still allow the field or Newtonian force to be in a quantum superposition36,37,38,39. Here, we keep to the notion that quantum superposition is a purely quantum-mechanical phenomena such that Φ(x) is not superposed in equation (9) and is not a quantum operator. However, despite this, equation (9) generically results in entanglement, as θ 1i (x) and θ 2i (x) are different for the different superposition paths and are connected through ∣x − y∣, such that \\({\\alpha }_{ij}^{(4)}\\) is different for each superposition path, except for \\({\\alpha }_{{\\rm{LL}}}^{(4)}={\\alpha }_{{\\rm{RR}}}^{(4)}\\) from symmetry. We can understand this from the Feynman diagram in Fig. 2 (top right), where, unlike the gravitational potential, the virtual matter particles enter into a quantum superposition with the different mass states and the distance the virtual matter particles propagate is different in each branch, resulting in the spatial integrals in equation (9) being connected through ∣x − y∣.\n\nAs Φ(x) comes from a superposition of matter in equation (9), we must consider exactly how gravity is sourced by quantum matter in a fundamental theory of classical gravity. In the approach most considered40,41, Φ(x) in equation (9) is the sum of the average potentials of the superposition states of the two objects. In this case, ϑ ij is inversely proportional to d ij such that, if Δx ≫ d RL , the RL amplitude dominates over all others and is ∣ϑ RL ∣ ≈ ϑ, where (Methods)\n\n$$\\sqrt{{\\vartheta }}=\\frac{6}{25}\\frac{{G}^{2}{m}^{2}{M}^{3}Rt}{{\\hbar }^{3}{d}_{{\\rm{RL}}}}.$$ (10)",
      "url": "https://www.nature.com/articles/s41586-025-09595-7",
      "source": "Nature",
      "published": "2025-10-23",
      "sentiment_score": 0.85,
      "reasoning": "The article reports a significant theoretical breakthrough showing that classical gravity theories can produce quantum entanglement, implying gravity can transmit quantum information. This is a major advancement in fundamental physics with broad implications for quantum gravity research, quantum information science, and our understanding of the universe. The detailed explanation and experimental proposals provide substantive context and demonstrate meaningful scientific progress.",
      "category": "Technology",
      "personality_title": "New study shows classical gravity can create quantum entanglement",
      "personality_presentation": "**Context** – Scientists have long wondered how gravity fits with the strange rules of quantum physics. Usually, gravity is seen as a classical force, while quantum physics deals with tiny particles and their weird behaviors, like entanglement, where particles become connected even when far apart.\n\n**What happened** – Researchers used advanced math from quantum field theory to study a famous thought experiment involving two small spherical masses placed in special quantum states called superpositions. They found that even if gravity is classical (not quantum), it can still cause these masses to become entangled by exchanging quantum information through local physical processes. This means gravity can link particles in a quantum way without itself being quantum.\n\n**Impact** – This is important because it challenges the belief that only a quantum version of gravity can create entanglement. It shows that classical gravity theories can also transmit quantum information, which could change how scientists think about gravity and quantum mechanics working together. The study also suggests new ways to test gravity’s role in quantum experiments.\n\n**What's next step** – The next move is to design and perform experiments that can detect this gravitationally induced entanglement in the lab. These tests could use matter-wave beam splitters or spin-based devices to confirm if classical gravity really does create quantum connections as predicted.\n\n**One-sentence takeaway** – New research reveals that classical gravity can produce quantum entanglement by transmitting quantum information, opening fresh paths to understand gravity’s role in quantum physics.",
      "personality_title_fr": "Une nouvelle étude montre que la gravité classique peut créer de l’intrication quantique",
      "personality_presentation_fr": "**Contexte** – Les scientifiques se demandent depuis longtemps comment la gravité s’accorde avec les règles étranges de la physique quantique. Habituellement, la gravité est vue comme une force classique, tandis que la physique quantique décrit le comportement bizarre des particules minuscules, comme l’intrication, où des particules restent liées même à distance.\n\n**Ce qui s’est passé** – Des chercheurs ont utilisé des mathématiques avancées de la théorie quantique des champs pour étudier une expérience de pensée célèbre impliquant deux petites masses sphériques placées dans des états quantiques spéciaux appelés superpositions. Ils ont découvert que même si la gravité est classique (pas quantique), elle peut quand même rendre ces masses intriquées en échangeant de l’information quantique par des processus physiques locaux. Cela veut dire que la gravité peut relier des particules de façon quantique sans être elle-même quantique.\n\n**Impact** – C’est important car cela remet en question l’idée que seule une version quantique de la gravité peut créer de l’intrication. Cela montre que les théories classiques de la gravité peuvent aussi transmettre de l’information quantique, ce qui pourrait changer la façon dont les scientifiques pensent la gravité et la mécanique quantique ensemble. L’étude suggère aussi de nouvelles manières de tester le rôle de la gravité dans les expériences quantiques.\n\n**Prochaine étape** – La prochaine étape est de concevoir et réaliser des expériences capables de détecter cette intrication causée par la gravité en laboratoire. Ces tests pourraient utiliser des séparateurs d’ondes de matière ou des dispositifs basés sur le spin pour confirmer si la gravité classique crée vraiment des liens quantiques comme prévu.\n\n**Une phrase clé** – Une nouvelle recherche révèle que la gravité classique peut produire de l’intrication quantique en transmettant de l’information quantique, ouvrant de nouvelles voies pour comprendre le rôle de la gravité en physique quantique.",
      "personality_title_es": "Nuevo estudio muestra que la gravedad clásica puede crear entrelazamiento cuántico",
      "personality_presentation_es": "**Contexto** – Los científicos han preguntado por mucho tiempo cómo encaja la gravedad con las extrañas reglas de la física cuántica. Normalmente, la gravedad se ve como una fuerza clásica, mientras que la física cuántica trata con partículas muy pequeñas y sus comportamientos extraños, como el entrelazamiento, donde partículas se conectan aunque estén lejos.\n\n**Qué pasó** – Investigadores usaron matemáticas avanzadas de la teoría cuántica de campos para estudiar un experimento mental famoso con dos pequeñas masas esféricas en estados cuánticos llamados superposiciones. Descubrieron que, aunque la gravedad sea clásica (no cuántica), puede hacer que estas masas se entrelacen al transmitir información cuántica mediante procesos físicos locales. Esto significa que la gravedad puede conectar partículas de forma cuántica sin ser ella misma cuántica.\n\n**Impacto** – Esto es importante porque desafía la creencia de que solo una gravedad cuántica puede crear entrelazamiento. Muestra que teorías clásicas de la gravedad también pueden transmitir información cuántica, lo que podría cambiar cómo los científicos piensan la gravedad y la física cuántica juntas. El estudio también sugiere nuevas formas de probar el papel de la gravedad en experimentos cuánticos.\n\n**Próximo paso** – El siguiente paso es diseñar y hacer experimentos que puedan detectar este entrelazamiento causado por la gravedad en el laboratorio. Estas pruebas podrían usar divisores de onda de materia o dispositivos basados en espín para confirmar si la gravedad clásica realmente crea conexiones cuánticas como se predice.\n\n**Frase clave** – Una nueva investigación revela que la gravedad clásica puede producir entrelazamiento cuántico al transmitir información cuántica, abriendo nuevos caminos para entender el papel de la gravedad en la física cuántica.",
      "image_url": "public/images/news_image_Classical-theories-of-gravity-produce-entanglement.png",
      "image_prompt": "Two glowing, softly illuminated spheres suspended in a delicate quantum superposition of two nearby positions, connected by shimmering, intertwining threads of light symbolizing gravitational entanglement, set against a calm, abstract cosmic background with gentle, natural hues of blue, red, and warm neutrals, rendered as a detailed, warm-toned drawing or painting."
    },
    {
      "title": "Optimization by decoded quantum interferometry",
      "summary": "Nature, Published online: 22 October 2025; doi:10.1038/s41586-025-09527-5Decoded quantum interferometry is a quantum algorithm that uses the quantum Fourier transform to reduce optimization problems to decoding problems.",
      "content": "NP-hardness results indicate that finding exact optima and even sufficiently good approximate optima for worst-case instances of many optimization problems is probably out of reach for polynomial-time algorithms both classical and quantum4. Nevertheless, there remain combinatorial optimization problems, such as the closest vector problem, for which there is a large gap between the best approximation achieved by a polynomial-time classical algorithm5 and the strongest complexity-theoretic inapproximability result6. When considering average-case complexity, such gaps become more prevalent, as few average-case inapproximability results are known. These gaps present a potential opportunity for quantum computers, namely achieving in polynomial time an approximation that requires superpolynomial time to achieve using known classical algorithms.\n\nQuantum algorithms for combinatorial optimization have been the subject of intense research over the last three decades7,8,9,10,11,12,13, which has uncovered some evidence of a possible superpolynomial quantum speed-up for certain optimization problems14,15,16,17,18,19,20. Nevertheless, the problem of finding a superpolynomial quantum advantage for optimization is extremely challenging and remains largely open.\n\nHere we propose a quantum algorithm for optimization that uses interference patterns as its main underlying principle. We call this algorithm decoded quantum interferometry (DQI). DQI uses a quantum Fourier transform to arrange that amplitudes interfere constructively on symbol strings for which the objective value is large, thereby enhancing the probability of obtaining good solutions upon measurement. Most previous approaches to quantum optimization have been Hamiltonian-based7,8, with a notable exception being the superpolynomial speed-up due to Chen, Liu and Zhandry16 for finding short lattice vectors, which uses Fourier transforms and can be seen as an ancestor of DQI. Whereas Hamiltonian-based quantum optimization methods are often regarded as exploiting the local structure of the optimization landscape (for example, tunnelling across barriers21), our approach instead exploits the sparsity that is routinely present in the Fourier spectrum of the objective functions for combinatorial optimization problems, and it can also exploit more elaborate structure in the spectrum if present.\n\nBefore presenting evidence that DQI can efficiently obtain approximate optima not achievable by known polynomial-time classical algorithms, we quickly illustrate the essence of the DQI algorithm by applying it to max-XORSAT. We use max-XORSAT as our first example because, although it is not the problem on which DQI has achieved its greatest success, it is the context in which DQI is simplest to explain.\n\nGiven an m × n matrix B with m > n, the max-XORSAT problem is to find an n-bit string x satisfying as many as possible of the m linear mod-2 equations Bx = v. As we are working modulo 2, we regard all entries of the matrix B and the vectors x and v as coming from the finite field \\({{\\mathbb{F}}}_{2}\\). The max-XORSAT problem can be rephrased as maximizing the objective function\n\n$$f({\\bf{x}})=\\mathop{\\sum }\\limits_{i=1}^{m}{(-1)}^{{v}_{i}+{{\\bf{b}}}_{i}\\cdot {\\bf{x}}},$$ (1)\n\nwhere b i is the ith row of B and v i is the ith entry of v. Thus, f(x) is the number of the m linear equations that are satisfied minus the number unsatisfied.\n\nFrom equation (1), one can see that the Hadamard transform of f is extremely sparse: it has m non-zero amplitudes, which are on the strings b 1 , …, b m . The state \\({\\sum }_{{\\bf{x}}\\in {{\\mathbb{F}}}_{2}^{n}}f({\\bf{x}})| {\\bf{x}}\\rangle \\) is, thus, easy to prepare. Simply prepare the superposition \\({\\sum }_{i=1}^{m}{(-1)}^{{v}_{i}}| {{\\bf{b}}}_{i}\\rangle \\) and apply the quantum Hadamard transform. (Here, for simplicity, we have omitted normalization factors). Measuring the state \\({\\sum }_{{\\bf{x}}\\in {{\\mathbb{F}}}_{2}^{n}}f({\\bf{x}})| {\\bf{x}}\\rangle \\) in the computational basis yields a biased sample, where a string x is obtained with probability proportional to f(x)2, which slightly enhances the likelihood of obtaining strings with a large objective value relative to uniform random sampling.\n\nTo obtain stronger enhancement, DQI prepares states of the form\n\n$$| P(f)\\rangle =\\sum _{{\\bf{x}}\\in {{\\mathbb{F}}}_{2}^{n}}P(f({\\bf{x}}))| {\\bf{x}}\\rangle ,$$ (2)\n\nwhere P is an appropriately normalized degree-ℓ polynomial. The Hadamard transform of such a state always takes the form\n\n$$\\mathop{\\sum }\\limits_{k=0}^{{\\ell }}\\frac{{w}_{k}}{\\sqrt{\\left(\\begin{array}{c}m\\\\ k\\end{array}\\right)}}\\sum _{\\begin{array}{c}{\\bf{y}}\\in {{\\mathbb{F}}}_{2}^{m}\\\\ | {\\bf{y}}| =k\\end{array}}{(-1)}^{{\\bf{v}}\\cdot {\\bf{y}}}| {B}^{{\\rm{T}}}{\\bf{y}}\\rangle ,$$ (3)\n\nfor some coefficients w 0 , …, w ℓ . Here ∣y∣ denotes the Hamming weight of the bit string y. The DQI algorithm prepares |P(f)⟩ in five steps. The first step is to prepare the superposition \\({\\sum }_{k=0}^{{\\ell }}{w}_{k}| {D}_{m,k}\\rangle \\), where\n\n$$| {D}_{m,k}\\rangle =\\frac{1}{\\sqrt{\\left(\\begin{array}{c}m\\\\ k\\end{array}\\right)}}\\sum _{\\begin{array}{c}{\\bf{y}}\\in {{\\mathbb{F}}}_{2}^{m}\\\\ | {\\bf{y}}| =k\\end{array}}| {\\bf{y}}\\rangle $$ (4)\n\nis the Dicke state of weight k. Preparing such superpositions over Dicke states can be done with \\({\\mathcal{O}}({m}^{2})\\) quantum gates using the techniques in refs. 22,23. Second, the phase (−1)v⋅y is imposed by applying the product \\({Z}_{1}^{{v}_{1}}\\otimes \\ldots \\otimes {Z}_{m}^{{v}_{m}}\\), where Z m is the Pauli-Z operator acting on the mth qubit. Third, the quantity BTy is computed into an ancilla register using a reversible circuit for matrix multiplication. This yields the state\n\n$$\\mathop{\\sum }\\limits_{k=0}^{{\\ell }}\\frac{{w}_{k}}{\\sqrt{\\left(\\begin{array}{c}m\\\\ k\\end{array}\\right)}}\\sum _{\\begin{array}{c}{\\bf{y}}\\in {{\\mathbb{F}}}_{2}^{m}\\\\ | {\\bf{y}}| =k\\end{array}}{(-1)}^{{\\bf{v}}\\cdot {\\bf{y}}}| {\\bf{y}}\\rangle | {B}^{{\\rm{T}}}{\\bf{y}}\\rangle .$$ (5)\n\nThe fourth step is to use the value BTy to infer y, which can then be subtracted from \\(| {\\bf{y}}\\rangle \\), thereby bringing it back to the all zeros state, which can be discarded. (This is known as ‘uncomputation’24). The fifth and final step is to apply a Hadamard transform to the remaining register, yielding |P(f)⟩. This sequence of steps is illustrated in Fig. 1.\n\nFig. 1: Schematic illustration of DQI. As the initial Dicke state is of weight ℓ, the final polynomial P is of degree ℓ. Here, for simplicity, we take w ℓ = 1 and w k = 0 for all k ≠ ℓ. Recycling icon adapted from FreeSVG (https://freesvg.org) under a CC0 1.0 Universal Public Domain licence. Full size image\n\nThe fourth step, in which |y⟩ is uncomputed, is not straightforward because B is a non-square matrix and, thus, inferring y from BTy is an underdetermined linear algebra problem. However, we also know that ∣y∣ ≤ ℓ. The problem of solving this underdetermined linear system with a Hamming weight constraint is precisely the syndrome decoding problem for the classical error-correcting code \\({C}^{\\perp }=\\{{\\bf{d}}\\in {{\\mathbb{F}}}_{2}^{m}:{B}^{{\\rm{T}}}{\\bf{d}}={\\bf{0}}\\}\\) with up to ℓ errors.\n\nIn general, syndrome decoding is an NP-hard problem25. However, when B is very sparse or has certain kinds of algebraic structure, the decoding problem can be solved by polynomial-time classical algorithms even when ℓ is large (for example, linear in m). By solving this decoding problem using a reversible implementation of such a classical decoder, one uncomputes \\(| {\\bf{y}}\\rangle \\) in the first register. If the decoding algorithm requires T quantum gates, then the number of gates required to prepare |P(f)⟩ is \\({\\mathcal{O}}(T+{m}^{2})\\).\n\nApproximate solutions to the optimization problem are obtained by measuring |P(f)⟩ in the computational basis. The higher the degree of the polynomial in |P(f)⟩, the greater one can bias the measured bit strings towards solutions with a large objective value. However, this requires solving a harder decoding problem, as the maximum number of errors is equal to the degree of P. Next, we summarize how, by making an optimal choice of P and a judicious choice of decoder, DQI can be a powerful optimizer for some classes of problems.\n\nAlthough DQI can be applied more broadly, the most general optimization problem that we apply DQI to in this paper is max-LINSAT, which we define as follows.\n\nDefinition 1\n\nLet \\({{\\mathbb{F}}}_{p}\\) be a finite field and let \\(B\\in {{\\mathbb{F}}}_{p}^{m\\times n}\\). For each i = 1, …, m, let \\({F}_{i}\\subset {{\\mathbb{F}}}_{p}\\) be an arbitrary subset of \\({{\\mathbb{F}}}_{p}\\), which yields a corresponding constraint \\({\\sum }_{j=1}^{n}{B}_{ij}{x}_{j}\\in {F}_{i}\\). The max-LINSAT problem is to find \\({\\bf{x}}\\in {{\\mathbb{F}}}_{p}^{n}\\) satisfying as many as possible of these m constraints.\n\nWe focus primarily on the case that p has at most polynomially large magnitude and the subsets F 1 , …, F m are given as explicit lists. The max-XORSAT problem is the special case where p = 2 and ∣F i ∣ = 1 for all i.\n\nConsider a max-LINSAT instance where the sets F 1 , …, F m each have size r. Let ⟨s⟩ be the expected number of constraints satisfied by the symbol string sampled in the final measurement of the DQI algorithm. Suppose we have a polynomial-time algorithm that can correct up to ℓ bit flip errors on codewords from the code \\({C}^{\\perp }=\\{{\\bf{d}}\\in {{\\mathbb{F}}}_{p}^{m}:{B}^{{\\rm{T}}}{\\bf{d}}={\\bf{0}}\\}\\). Then, in polynomial time, DQI achieves the following approximate optimum to the max-LINSAT problem:\n\n$$\\frac{\\langle s\\rangle }{m}={\\left(\\sqrt{\\frac{{\\ell }}{m}\\left(1-\\frac{r}{p}\\right)}+\\sqrt{\\frac{r}{p}\\left(1-\\frac{{\\ell }}{m}\\right)}\\right)}^{2},$$ (6)\n\nif r/p ≤ 1 − ℓ/m and ⟨s⟩/m = 1 otherwise. See Supplementary Theorem 1.1 for the precise statement for perfect decoding and Supplementary Theorem 7.1 for the analogous statement in the presence of decoding errors. This is achieved by a specific optimal choice of the coefficients w 0 , …, w ℓ , which can be classically precomputed in polynomial time, as described in Supplementary Information section 6.\n\nNote that r/p is the fraction of constraints that would be satisfied if the variables were assigned uniformly at random. When r/p = 1/2, equation (6) becomes the equation of a semicircle. Hence, we informally refer to equation (6) as the ‘semicircle law’.\n\nFrom equation (6), any result on decoding a class of linear codes implies a corresponding result regarding the performance of DQI for solving a class of combinatorial optimization problems that are dual to these codes. This enables two new lines of research in quantum optimization. The first is to harvest the coding theory literature for rigorous theorems on the performance of decoders for various codes and obtain as corollaries guarantees on the approximation achieved by DQI for the corresponding optimization problems. The second is to perform computer experiments to determine the empirical performance of classical heuristic decoders, which, through equation (6), can be compared against the empirical performance of classical heuristic optimizers. In this manner, DQI can be benchmarked instance-by-instance against classical heuristics, even for optimization problems far too large to attempt on present-day quantum hardware. We next describe our results so far from each of these two lines of research.\n\nWe first use rigorous decoding guarantees to analyse the performance of DQI on the following problem.\n\nDefinition 2\n\nGiven integers n < p − 1 with p prime, an instance of the optimal polynomial intersection (OPI) problem is as follows. Let F 1 , …, F p−1 be subsets of the finite field \\({{\\mathbb{F}}}_{p}\\). Find a polynomial \\(Q\\in {{\\mathbb{F}}}_{p}[y]\\) of degree at most n − 1 that maximizes f OPI (Q) = ∣{y ∈ {1, …, p − 1}: Q(y) ∈ F y }∣, that is, that intersects as many of these subsets as possible.\n\nAn illustration of this problem is given in Fig. 2.\n\nFig. 2: Illustration of OPI problem. A stylized example of the OPI problem. For \\({y}_{1}\\in {{\\mathbb{F}}}_{p}\\), the orange set above the point y 1 represents \\({F}_{{y}_{1}}\\). Both polynomials Q 1 (y) and Q 2 (y) represent solutions that have a large objective value, as they each intersect all but one set F y . Full size image\n\nIn Supplementary Information section 2, we show that OPI is a special case of max-LINSAT over \\({{\\mathbb{F}}}_{p}\\) with m = p − 1 constraints in which B is a Vandermonde matrix and, thus, C⊥ is a Reed–Solomon code. Syndrome decoding for Reed–Solomon codes can be solved in polynomial time out to half the distance of the code, for example, using the Berlekamp–Massey algorithm26. Consequently, in DQI we can take ℓ = ⌊(n + 1)/2⌋. For the regime where r/p and n/p are constants and p is taken asymptotically large, the fraction of satisfied constraints achieved by DQI using the Berlekamp–Massey decoder can be obtained by substituting ℓ/m = n/2p into equation (6).\n\nOPI and special cases of it have been studied in several domains. In the coding theory literature, OPI is studied under the name ‘list-recovery’, and in the cryptography literature it is studied under the name ‘noisy polynomial reconstruction/interpolation’27,28. OPI can also be viewed as a generalization of the polynomial approximation problem, studied in refs. 29,30,31, in which each set F i is a contiguous range of values in \\({{\\mathbb{F}}}_{p}\\). In Supplementary Information section 8, we analyse the algorithms from these works in the literature and find that, for the parameter regime addressed by DQI, the best approximation achieved in polynomial time classically is 1/2 + n/2p using Prange’s algorithm. As shown in Fig. 3, for r/p = 1/2 and any fixed 0 < n/p < 1, DQI with the Berlekamp–Massey decoder exceeds the satisfaction fraction achieved by Prange’s algorithm in the limit of large p. Classically, the only methods we are aware of to exceed the satisfaction fraction achieved by Prange’s algorithm are brute force search or slight refinements thereof, which have exponential runtime. Thus, DQI achieves a superpolynomial speed-up for this problem, assuming no polynomial-time algorithm is found that can match the satisfaction fraction that DQI achieves.\n\nFig. 3: Approximate optima for OPI. Plot of the expected fraction ⟨s⟩/p of satisfied constraints achieved by DQI with the Berlekamp–Massey decoder and by Prange’s algorithm for the OPI problem in the balanced case r/p = 1/2, as a function of the ratio of variables to constraints n/p. At n/p = 1/10, Prange’s algorithm satisfies a fraction 0.55 of the clauses whereas DQI satisfies \\(\\langle s\\rangle /p=1/2+\\sqrt{19}/20\\approx 0.7179\\). As a concrete challenge to the classical algorithms community, we propose matching or exceeding this value in polynomial time. In our concrete resource estimation, we consider n/p = 1/2, where OPI achieves \\(\\langle s\\rangle /p=1/2+\\sqrt{3}/4\\approx 0.9330\\) and Prange’s algorithm achieves 0.75. BM, Berlekamp–Massey decoder. Full size image\n\nAt present, there are no results directly showing that the OPI problem in the parameter regime that we consider is classically intractable under any standard complexity-theoretic or cryptographic assumptions. However, such results are known for certain limiting cases of the OPI problem, and we propose the task of extending these results to regimes more relevant to DQI for future research. The hardness of the special case of OPI when \\(| \\,{f}_{i}^{-1}(\\,+1)| =1\\), in a certain parameter regime, has been proposed as a cryptographic assumption in ref. 32, which has not been broken to our knowledge. Finding exact optima for OPI with \\(| \\,{f}_{i}^{-1}(\\,+1)| =1\\) can be cast as maximum-likelihood decoding for Reed–Solomon codes, which is known to be NP-hard33,34. Finding sufficiently good approximate optima is known to be as hard as discrete log35,36, but these hardness results do not match the parameter regime addressed by DQI.\n\nAs a concrete example, for n ≈ p/10 and r/p ≈ 1/2, the fraction of constraints satisfied by Prange’s algorithm is 0.55, whereas DQI achieves \\(1/2+\\sqrt{19}/20\\approx 0.7179\\). As a specific point of comparison, we challenge the algorithms community to beat this with a classical polynomial-time algorithm. Interestingly, for these parameters, one statistically expects that solutions satisfying all p − 1 constraints exist, but they apparently remain out of reach of polynomial-time algorithms both quantum and classical.\n\nTo find classically intractable instances of OPI solvable by DQI with minimal quantum resources, we find it is advantageous to choose n/p ≈ r/p ≈ 1/2. For these parameters, DQI achieves satisfaction fraction 0.933. As discussed in Supplementary Information section 13, achieving this using classical algorithms known to us has a prohibitive computational cost for p as small as 521. The dominant cost in DQI plus the Berlekamp–Massey decoder is the reversible implementation of the subroutine to find the shortest linear-feedback shift register used in the Berlekamp–Massey algorithm. Using Qualtran37, we find that at p = 521, the linear-feedback shift register can be found using approximately 1 × 108 logical Toffoli gates and 9 × 103 logical qubits.\n\nWe next use computer experiments to benchmark the performance of DQI against classical heuristics on average-case instances from certain families of max-XORSAT with sparse B. DQI reduces such problems to decoding problems on codes with sparse parity-check matrices. Such codes are known as low-density parity-check (LDPC) codes. Polynomial-time classical algorithms, such as belief propagation, can decode randomly sampled LDPC codes up to numbers of errors that nearly saturate information-theoretic limits3,38,39. This makes sparse max-XORSAT an enticing target for DQI. Although we use max-XORSAT as a convenient test bed for DQI, other commonly studied optimization problems, such as max-k-SAT, could be addressed similarly. Specifically, consider any binary optimization problem in which the objective function counts the number of satisfied constraints, where each constraint is a Boolean function of at most k variables. By taking the Hadamard transform of the objective function, one converts such a problem into an instance of weighted max-k-XORSAT, where the number of variables is unchanged and the number of constraints has been increased by at most a factor of 2k.\n\nAlthough we are able to analyse the asymptotic average-case performance of DQI rigorously, we do not restrict the classical competition to algorithms with rigorous performance guarantees. Instead, we choose to set a high bar by also attempting to beat the empirical performance of classical heuristics that lack such guarantees.\n\nThrough careful tuning of sparsity patterns in B, we are able to find some families of sparse max-XORSAT instances for which DQI with standard belief-propagation decoding finds solutions satisfying a larger fraction of constraints than we are able to find using a comparable number of computational steps by any of the general-purpose classical optimization heuristics that we tried, which are listed in Table 1. However, unlike our OPI example, we do not put this forth as a potential example of superpolynomial quantum advantage. Rather, we are able to construct a tailored classical algorithm specialized to these instances, which, with 7 min of runtime, finds solutions where the fraction of constraints satisfied slightly beats DQI plus belief propagation (DQI + BP). As discussed in Supplementary Information section 9, our tailored heuristic is a variant of simulated annealing that assigns temperature-dependent weights to the terms in the cost function determined by how many variables they contain.\n\nTable 1 Approximate optima for max-XORSAT Full size table\n\nThe comparison against simulated annealing is complicated because, as shown in Supplementary Information section 8.2, the fraction of clauses satisfied by simulated annealing increases as a function of the duration of the anneal. Thus, there is not a unique sharply defined number indicating the maximum satisfaction fraction reachable by simulated annealing. DQI reduces our sparsity-tuned max-XORSAT problem to an LDPC decoding problem, which our implementation of belief propagation solves in approximately 8 s on a single core, excluding the time used to load and parse the instance. Thus, a natural point of comparison is the result obtained by simulated annealing with a similar runtime. By running our optimized C++ implementation of simulated annealing for 8 s, we are only able to reach 0.764. If we allow the parallel execution of several anneals and increase our runtime allowance, we are able to eventually replicate the satisfaction fraction achieved by DQI + BP using simulated annealing. The shortest anneal that achieved this used five cores and ran for 73 h, which is five orders of magnitude longer than our belief-propagation decoder. Although this is dependent on the implementation details, we can take this ratio of runtimes as a rough indicator of the ratio of computational steps. In the context of DQI, the decoder would need to be implemented as a reversible circuit and subject to an overhead due to quantum error correction, so this should not be interpreted as an indicator of the quantum versus the classical runtime.",
      "url": "https://www.nature.com/articles/s41586-025-09527-5",
      "source": "Nature",
      "published": "2025-10-23",
      "sentiment_score": 0.85,
      "reasoning": "The article reports a significant breakthrough in quantum algorithms for combinatorial optimization, proposing a new quantum algorithm (decoded quantum interferometry) that can achieve superpolynomial speed-ups over classical algorithms for certain important optimization problems. This advancement has broad implications for computational complexity, quantum computing, and optimization, potentially enabling solutions to problems previously considered intractable. The article provides detailed context, theoretical foundations, and empirical benchmarks, demonstrating meaningful progress in quantum optimization technology with potential wide-ranging benefits.",
      "category": "Technology",
      "personality_title": "New quantum algorithm offers faster solutions for complex optimization problems",
      "personality_presentation": "**Context** – Many important optimization problems are very hard to solve exactly, even with powerful classical or quantum computers working quickly. These problems appear in fields like coding, cryptography, and computer science. Scientists have been searching for quantum algorithms that can solve such problems faster than classical methods, but finding a big advantage has been very difficult.\n\n**What happened** – Researchers introduced a new quantum algorithm called decoded quantum interferometry (DQI). This method uses a special quantum process called the quantum Fourier transform to focus on the best possible solutions by making certain patterns in the data interfere constructively. DQI turns optimization problems into decoding problems, which can sometimes be solved efficiently. They tested DQI on problems like max-XORSAT and a more general problem called max-LINSAT, showing it can find better approximate solutions than known classical algorithms, sometimes with much less computing time.\n\n**Impact** – This approach is unique because it uses the hidden structure in problems’ Fourier spectra instead of relying on local search methods used in earlier quantum algorithms. For some problems, like the optimal polynomial intersection (OPI), DQI achieves solutions that classical algorithms can only reach with much longer, possibly exponential, runtimes. In one example, DQI satisfied about 72% of constraints, while the best classical polynomial-time algorithm satisfied only 55%. This suggests DQI can provide a superpolynomial speed-up for certain tasks, moving closer to solving problems once believed too hard for efficient quantum or classical methods.\n\n**What's next step** – Future work will focus on testing DQI on larger and more varied problems, improving the decoding techniques it relies on, and exploring its limits. Researchers also plan to challenge the classical algorithms community to match or beat DQI’s performance using new classical methods. Additionally, implementing DQI on real quantum hardware and refining its resource requirements will be important steps toward practical applications.\n\n**One-sentence takeaway** – The new quantum algorithm decoded quantum interferometry offers a promising way to solve some complex optimization problems faster than classical methods by cleverly using quantum interference and decoding techniques.",
      "personality_title_fr": "Un nouvel algorithme quantique accélère la résolution de problèmes d’optimisation complexes",
      "personality_presentation_fr": "**Contexte** – De nombreux problèmes d’optimisation importants sont très difficiles à résoudre exactement, même avec des ordinateurs classiques ou quantiques puissants. Ces problèmes apparaissent dans des domaines comme la codification, la cryptographie et l’informatique. Les chercheurs cherchent depuis longtemps des algorithmes quantiques capables de résoudre ces problèmes plus rapidement que les méthodes classiques, mais trouver un avantage important a été très difficile.\n\n**Ce qui s’est passé** – Des chercheurs ont présenté un nouvel algorithme quantique appelé interférométrie quantique décodée (DQI). Cette méthode utilise un processus quantique spécial appelé transformée de Fourier quantique pour concentrer l’attention sur les meilleures solutions possibles en faisant interférer certaines structures dans les données. DQI transforme les problèmes d’optimisation en problèmes de décodage, parfois solvables efficacement. Ils ont testé DQI sur des problèmes comme max-XORSAT et un problème plus général appelé max-LINSAT, montrant qu’il peut trouver de meilleures solutions approximatives que les algorithmes classiques connus, souvent en beaucoup moins de temps.\n\n**Impact** – Cette approche est unique car elle exploite la structure cachée dans le spectre de Fourier des problèmes, au lieu de s’appuyer sur des méthodes de recherche locale utilisées dans les algorithmes quantiques précédents. Pour certains problèmes, comme l’intersection polynomiale optimale (OPI), DQI atteint des solutions que les algorithmes classiques ne peuvent obtenir qu’en un temps beaucoup plus long, parfois exponentiel. Par exemple, DQI satisfait environ 72 % des contraintes, alors que le meilleur algorithme classique en temps polynomial en satisfait seulement 55 %. Cela suggère que DQI peut offrir une accélération superpolynomiale pour certaines tâches, rapprochant la résolution de problèmes autrefois jugés trop difficiles.\n\n**Prochaine étape** – Les travaux futurs viseront à tester DQI sur des problèmes plus grands et variés, à améliorer les techniques de décodage utilisées, et à explorer ses limites. Les chercheurs prévoient aussi de lancer un défi à la communauté des algorithmes classiques pour égaler ou dépasser les performances de DQI. Par ailleurs, l’implémentation de DQI sur du matériel quantique réel et l’optimisation de ses ressources seront des étapes importantes vers des applications pratiques.\n\n**Résumé en une phrase** – Le nouvel algorithme quantique interférométrie quantique décodée offre une méthode prometteuse pour résoudre certains problèmes d’optimisation complexes plus rapidement que les méthodes classiques en utilisant intelligemment l’interférence quantique et le décodage.",
      "personality_title_es": "Nuevo algoritmo cuántico acelera la solución de problemas complejos de optimización",
      "personality_presentation_es": "**Contexto** – Muchos problemas importantes de optimización son muy difíciles de resolver exactamente, incluso con computadoras clásicas o cuánticas potentes. Estos problemas aparecen en campos como codificación, criptografía e informática. Los investigadores han buscado durante mucho tiempo algoritmos cuánticos que puedan resolver estos problemas más rápido que los métodos clásicos, pero encontrar una ventaja significativa ha sido muy difícil.\n\n**Qué pasó** – Investigadores presentaron un nuevo algoritmo cuántico llamado interferometría cuántica decodificada (DQI). Este método usa un proceso cuántico especial llamado transformada de Fourier cuántica para enfocarse en las mejores soluciones posibles haciendo que ciertos patrones en los datos interfieran constructivamente. DQI convierte problemas de optimización en problemas de decodificación, que a veces pueden resolverse eficientemente. Probaron DQI en problemas como max-XORSAT y un problema más general llamado max-LINSAT, mostrando que puede encontrar mejores soluciones aproximadas que los algoritmos clásicos conocidos, a menudo con mucho menos tiempo de cómputo.\n\n**Impacto** – Este enfoque es único porque usa la estructura oculta en los espectros de Fourier de los problemas en lugar de depender de métodos de búsqueda local usados en algoritmos cuánticos anteriores. Para algunos problemas, como la intersección polinómica óptima (OPI), DQI logra soluciones que los algoritmos clásicos solo pueden alcanzar con tiempos mucho mayores, posiblemente exponenciales. En un ejemplo, DQI cumplió cerca del 72% de las restricciones, mientras que el mejor algoritmo clásico en tiempo polinomial cumplió solo el 55%. Esto sugiere que DQI puede ofrecer una aceleración superpolinomial para ciertas tareas, acercándose a resolver problemas que antes se creían demasiado difíciles.\n\n**Próximo paso** – El trabajo futuro se centrará en probar DQI en problemas más grandes y variados, mejorar las técnicas de decodificación que utiliza y explorar sus límites. Los investigadores también planean desafiar a la comunidad de algoritmos clásicos a igualar o superar el desempeño de DQI con nuevos métodos clásicos. Además, implementar DQI en hardware cuántico real y optimizar sus recursos serán pasos importantes hacia aplicaciones prácticas.\n\n**Conclusión en una frase** – El nuevo algoritmo cuántico interferometría cuántica decodificada ofrece una forma prometedora de resolver algunos problemas complejos de optimización más rápido que los métodos clásicos usando de manera inteligente la interferencia cuántica y técnicas de decodificación.",
      "image_url": "public/images/news_image_Optimization-by-decoded-quantum-interferometry.png",
      "image_prompt": "A warm, detailed painting of an intricate quantum circuit composed of softly glowing, interwoven light waves and delicate lattice patterns symbolizing quantum interference, with elegant arcs representing Fourier transforms and sparse matrix structures, all set against a calm, abstract backdrop of softly blended natural tones that evoke harmony and precision in quantum optimization."
    }
  ]
}