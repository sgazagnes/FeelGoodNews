{
  "personality": null,
  "timestamp": "2025-09-04T04:38:43.766811",
  "category": "Technology",
  "news_summary": "Advancements in technology are enhancing infrastructure resilience, accelerating AI computation with analog optical systems, enabling detailed genetic analysis, and facilitating scalable quantum computer manufacturing through 3D printing.",
  "news_summary_fr": "Les progrès technologiques améliorent la résilience des infrastructures, accélèrent le calcul de l'intelligence artificielle grâce à des systèmes optiques analogiques, permettent des analyses génétiques détaillées et facilitent la fabrication d'ordinateurs quantiques évolutifs grâce à l'impression 3D.",
  "news_summary_es": "Los avances tecnológicos están mejorando la resistencia de las infraestructuras, acelerando la computación de la IA con sistemas ópticos analógicos, permitiendo análisis genéticos detallados y facilitando la fabricación de ordenadores cuánticos escalables mediante impresión 3D.",
  "articles": [
    {
      "title": "Latent resistance mechanisms of steel truss bridges after critical failures",
      "summary": "Nature, Published online: 03 September 2025; doi:10.1038/s41586-025-09300-8Experimental tests and computational simulations of a scaled-down specimen of a real steel truss bridge identify and characterize the latent resistance mechanisms following critical failures, demonstrating how loads supported by failed components can be redistributed and enable structure resilience.",
      "content": "Specimen design\n\nWe performed experimental tests on a scaled specimen that reproduces one of the simply supported spans of a real railway bridge (Supplementary Video 1). The considered simply supported span (Extended Data Fig. 1) consists of two lateral Pratt trusses (the main system) connected by vertical transverse X-bracings, horizontal upper and lower X-bracings and lower transverse beams (the bracing system), as well as upper transversal floor beams with longitudinal rail beams (that is, stringers) joined to them (the floor system).\n\nTo accurately reproduce structural behaviour, load redistribution and modes of failure, we designed the scaled bridge specimen using the following scaling criteria based on dimensional analysis38,39:\n\n1. The model is made of steel with a yield strength f y = 275 MPa (S275 according to EN 10025-1:2004 (ref. 51)). The density and mechanical properties of the steel of the actual bridge are practically the same as the density and mechanical properties of S275 steel. Therefore, all scaling factors related to the material (density: λ ρ , Young’s modulus: λ E , Poisson’s ratio: λ ν ) are taken equal to one. 2. A scale factor λ L = 3.5 has been chosen for member lengths. Cross-section areas and moments of inertia have been scaled accordingly (\\({\\lambda }_{{\\rm{A}}}={\\lambda }_{{\\rm{L}}}^{2}\\), \\({\\lambda }_{{\\rm{I}}}={\\lambda }_{{\\rm{L}}}^{4}\\)) (Supplementary Information Section 1). 3. Assuming that the scale of forces is λ F , measured displacements of the model scale according to \\({\\lambda }_{{\\rm{d}}}={\\lambda }_{{\\rm{F}}}\\frac{{\\lambda }_{{\\rm{L}}}}{{\\lambda }_{E}{\\lambda }_{{\\rm{A}}}}=\\frac{{\\lambda }_{{\\rm{F}}}}{{\\lambda }_{{\\rm{L}}}}\\) and measured rotations scale according to \\({\\lambda }_{\\theta }=\\frac{{\\lambda }_{{\\rm{F}}}}{{\\lambda }_{{\\rm{L}}}^{2}}\\). 4. Internal forces (axial and shear) are scaled according to λ N,V = λ F and bending moments are scaled according to λ M = λ F λ L . 5. Three similitude conditions are especially relevant for the selection of the scale factor for the external forces: a. Similitude of stress and yield strength: \\({\\lambda }_{\\sigma }=\\frac{{\\lambda }_{{\\rm{F}}}}{{\\lambda }_{{\\rm{A}}}}={\\lambda }_{{f}_{{\\rm{y}}}}=1\\). This criterion leads to \\({\\lambda }_{{\\rm{F}}}={\\lambda }_{{\\rm{L}}}^{2}\\). b. Similitude of axial force and member buckling load: \\({\\lambda }_{{\\rm{F}}}=\\frac{{\\lambda }_{E}{\\lambda }_{{\\rm{I}}}}{{\\lambda }_{{\\rm{L}}}^{2}}={\\lambda }_{{\\rm{L}}}^{2}\\). c. Similitude of stress and local buckling stress. The critical local buckling stress is given by a relation of the type \\({\\sigma }_{{\\rm{cr}}}=k{\\rm{\\pi }}\\frac{E{t}^{2}}{12(1-{\n\nu }^{2}){b}^{2}}\\), in which k is a buckling constant, E is Young’s modulus, ν is Poisson’s ratio, t is the section wall thickness and b is a representative wall length. Therefore, \\({\\lambda }_{\\sigma }=\\frac{{\\lambda }_{{\\rm{F}}}}{{\\lambda }_{{\\rm{L}}}^{2}}=\\frac{{\\lambda }_{{\\rm{E}}}{\\lambda }_{t}^{2}}{{\\lambda }_{(1-{\n\nu }^{2})}{\\lambda }_{b}^{2}}=\\frac{{\\lambda }_{t}^{2}}{{\\lambda }_{b}^{2}}\\) and this criterion leads to \\({\\lambda }_{{\\rm{F}}}={\\lambda }_{{\\rm{L}}}^{2}\\frac{{\\lambda }_{t}^{2}}{{\\lambda }_{b}^{2}}\\). We have selected a scaling factor \\({\\lambda }_{{\\rm{F}}}={\\lambda }_{{\\rm{L}}}^{2}\\) for the external forces. As well as reproducing the similitude of stresses, yield strength and axial forces, this choice ensures the similitude of member buckling loads; this is crucial, as member buckling determines the load redistribution in the tested specimen near the collapse. 6. The selected scale factor for external forces is not applicable to the mass, because the latter scales with the volume (\\({\\lambda }_{\\rho }={\\lambda }_{{\\rm{L}}}^{3}\\)). To keep the ratio of vehicle load to self-weight of the scaled-down specimen as in the actual bridge and for the analysis of forces in the bridge, self-weight should be considered as an external load, with a scaling factor \\({\\lambda }_{{\\rm{L}}}^{2}\\). This leads to a scaled self-weight load larger than the actual weight of the bridge specimen plus the dead load of the testing rig used for distributing the test loads. To take this into account, we have defined the total value of the test load so that it includes the correction of the self-weight. The experimental setup and the justification of the test load considering the self-weight correction are explained in the following section.\n\nThe scaled bridge design is shown in Extended Data Fig. 2 and the complete geometric properties of all cross-sections are provided in Supplementary Information Section 1 (together with comparisons between theoretical and actual scaled-down values). Riveted connections typically used in such steel truss bridges do not behave as the idealized pin joints assumed in design. Instead, they effectively perform as rigid connections and usually have a high capacity for resisting bending moments28. As such, all joints of the scaled-down specimen were welded to ensure full-strength connections in a simplified manner.\n\nOn the basis of the selected scaling factors, the measured displacements scale in the same way as member lengths, λ d = λ L ; rotations and load factors do not require scaling, λ θ = λ Load factor = 1; internal forces scale as the external ones, \\({\\lambda }_{{\\rm{N,V}}}={\\lambda }_{{\\rm{L}}}^{2}\\); and bending moments scale as \\({\\lambda }_{{\\rm{M}}}={\\lambda }_{{\\rm{L}}}^{3}\\). The results presented in this work can thus be easily scaled up by applying these conversions.\n\nExperiment and monitoring design\n\nAfter fabrication, the scaled-down bridge specimen had a self-weight of 4.2 kN. It was transported to the ICITECH laboratory and positioned to reproduce the same boundary conditions as those of the real bridge span, with a hinged support on one end and a rolling support on the other (Extended Data Fig. 2). Tests were carried out on the steel material according to EN ISO 6892-1 (ref. 52) to characterize its stress–strain behaviour (Supplementary Information Section 1).\n\nThe load applied during tests corresponded to the operational loads of the reference bridge (Extended Data Fig. 3). The real bridge is loaded by actual convoys of two vehicles with two-axle bogies and a maximum nominal axle load of 88.4 kN. The loading setup used for testing corresponds to the convoy position that is most unfavourable for global bending of the bridge span. Specifically, this occurs when the heaviest bogies of each vehicle are centred with respect to mid-span (with four axles of 88.4 kN). For testing, these loads were applied using a hydraulic jack and distributed on the main system of the bridge through a testing rig with a weight of 5.4 kN. Loads of the same bogie were grouped together.\n\nThe load of the hydraulic jack was introduced pseudo-statically (at a displacement rate of 0.05 mm s−1). The magnitude of this load results from multiplying the nominal load of the convoys by the following dynamic effects: (1) an estimated dynamic impact factor of 1.18 representing the dynamic effect of the vehicles passing through the bridge at a certain speed53 and (2) a dynamic amplification factor of 1.9 owing to the sudden loss of structural members during the analysed damage scenarios54,55. Consequently, the operational load for the bridge specimen was scaled as follows:\n\n$$\\text{Scaled operational load}\\,=\\,88.4\\times 4\\times 1.18\\times 1.9/{3.5}^{2}=64\\,{\\rm{kN}}$$\n\nTo (1) keep the ratio of operational load to self-weight of the scaled-down specimen as in the actual bridge and (2) correctly analyse forces in the bridge, self-weight should also be considered as an external load with a scaling factor \\({\\lambda }_{{\\rm{L}}}^{2}\\). Considering that the actual self-weight of the real bridge span is 171 kN and that only the dynamic amplification factor owing to sudden loss of structural members (that is, 1.9) is applicable in this case, the scaled self-weight should be:\n\n$$\\text{Scaled self-weight}\\,=\\,171\\times 1.9/{3.5}^{2}=26\\,{\\rm{kN}}$$\n\nThis leads to a scaled self-weight load that is larger than the actual weight of the bridge specimen (4.2 kN) plus the dead load (5.4 kN) of the testing rig used for distributing test loads. The total test load applied by the hydraulic jack was therefore corrected to account for this:\n\n$$\\text{Scaled total load for tests}\\,=\\,64+26-4.2-5.4=80\\,{\\rm{kN}}$$\n\nBefore tests, it was also checked that the load was small enough to ensure that none of the components of the bridge would enter a nonlinear mechanical regime, thus preventing failure and damage of one test from affecting the next.\n\nUp to nine different damage scenarios were considered through component removal (that is, by cutting main components), representing the failure of different member types at critical positions in which they support very high levels of internal load. The considered damage scenarios were: (1) a lower chord in the middle of the span; (2) one of the first diagonals of the span; (3) one of the second verticals of the span; (4) and (5) a horizontal and a vertical bracing located close to the load applied by the jack; and (6) a transversal beam. The fourth and fifth damage scenarios were performed in two testing phases representing the failure of one and two bracing elements of each damage scenario type. The sixth damage scenario was also performed in two phases representing partial and total loss of a transversal beam. For these damage scenarios that involve the failure of transversal beams, the load setup was changed because a transversal beam is most critically loaded when only one of the axles of a bogey is positioned directly on it (Extended Data Fig. 3). For these tests, the scaled operational load only considers the highest traffic load that can be transmitted through a single axle and corresponds to 16 kN. Because the dead load of the testing rig was 2 kN in this case and the self-weight of the entire bridge span is not involved in the structural response of the transversal beam, the total applied load by the hydraulic jack was 14 kN. The defined damage scenarios with the different load setups are shown in Extended Data Figs. 3 and 4. For each damage scenario, the bridge specimen was tested twice, once with the undamaged state and again after component removal. After finishing both tests for each damage scenario, the two parts of the cut component were welded together to retrofit the bridge to its original state before assessing the following damage scenario (Supplementary Information Section 1 and Supplementary Video 2). For the last damage scenario (involving the removal of a diagonal), as well as testing with the specified load of 80 kN, the applied loads were also increased until the system collapsed. This last test was used for the validation of the computational models presented in the final section of Methods.\n\nTo monitor the structural behaviour during tests, we heavily instrumented the bridge specimen with several sensors. A total of 80 strain gauges and 14 displacement transducers were placed at key locations in different parts of the bridge. The data from these sensors were complemented by pictures and videos of the structural response captured by three high-resolution cameras (Supplementary Information Section 1 and Extended Data Fig. 5).\n\nSimulations for studying the onset of ALPs\n\nWe performed an extensive set of computational simulations to characterize how the structural response of the bridge changes after the failure of main components. Each failure was represented by removing a single component in the corresponding simulation. At this stage, our computational simulations are aimed at identifying and extracting the resistance mechanisms and the corresponding ALP patterns that the bridge can activate for several damage scenarios.\n\nThe kernel of the computational campaign is a three-dimensional finite element model of the undamaged scaled bridge subjected to the same conditions reproduced in tests. With this model, we performed 222 different simulations, each involving the removal of a main component. For each damage scenario simulation, we analysed joint displacements, support reactions and element internal forces at different points on the element cross-sections. The entire process (including modifying the base finite element model and post-processing results) was implemented using a MATLAB56 code.\n\nThe finite element model of the scaled bridge was implemented in DIANA FEA software57. The bridge was modelled using a total of 2,798 two-node, three-dimensional Timoshenko/Mindlin57 (shear-deformable) beam elements with six degrees of freedom per node. Geometric and mechanical nonlinearities were considered. The former considers possible member and system instabilities, whereas the latter considers the nonlinear stress–strain characteristics of the material derived from tensile tests (Supplementary Information Section 1). We assumed58 a Poisson’s ratio of 0.3 and a density of 7,850 kg m−3. Each structural member was discretized into at least five beam finite elements. All joints were modelled as rigid. The boundary conditions and load arrangement reproduce those of the experimental campaign (see Extended Data Figs. 2 and 6).\n\nBefore running all simulations, we validated the base finite element model by comparing the results of the laboratory tests with the results of simulations for the undamaged state and the nine damage scenarios tested in the laboratory (Fig. 2f, Supplementary Information Section 2 and Supplementary Video 3). Specifically, vertical displacements of the lower joints and strains at the midpoints of diagonals and chords were used for the validation (Fig. 2f). The differences in structural response variables (for example, internal forces) between damaged and undamaged states define the way in which loads are redistributed. Two complementary approaches were used to characterize the resistance mechanisms for each damage scenario:\n\n1. We can conceptually understand the changes in the response after damage (for example, after the loss of a diagonal component) by analysing the structure, without the damaged component, subjected to joint forces that would counteract the axial force carried by the component before its removal (Supplementary Information Section 2). This provides a sound conceptual picture of the resistance mechanisms activated in each damage scenario and informs the interpretation of the bulk of data provided by the simulation campaign. 2. We also followed a quantitative and systematic approach to reveal and track the changes caused by each damage scenario and to support the definition of ALPs and resistance mechanisms. For this purpose, the following set of key performance indicators were defined: a. Increment of vertical displacements, ΔDz, at truss joints: $$\\Delta {Dz}_{i,j}={Dz}_{i,j}^{{\\rm{damaged}}}-{Dz}_{j}^{{\\rm{undamaged}}}$$ in which the indices refer to the ith damage scenario and jth measurement point. b. Increment of member axial forces, ΔFx: $${\\Delta Fx}_{i,k}={Fx}_{i,k}^{{\\rm{damaged}}}-{Fx}_{k}^{{\\rm{undamaged}}}$$ in which the indices refer to the ith damage scenario and kth structural member. c. Maximum increment of member bending moments, ΔMyz The norm of the bending moment at a given member cross-section j of a member k is expressed as: $${Myz}_{k,j}=\\sqrt{{My}_{k,j}^{2}+{Mz}_{k,j}^{2}}$$ The increments of bending moment at the start section, j_start, and at the end section, j_end, of the structural member k, for each damage scenario i, are: $${\\Delta Myz}_{i,k,{\\rm{j}}\\_{\\rm{start}}}={Myz}_{i,k,{\\rm{j}}\\_{\\rm{start}}}^{{\\rm{damaged}}}-{Myz}_{k,{\\rm{j}}\\_{\\rm{start}}}^{{\\rm{undamaged}}}$$ $${\\Delta Myz}_{i,k,{\\rm{j}}\\_{\\rm{end}}}={Myz}_{i,k,{\\rm{j}}\\_{\\rm{end}}}^{{\\rm{damaged}}}-{Myz}_{k,{\\rm{j}}\\_{\\rm{end}}}^{{\\rm{undamaged}}}$$ Finally, the performance indicator is defined as follows: $$\\Delta {Myz}_{i,k}=\\left\\{\\begin{array}{ll}{\\Delta Myz}_{i,k,{\\rm{j}}\\_{\\rm{end}}} & {\\rm{if}}\\,| {\\Delta Myz}_{i,k,{\\rm{j}}\\_{\\rm{end}}}| > | {\\Delta Myz}_{i,k,{\\rm{j}}\\_{\\rm{start}}}| \\\\ {\\Delta Myz}_{i,k,{\\rm{j}}\\_{\\rm{start}}} & {\\rm{otherwise}}\\end{array}\\right.$$ d. Increment of joint support vertical reactions, ΔRz:\n\n$${\\Delta Rz}_{i,j}={Rz}_{i,j}^{{\\rm{damaged}}}-{Rz}_{j}^{{\\rm{undamaged}}}$$\n\nin which the indices refer to the ith damage scenario and jth support point.\n\nFor interpreting the results, we grouped the damage scenarios according to the types of structural member. Each group was also divided into subgroups, which are labelled according to their position:\n\n1. Removal of chord segments: south lower (SL), north lower (NL), south upper (SU) and north upper (NU). 2. Removal of diagonals: south (S) and north (N). 3. Removal of verticals: south (S) and north (N). 4. Removal of horizontal bracings: south to north lower (SNL), north to south lower (NSL), south to north upper (SNU) and north to south upper (NSU). 5. Removal of vertical bracings: north upper to south lower (NU-SL) and north lower to south upper (NL-SU). 6. Removal of stringers: south (S) and north (N). 7. Removal of transversal beams: lower (L) and upper (U).\n\nWe assessed the effect of each group of damage scenarios on the different performance indicators of each group of members. The values of each performance indicator were represented in three-dimensional illustrations of the bridge and in heat map charts (Extended Data Fig. 7). In the latter, colour temperature and intensity show the magnitude of the indicator for each group of bridge members (horizontal axis) and for each damage scenario (vertical axis). These heat maps are key to identifying patterns and variations along consecutive structural members and across different damage scenarios.\n\nCombining these conceptual and quantitative approaches, we analysed changes in the deformed shape as well as changes in the magnitude of axial forces, bending moments and reactions, to reveal meaningful ALP patterns for each group of damage scenarios.\n\nSimulations for studying the evolution of ALPs\n\nWe studied the way in which internal forces are redistributed when external loads increase until the collapse of the bridge. This redistribution is a consequence of the propagation of failures within the whole structural system.\n\nIn this case, we carried out nonlinear analyses of a set of ten representative damage scenarios up to the collapse of the bridge. The numerical strategy uses arc-length control on the equilibrium path to be able to track the structural behaviour including consecutive local buckling of structural members. Otherwise, the simulations use the same basic modelling assumptions described in the previous section. The ten damage scenarios (Fig. 4) include three different upper and lower chord failures near the centre of the span (six scenarios) and two different failures of diagonals and verticals near the supports (four scenarios).\n\nWe validated the collapse simulations by comparing measurements from the last experimental test, in which loads were increased until collapse, with results from a simulation of the same scenario (Extended Data Fig. 8 and Supplementary Information Section 3).\n\nIn this part of the research, we analysed the information at two levels:\n\n1. At a global level, we plotted force–displacement curves for the entire structural system for each of the ten representative scenarios. In these curves, displacement corresponds to the vertical displacement measured at mid-span. The force is shown as an overload factor, which represents the multiplier of the reference load of 80 kN used for operational conditions. These curves (Fig. 4) provide information about the evolution of the global stiffness of the structure. They also show the occurrence of local instabilities as snap-through/snap-back events on the equilibrium path, as well as the degradation of global stiffness until collapse. 2. At a local level, we defined two new sets of performance indicators that can be compared to understand the progression of damage and changes to the load-redistribution patterns within the system: a. The first indicators characterize changes that occur under the effect of the reference operational load of 80 kN. Specifically, they quantify the differences in internal forces (IF), that is, axial forces or bending moments, between the undamaged and initial damaged condition, normalized by the applied load (that is, 80 kN). This allows analysing changes in load-redistribution patterns independently of the total applied load to the bridge. The sign of the indicator is defined as positive if there is an increase in the magnitude of the load (regardless of whether it is tension or compression), whereas if the total magnitude of the indicator decreases, the index will be negative, indicating an unloading or decrease in demand, for both axial forces and bending moments: $$\\Delta {{\\rm{IF}}}_{\\text{1st line}}={\\rm{abs}}\\left(\\frac{{{\\rm{IF}}}_{{\\rm{Damaged}}(80{\\rm{kN}})}-{{\\rm{IF}}}_{{\\rm{Undamaged}}(80{\\rm{kN}})}}{80{\\rm{kN}}}\\right)\\times {\\rm{sgn}}$$ $${\\rm{with}}\\,\\left\\{\\begin{array}{c}{\\rm{sgn}}=1\\,{\\rm{if}}\\;{\\rm{abs}}({{\\rm{IF}}}_{{\\rm{Damaged}}}) > {\\rm{abs}}({{\\rm{IF}}}_{{\\rm{Undamaged}}})\\\\ {\\rm{sgn}}=-\\,1\\,{\\rm{if}}\\;{\\rm{abs}}({{\\rm{IF}}}_{{\\rm{Damaged}}}) < {\\rm{abs}}({{\\rm{IF}}}_{{\\rm{Undamaged}}})\\end{array}\\right.$$ b. The second indicator pertains to the collapse condition. It represents the same differences but, in this case, they are normalized by the collapse load obtained from the simulation considering initial damage:\n\n$$\\Delta {{\\rm{IF}}}_{{\\rm{Collapse}}}={\\rm{abs}}\\left(\\frac{{{\\rm{IF}}}_{{\\rm{Damaged(collapse)}}}-{{\\rm{IF}}}_{{\\rm{Undamaged(collapse)}}}}{\\text{Collapse load}}\\right)\\times {\\rm{sgn}}$$\n\n$${\\rm{with}}\\,\\left\\{\\begin{array}{c}{\\rm{sgn}}=1\\,\\text{if abs}\\,({{\\rm{IF}}}_{{\\rm{Damaged}}}) > {\\rm{abs}}({{\\rm{IF}}}_{{\\rm{Undamaged}}})\\\\ {\\rm{sgn}}=-\\,1\\,\\text{if abs}({{\\rm{IF}}}_{{\\rm{Damaged}}}) < {\\rm{abs}}({{\\rm{IF}}}_{{\\rm{Undamaged}}})\\end{array}\\right.$$\n\nWe extracted conclusions from both approaches to characterize the evolution of ALPs for different damage scenarios (Extended Data Fig. 9) and to identify the mechanisms that are activated by the propagation of failures until collapse.",
      "url": "https://www.nature.com/articles/s41586-025-09300-8",
      "source": "Nature",
      "published": "2025-09-04",
      "sentiment_score": 0.85,
      "reasoning": "The article reports a significant scientific and engineering advancement in understanding the latent resistance mechanisms of steel truss bridges after critical failures. This research, involving detailed experimental tests and computational simulations on scaled specimens, demonstrates how load redistribution enables structural resilience. The findings have broad implications for infrastructure safety and resilience, potentially benefiting public safety and engineering practices worldwide. The article is focused, detailed, and presents a clear breakthrough in structural engineering technology.",
      "category": "Technology",
      "personality_title": "New study reveals how steel truss bridges stay strong after damage",
      "personality_presentation": "**Context**\nSteel truss bridges are important structures that carry trains and vehicles. When parts of these bridges fail, it can be dangerous, but engineers have noticed that some bridges can still hold up due to hidden ways they share the load. Understanding these hidden resistance methods can help improve bridge safety.\n\n**What happened**\nScientists created a smaller model of a real steel truss railway bridge and tested it in a laboratory. They applied loads similar to real train weights and then deliberately removed or damaged key parts to see how the bridge reacted. Alongside these experiments, they ran computer simulations to study how forces moved around the damaged areas.\n\n**Impact**\nThe study showed that even after critical parts fail, steel truss bridges have built-in ways to transfer loads to other components, preventing sudden collapse. This load redistribution helps the structure stay stable longer. The research provides detailed data and new methods to predict how bridges behave after damage, which is valuable for engineers working to design safer bridges and assess existing ones.\n\n**What's next step**\nThe findings can be used to improve bridge design standards and safety inspections. Future work might focus on applying this knowledge to full-size bridges and exploring how to strengthen bridges to better use these latent resistance mechanisms during emergencies.\n\n**One-sentence takeaway**\nCareful tests and simulations reveal how steel truss bridges naturally share loads after damage, helping them resist collapse and stay safe.\n\n",
      "personality_title_fr": "Une nouvelle étude révèle comment les ponts en treillis d'acier restent solides après des dommages",
      "personality_presentation_fr": "**Contexte**\nLes ponts en treillis d'acier sont des structures importantes qui supportent trains et véhicules. Lorsque certaines parties de ces ponts tombent en panne, cela peut être dangereux, mais les ingénieurs ont remarqué que certains ponts peuvent encore tenir grâce à des façons cachées de répartir la charge. Comprendre ces méthodes cachées peut aider à améliorer la sécurité des ponts.\n\n**Ce qui s'est passé**\nDes scientifiques ont créé un modèle réduit d'un vrai pont en treillis d'acier pour chemin de fer et l'ont testé en laboratoire. Ils ont appliqué des charges similaires au poids réel des trains puis ont délibérément enlevé ou endommagé des parties clés pour observer la réaction du pont. En parallèle, ils ont réalisé des simulations informatiques pour étudier comment les forces se déplaçaient autour des zones endommagées.\n\n**Impact**\nL'étude a montré que même après la défaillance de parties cruciales, les ponts en treillis d'acier ont des moyens intégrés de transférer les charges vers d'autres composants, évitant un effondrement soudain. Cette redistribution des charges aide la structure à rester stable plus longtemps. La recherche fournit des données détaillées et de nouvelles méthodes pour prédire le comportement des ponts après un dommage, ce qui est précieux pour les ingénieurs qui conçoivent des ponts plus sûrs et évaluent ceux déjà existants.\n\n**Prochaine étape**\nLes résultats peuvent être utilisés pour améliorer les normes de conception des ponts et les inspections de sécurité. Les travaux futurs pourraient se concentrer sur l’application de ces connaissances aux ponts grandeur nature et sur la façon de renforcer les ponts pour mieux utiliser ces mécanismes de résistance cachés en cas d'urgence.\n\n**Résumé en une phrase**\nDes tests précis et des simulations montrent comment les ponts en treillis d'acier partagent naturellement les charges après un dommage, ce qui les aide à résister à l'effondrement et à rester sûrs.\n\n",
      "personality_title_es": "Nuevo estudio revela cómo los puentes de celosía de acero mantienen su resistencia tras daños",
      "personality_presentation_es": "**Contexto**\nLos puentes de celosía de acero son estructuras importantes que soportan trenes y vehículos. Cuando algunas partes de estos puentes fallan, puede ser peligroso, pero los ingenieros han observado que algunos puentes pueden seguir resistiendo gracias a formas ocultas de repartir la carga. Entender estos métodos ocultos puede ayudar a mejorar la seguridad de los puentes.\n\n**Qué pasó**\nCientíficos crearon un modelo a escala de un puente ferroviario de celosía de acero y lo probaron en un laboratorio. Aplicaron cargas similares al peso real de los trenes y luego retiraron o dañaron partes clave a propósito para ver cómo reaccionaba el puente. Junto con estos experimentos, realizaron simulaciones por computadora para estudiar cómo se movían las fuerzas alrededor de las áreas dañadas.\n\n**Impacto**\nEl estudio mostró que incluso después de que fallan partes críticas, los puentes de celosía de acero tienen formas integradas de transferir cargas a otros componentes, evitando un colapso repentino. Esta redistribución de cargas ayuda a que la estructura se mantenga estable por más tiempo. La investigación ofrece datos detallados y nuevos métodos para predecir cómo se comportan los puentes después de un daño, lo que es valioso para los ingenieros que diseñan puentes más seguros y evalúan los existentes.\n\n**Próximo paso**\nLos hallazgos pueden usarse para mejorar las normas de diseño y las inspecciones de seguridad de puentes. El trabajo futuro podría enfocarse en aplicar este conocimiento a puentes a tamaño real y explorar cómo fortalecerlos para aprovechar mejor estos mecanismos latentes de resistencia durante emergencias.\n\n**Resumen en una frase**\nPruebas cuidadosas y simulaciones revelan cómo los puentes de celosía de acero comparten cargas tras daños, ayudándolos a resistir colapsos y mantenerse seguros.\n\n",
      "image_url": "public/images/news_image_Latent-resistance-mechanisms-of-steel-truss-bridge.png",
      "image_prompt": "A detailed, warm-toned painting of a sturdy, intricately interconnected steel truss bridge model resting on simple supports, with delicate symbolic beams and braces glowing softly to represent strength and resilience; subtle, abstract lines flow around the structure illustrating the invisible forces and load redistributions, all rendered in natural muted blues, grays, and earth tones without any human figures or text."
    },
    {
      "title": "Analog optical computer for AI inference and combinatorial optimization",
      "summary": "Nature, Published online: 03 September 2025; doi:10.1038/s41586-025-09430-zAn analog optical computer that combines analog electronics, three-dimensional optics, and an iterative architecture accelerates artificial intelligence inference and combinatorial optimization in a single platform, paving a promising path for faster and sustainable computing.",
      "content": "Experimental set-up\n\nThe key components of our experimental set-up are shown in Fig. 1a and Extended Data Fig. 1.\n\nOptical subsystem\n\nThe optical subsystem performs matrix–vector multiplication. The basic components are the optical sources (input vector), a system of fan-out optics to project the light onto the modulator matrix and a system of fan-in optics to project the light onto a photodetector array (output vector). The corresponding schematic is shown in Extended Data Fig. 2.\n\nThe incoherent light sources are an array of 16 independently addressable microLEDs. Each microLED is driven with a bias current and an offset voltage. The variable value is encoded by the light intensity, with a value of zero corresponding to the microLED bias point. Mathematical positive values are represented by microLED drive currents greater than the bias value. Negative values are represented by drive currents less than the bias value. The diameter of each emitter is 50 μm and the pitch is 75 μm. The sources are fabricated in gallium nitride wafers on a sapphire substrate and the die is wire-bonded onto a printed circuit board (Fig. 1c). The emission spectrum is centred at 520 nm with a full-width of half-maximum of 35 nm and the operational −3-dB bandwidth is 200 MHz at 20 mA, see Supplementary Fig. 1.\n\nAfter the sources, there is a polarizing beamsplitter (PBS). From this point, there are two equivalent optical paths in this set-up. Each path performs two functions: first, they allow us to use both polarizations of the unpolarized light output; second, they allow us to perform non-negative and non-positive multiplications with only intensity modulation. Each path contains one amplitude modulator matrix and one photodetector array. The modulator matrix is a reflective parallel-aligned nematic liquid-crystal SLM. We refer to the first part of the optical system as the fan-out system. The task of this fan-out system is to image the microLEDs onto the SLM, where the weights are displayed, and to spread the light horizontally into lines. The microLEDs are arranged in a one-dimensional line (let this be the y axis) and are imaged onto the SLM using a 4F system composed of a high-numerical-aperture (Thorlabs TL10X-2P, numerical aperture 0.5, ×10 magnification, 22-mm field number) collection objective and a lower-numerical-aperture lens group composed of 2 achromatic doublets with combined focal length 77 mm. There is a cylindrical lens, Thorlabs LJ1558L1, in infinity space of this 4F system. This lens adds defocus to the image of the source array on the SLM but only in the x direction, so that the projected light pattern is a set of long horizontal lines, one per microLED. Each matrix element occupies a patch of 12 (height) × 10 (width) pixels of the modulator array. An 8-bit look-up table is used to linearize the SLM response as a function of grey level.\n\nThe SLM is imaged onto the photodetector array using a 4F system (the fan-in system). The first lens group of the fan-in is the same as the second lens group of the fan-out system as this is in double pass. From here, the light is directed towards the intended photodetector array through a second PBS. The light from each column of the SLM is collected by an array of 16 silicon photodetectors to perform the required summation operation. The active area of each element is 3.6 × 0.075 mm2. The photodetectors are on a pitch of 0.125 mm. The operation bandwidth is 490 MHz at −10 V measured at 600 nm.\n\nAnalog electronic subsystem\n\nAfter the photodetector array, the signals are in the analog electronic domain. The photocurrents from each photodetector element are amplified by a linear trans-impedance amplifier (Analog Devices MAX4066). Each trans-impedance amplifier provides 25-kΩ gain and is characterized by an input referred noise of \\(3\\,{\\rm{pA}}\\,\\sqrt{\\text{Hz}}\\) and has differential outputs. The corresponding 2 sets (1 per photodetector board) of 16 differential pairs of signals are fed to the main boards where the per-channel nonlinear operation and other analog electronic processing is carried out. Each of the 16 signals sees the following circuitry: (1) a variable gain amplifier (VGA; Texas Instruments VCA824) to allow the input signal range to be set and equalized across channels; (2) a difference amplifier to perform the operation of subtracting the negative input signal from the positive one and achieve signed voltages (signed multiplications); (3) a VGA that adds and subtracts signals from the described path, referred to as gradient term, to the annealing and momentum terms, as per equation (1), while providing a common gain control to all these paths; (4) an electronic switch (ADG659) to open and close the loop to set and reset the solving state; (5) a buffer amplifier to distribute the signal to the gradient, annealing and momentum paths; (6) a bipolar differential pair to implement the tanh nonlinearity; (7) a VGA to adjust the signal level between the nonlinearity and the required voltage and current onto the microLED alternating-current input circuit. Both the annealing and momentum paths have VGAs with a common external control so that we can implement time-varying annealing and momentum schedules.\n\nEach channel also has an offset to the common control signal added to allow minor adjustment or correction of channel-to-channel variations. The other VGAs are set with digital-to-analog converters controlled over an inter-integrated circuit (I2C) bus. This allows slower control at per-experiment timescales.\n\nNonlinearity\n\nThe per-channel nonlinear function is an approximation to a tanh. This is shown in Supplementary Fig. 5d. The system is designed so that all signals follow the same path through the solver. For ML workloads, the input domain of the tanh function is unrestricted by hardware; there are no gain variations across channels. The trained weights and equilibrium model input ensure that signals evolve accurately. For optimization workloads, binary and continuous variables require different handling in hardware. Here we set the gain after the trans-impedance amplifier and before the tanh nonlinearity to be lower for continuous variables than for binary variables. This adjustment ensures that the input domain of the nonlinearity results in a more linear output for continuous variables than for binary variables.\n\nEvaluation of matrix–vector multiplication accuracy\n\nWe characterized and calibrated the key opto-electronic and electronic components to equalize the gain of each AOC path. For example, we calibrate the optical paths by applying a set of 93 reference matrices and for each we digitally compute the result of the vector–matrix product. We then adjust the gain per channel slightly so that, averaged over the set of 93 computed vectors, the AOC result is as close as possible to the digital result.\n\nFollowing this, the accuracy of the matrix–vector multiplication is characterized using the same 93 reference matrices on each SLM and measuring the output of the system, shown in Supplementary Fig. 3a. For each reference matrix in the set, we calculate the MSE between the known and the measured output. The mean MSE across all dot products is 5.5 × 10−3, and the matrix–vector multiplication MSE as a function of matrix (instance) is shown in Supplementary Fig. 3b. For these experiments, we configure the system in open-loop mode without feedback and turn off the annealing and momentum paths.\n\nML methods\n\nTraining and digital twin\n\nIn commercial deployments, training consumes less than 10% of the energy and, hence, is not targeted by the AOC. The equilibrium models are trained through our digital twin, which is based on equation (2). In the digital domain during training, the convergence criterion is set to ∣∣s t+1 − s t ∣∣ < ε, with ε = 10−3. The AOC-DT models up to seven non-idealities measured on the AOC device; each non-ideality can be switched on and off (Supplementary Fig. 5). The AOC-DT is implemented as a Pytorch module with the weight matrix W and bias terms b, as well as the gain β as trainable parameters. The weight matrix is normalized to fulfil ∥W∥ ∞ = 1 throughout training to simulate the passive SLM. The numeric scale of the matrix is instead modelled by the gain β. This separation of scale is necessary as several nonlinear non-idealities occur between the matrix multiplication and the gain in equation (2), as discussed in Supplementary Information section D.\n\nThe weight matrix is initialized with the default Pytorch initialization for a 16 × 16 matrix, the bias term is initialized to 0 and β is initialized at 1. We trained all models with a batch size of B = 8, at a learning rate of η = 3 × 10−4 for MNIST and Fashion-MNIST and η = 7 × 10−4 for regression tasks. We used the Adam optimizer50. In all cases, models are trained end-to-end, with the equilibrium-section trained through our AOC-DT using the implicit gradient method17, which avoids storing activations for the fixed-point iterations. This decouples memory cost from iteration depth as intermediate activations do not need to be stored. In all experiments, the α gain in equation (2) is set to 0.5 to strike a balance between overall signal amplitude and speed of convergence. Low α values cause the signal to be too weak, resulting in a low signal-to-noise ratio (Supplementary Information section D).\n\nInference and export to the AOC\n\nOnce training has completed, the weight matrix W is quantized to signed 9-bit integers using\n\n$$W\\approx \\frac{\\text{max}(W)}{255}\\,\\text{clamp}\\,{\\left[\\text{round}\\,\\left(\\frac{W}{\\text{max}(W)}\\times 255\\right)\\right]}_{\\text{min}=-256}^{\\text{max}=255}\\,=\\,\\frac{\\text{max}(W)}{255}{W}_{{\\rm{Q}}},$$ (4)\n\nwith the rounded and clamped matrix on the right-hand side being the quantized weight matrix W Q . Whenever we report AOC-DT results, we report results obtained with the quantized matrix.\n\nExporting trained models to the AOC requires several further steps. First, the model inputs x and the bias term b need to be condensed into a single vector b AOC = b + x followed by clamp to ensure the values fit into the dynamic range of the AOC device (Supplementary Information section D). Second, as the optical matrix multiplication is implemented using SLMs, elements of the weight matrix are bounded by one such that all quantization-related factors disappear. However, the original maximum element of the matrix max(W) needs to be re-injected, which we achieve via the β gain in equation (2), approximately restoring the original matrix W.\n\nThe quantized matrix is split into positive and negative parts, \\({W}_{{\\rm{Q}}}={W}_{{\\rm{Q}}}^{+}-{W}_{{\\rm{Q}}}^{-}\\), and each part is displayed on its respective SLM.\n\nAOC sampling and workflow\n\nEach classification instance (that is, MNIST or Fashion-MNIST test image) is run once on the AOC, and the fixed point is sampled at the point marked in Extended Data Fig. 3 after a short 2.5-μs cooldown window after the switch is closed, as shown in Extended Data Fig. 5a,b. The sampling window extends over 40 samples at 6.25 MHz, corresponding to 6.4 μs. This ensures that the search of fixed points for the equilibrium models happens entirely in the analog domain. Once sampled, we digitally project the vector into the output space. For classification, the input is projected from 784 to 16 dimensions, the output is projected from 16 to 10 classes. The label is then determined by the index of the largest element in the output vector (argument-max). For regression tasks, the IP and OP layers transform a scalar to 16 dimensions and back, respectively. The MSE results in Fig. 2c were obtained by averaging over 11 repeats for each input. This means that we restart the solution process 11 times, including the sampling window, and average the resulting latent fixed-point vectors. Importantly, the solve-to-solve variability appears to be centred close to the curve produced by the AOC-DT, enabling us to average this variability out (Supplementary Fig. 6).\n\nThe 4,096-weight ensemble model\n\nWe can expand the model sizes supported by the hardware by using an ensemble of small models that fit on it. These smaller 256-weight models are independent at inference time but are trained jointly by receiving slices 16-sized slices of a larger input vector and stacking their outputs before the OP. To scale to a 4,096-weight equilibrium model, we expand the input space from 16 to 16 × 16 = 4,096 dimensions and the output space from 10 to 10 × 16 = 160 dimensions. The IP matrix is consequently a 784 × 4,096-shaped matrix and the OP matrix is shaped 160 × 10. MNIST or Fashion-MNIST images are scaled to the range [−1, 1] and, projected to 4,096 dimensions and split into 16 slices of 16 dimensions. Each of the 16 equilibrium models then runs its respective slice of input vectors to a fixed-point. Once all 16 models are run on the AOC, we concatenate outputs and project them into the 10-dimensional output space where the largest dimension determines the predicted cipher.\n\nNonlinear regression\n\nThe first curve (I) is a Gaussian rescaled such that the Gaussian curve approximately stretches from −1 to 1, \\({f}_{{\\rm{I}}}(x)=2{{\\rm{e}}}^{-{x}^{2}/2{\\sigma }^{2}}-1\\) for σ = 0.25 and x ∈ [−1, 1]. The second curve (II) is given by \\({f}_{{\\rm{II}}}(x)=\\sqrt{| x| }\\,\\sin (3{\\rm{\\pi }}x)\\). For training sets, we choose 10,000 equidistant points x i in the range [−1, 1] whereas for test regression datasets, we choose 200 points randomly x i ≈ U([−1, 1]).\n\nError estimation\n\nFor regression tasks, we concatenate the 40 samples from all 11 repeats and calculate the standard deviation per point on the curve.\n\nClassification datasets\n\nWe trained the MNIST and Fashion-MNIST models on 48,000 images from their respective training set, validated on a set of 12,000 images and tested them on the full test set comprising 10,000 images.\n\nError estimation\n\nFor experimental results, the error bars in Fig. 2d were estimated using a Bayesian approach for the decision variable c t ∈ {0, 1, …, 9} for each sample t along the sampling window per image. We assume an uninformative prior p(c t ) = beta(1, 1), which we then update with the observed number of correct decisions n success and failures n failure over the sampling window. The variance of the conjugate posterior of a beta distribution is given by \\(\\mathrm{Var}({c}_{t}| {n}_{\\mathrm{success}},{n}_{{\\rm{failure}}})=\\frac{(1+{n}_{\\mathrm{success}})(1+{n}_{\\mathrm{failure}})}{{(2+{n}_{\\mathrm{success}}+{n}_{\\mathrm{failure}})}^{2}(3+{n}_{\\mathrm{success}}+{n}_{\\mathrm{failure}})}\\). We use this to estimate the variance and, by taking the square root, the standard deviation per input image. The dataset error bars are then estimated as the mean of the standard deviations over all members of the dataset.\n\nOptimization methods\n\nPositive and negative problem weights\n\nTo address optimization problems involving positive and negative weights on the AOC hardware, QUMO instances without linear terms can have up to eight variables, which applies to both transaction-settlement scenarios and reconstruction of one-dimensional line of the Shepp–Logan phantom image. The weight matrices are unsigned in synthetic QUMO and QUBO hardware benchmarks; hence the AOC hardware can accommodate up to 16-variable instances in the absence of linear terms. Such instance size difference arises because, when both positive and negative weights are present, non-idealities in the dual-SLM configuration reduce the accuracy of matrix–vector multiplication. To mitigate this, a single SLM is used to process both positive and negative weights, effectively halving the number of variables per instance.\n\nIndustrial optimization problems\n\nFor the transaction-settlement scenario and the Shepp–Logan phantom image slice, their 41-variable and 64-variable QUMO instances are decomposed into smaller 7-variable QUMO instances. For each of these subinstances, the 7 variables are connected with the rest of the variables via a linear vector b, which is incorporated into the quadratic matrix W via an additional binary variable. This decomposition is repeated for each subinstance and the linear vector b is updated at the end of each BCD iteration to create the next QUMO instance. Such an approach yields 8-variable QUMO instances and a single SLM is used to represent their positive and negative matrix elements, with analog electronics handling their subtraction, which effectively utilizes the full 16-variable capacity available in hardware. The required number of BCD iterations varies depending on factors such as the initial random state of the optimization instance variables, the selection of variable blocks among subinstances, and the order in which they are optimized.\n\nFor the one-dimensional Shepp–Logan phantom image, 12 out of 32 measurements are omitted, corresponding to a 37.5% data loss or a 1.6 undersampling (acceleration) rate. Although typical MRI acceleration ranges from 2 to 8, this rate is used here owing to the image’s non-smoothness at a 32-pixel resolution.\n\nBinary and continuous variables\n\nIn the AOC, binary variables are encoded using a hyperbolic tangent function, whereas continuous variables utilize the near-linear region of the function, connecting optimization variables to state variables via x = f(s). In simulations at scale with the AOC-DT, linear and sign functions are used for continuous and binary variables, respectively.\n\nHardware QUMO instances\n\nTo ensure that some variables take indeed continuous values in the global optimal solution, we plant random continuous values and generate synthetic 16-variable QUMO instances. As the number of continuous variables increases for a given problem size, the problem instances become computationally easier to solve. Consequently, we consider instances with up to eight continuous variables.\n\nHardware QUBO instances\n\nWe generate up to 8-bit dense and sparse instances. The sparse instances belong to the QUBO model on three-regular graphs that are NP-hard51, although NP-hardness does not imply that every random instance is difficult to solve. To make these instances more challenging to solve, we verify that their global objective minimizer is distinct from the signs of the principal eigenvector of the weight matrix52.\n\nQPLIB benchmark\n\nThe QPLIB is a library of quadratic programming instances23 collected over almost a year-long open call from various communities, with the selected instances being challenging for state-of-the-art solvers. As described in the main part of the paper, we consider only the hardest instances within the QPLIB:QBL class of problems, which contains instances with quadratic objective and linear inequality constraints. The QPLIB:QCBO class of problems, which contains instances with quadratic objective and linear equality constraints, and the QPLIB:QBN class of problems, which contains QUBO instances, are considered in Supplementary Information section G.5.\n\nAOC-DT operation and parameters\n\nThe distinction of the AOC-DT algorithm is the simultaneous inclusion of both momentum and annealing terms, which markedly improves the performance of the standard steepest gradient-descent method on non-convex optimization problems. Typically, multiple hyperparameters need to be calibrated for heuristic methods to achieve their best performance in solving optimization problems. We consider \\(\\alpha (t)=1-\\widehat{\\alpha }(t)\\), where \\(\\widehat{\\alpha }(t)\\) is a linearly decreasing function from some initial value α 0 to 0 over time. From the hardware perspective, such an annealing schedule provides an explicit stopping criteria, which is an advantage for an all-analog hardware implementation as it avoids the complexity of multiple intermediate readouts that stochastic heuristic approaches suffer from53. In principle, the three main parameters {α 0 , β, γ} of the AOC fixed-point update rule need to be adjusted for each optimization instance. In our simulations, we notice that the algorithm is less sensitive to the momentum parameter value, whereas the α 0 and β values substantially affect the solution quality. We further perform a linear stability analysis of the algorithm to evaluate reasonable exploration regions for these two parameters and find that by scaling the β parameter as β = β 0 /λ largest , where λ largest is the largest eigenvalue of the weight matrix W, we get scaled parameters β 0 and α 0 being in a similar optimal unit range across a wide range of problems.\n\nWe design a two-phase approach for the AOC-DT to operate similar to a black-box solver that can quickly adjust the critical parameters within the given time limit. During the ‘exploration’ phase, we evaluate the relative algorithm performance across a vast range of parameters (α 0 , β 0 ). A subset of ‘good’ parameters is then passed for more extensive investigation in the ‘deep search’ phase (Supplementary Information section G.1).\n\nWe note that for two QPLIB:QUMO instances, namely, 5,935 and 5,962, we developed a pre-processing technique that greedily picks variables with the highest impact on the objective functions and considers their possible values, which is accounted in the reported time speed-up of the AOC-DT.\n\nCompeting solvers\n\nFor a fair comparison, we ensure that all methods use similar computing resources. Although the implementation of GPU- or central-processing-unit-based solvers can require highly varying engineering efforts, we try to estimate the cost of running solvers on the hardware, on which they are designed to run, and vary the time limit across solvers accordingly to ensure similar cost per solver run. In what follows, the Julia-based AOC-DT runs on a GV100 GPU for 5–300 s per instance across all benchmarks. In the case of Gurobi, our licence allows us to use only up to eight cores, and its time to achieve the best solution for the first time is used (not the time to prove its optimality).\n\nMore details about the AOC hardware and the AOC-DT performance on different optimization instances are provided in Supplementary Information section G.5.",
      "url": "https://www.nature.com/articles/s41586-025-09430-z",
      "source": "Nature",
      "published": "2025-09-04",
      "sentiment_score": 0.85,
      "reasoning": "The article reports a significant breakthrough in analog optical computing that accelerates AI inference and combinatorial optimization with a single platform. This advancement has broad implications for faster and more sustainable computing, which can impact many sectors relying on AI and optimization. The article provides detailed technical context and evaluation, demonstrating the substance and real-world potential of the technology.",
      "category": "Technology",
      "personality_title": "New analog optical computer speeds up AI tasks and optimization problems",
      "personality_presentation": "**Context** – Computers that use light, instead of just electricity, can process information faster and use less energy. This technology is especially useful for tasks like artificial intelligence (AI) and solving complex optimization problems. However, combining these abilities in one machine has been a challenge.\n\n**What happened** – Scientists have built an analog optical computer (AOC) that uses light and analog electronics together to perform AI inference and optimization tasks on the same platform. The system uses tiny light sources called microLEDs, special lenses, and liquid crystal modulators to multiply large sets of numbers quickly. The results are captured by photodetectors and processed by analog circuits. This setup was tested with common AI datasets like handwritten digits (MNIST) and fashion images and also applied to industrial optimization problems.\n\n**Impact** – This new computer can perform complex calculations much faster and more efficiently than traditional digital computers. It handles both AI prediction and hard optimization problems without needing separate machines. The use of light allows it to work quickly with low energy, which could make future computing devices faster and greener. The researchers also showed that the system can be scaled up by combining small models to handle bigger tasks.\n\n**What's next step** – The team plans to improve the hardware and software to handle even larger and more complex problems. They will also explore more real-world applications, such as speeding up medical imaging and financial optimizations. Further development could lead to commercial devices that bring these advantages to everyday technology.\n\n**One-sentence takeaway** – A new analog optical computer uses light and electronics together to speed up AI and optimization tasks on one efficient platform.\n",
      "personality_title_fr": "Un nouvel ordinateur optique analogique accélère les tâches d'IA et d'optimisation",
      "personality_presentation_fr": "**Contexte** – Les ordinateurs utilisant la lumière, plutôt que seulement l'électricité, peuvent traiter l'information plus rapidement et avec moins d'énergie. Cette technologie est particulièrement utile pour l'intelligence artificielle (IA) et la résolution de problèmes complexes d'optimisation. Mais combiner ces capacités dans une seule machine était un défi.\n\n**Ce qui s'est passé** – Des chercheurs ont construit un ordinateur optique analogique (AOC) qui utilise la lumière et l'électronique analogique ensemble pour effectuer l'inférence IA et les tâches d'optimisation sur une même plateforme. Le système utilise de minuscules sources lumineuses appelées microLED, des lentilles spéciales et des modulateurs à cristaux liquides pour multiplier rapidement de grandes quantités de nombres. Les résultats sont captés par des photodétecteurs et traités par des circuits analogiques. Ce dispositif a été testé avec des bases de données classiques d'IA comme les chiffres manuscrits (MNIST) et des images de mode, ainsi que sur des problèmes industriels d'optimisation.\n\n**Impact** – Ce nouvel ordinateur peut effectuer des calculs complexes beaucoup plus rapidement et efficacement que les ordinateurs numériques traditionnels. Il gère à la fois la prédiction IA et les problèmes d'optimisation difficiles sans machines séparées. L'utilisation de la lumière permet un fonctionnement rapide et peu énergivore, ce qui pourrait rendre les futurs appareils informatiques plus rapides et plus écologiques. Les chercheurs ont aussi montré que le système peut être étendu en combinant plusieurs petits modèles pour traiter des tâches plus grandes.\n\n**Prochaine étape** – L'équipe prévoit d'améliorer le matériel et le logiciel pour traiter des problèmes encore plus grands et complexes. Ils exploreront également plus d'applications concrètes, comme l'accélération de l'imagerie médicale et des optimisations financières. Un développement futur pourrait conduire à des appareils commerciaux apportant ces avantages à la technologie courante.\n\n**Résumé en une phrase** – Un nouvel ordinateur optique analogique utilise la lumière et l'électronique pour accélérer les tâches d'IA et d'optimisation sur une plateforme efficace.\n",
      "personality_title_es": "Nueva computadora óptica analógica acelera tareas de IA y problemas de optimización",
      "personality_presentation_es": "**Contexto** – Las computadoras que usan luz, en lugar de solo electricidad, pueden procesar información más rápido y con menos energía. Esta tecnología es especialmente útil para tareas como la inteligencia artificial (IA) y resolver problemas complejos de optimización. Sin embargo, combinar estas capacidades en una sola máquina ha sido un reto.\n\n**Qué pasó** – Científicos construyeron una computadora óptica analógica (AOC) que usa luz y electrónica analógica juntas para realizar inferencia de IA y tareas de optimización en la misma plataforma. El sistema usa pequeñas fuentes de luz llamadas microLED, lentes especiales y moduladores de cristal líquido para multiplicar grandes conjuntos de números rápidamente. Los resultados se capturan con fotodetectores y se procesan con circuitos analógicos. Este sistema fue probado con conjuntos de datos comunes de IA como dígitos manuscritos (MNIST) y imágenes de moda, y también en problemas industriales de optimización.\n\n**Impacto** – Esta nueva computadora puede realizar cálculos complejos mucho más rápido y eficientemente que las computadoras digitales tradicionales. Maneja tanto predicciones de IA como problemas difíciles de optimización sin necesitar máquinas separadas. El uso de luz permite que funcione rápido y con baja energía, lo que podría hacer que los dispositivos futuros sean más rápidos y ecológicos. Los investigadores también mostraron que el sistema puede ampliarse combinando modelos pequeños para tareas mayores.\n\n**Próximo paso** – El equipo planea mejorar el hardware y software para manejar problemas aún más grandes y complejos. También explorarán más aplicaciones reales, como acelerar imágenes médicas y optimizaciones financieras. El desarrollo futuro podría llevar a dispositivos comerciales que ofrezcan estas ventajas en la tecnología diaria.\n\n**Resumen en una frase** – Una nueva computadora óptica analógica usa luz y electrónica para acelerar tareas de IA y optimización en una plataforma eficiente.\n",
      "image_url": "public/images/news_image_Analog-optical-computer-for-AI-inference-and-combi.png",
      "image_prompt": "A warm, detailed painting of a glowing grid of sixteen small, radiant microLED-like orbs aligned in a row, their light beams elegantly spreading into two symmetrical, translucent fan-shaped paths that pass through intricate, semi-transparent liquid crystal panels patterned with matrix-like pixel patches, converging onto delicate arrays of softly glowing photodetectors; the scene is bathed in gentle green and soft yellow hues symbolizing analog optical computation and harmonious AI inference."
    },
    {
      "title": "Spatial joint profiling of DNA methylome and transcriptome in tissues",
      "summary": "Nature, Published online: 03 September 2025; doi:10.1038/s41586-025-09478-xDNA-methylation and gene-expression profiling of tissue sections at near single-cell resolution can be used to create detailed spatial maps showing how methylation and transcription interact to shape cell identity and tissue development.",
      "content": "Tissue slide preparation\n\nMouse C57 embryo sagittal frozen sections (MF-104-11-C57 and MF-104-13-C57) were purchased from Zyagen. Freshly collected E11 or E13 mouse embryos were snap frozen in optimal cutting temperature (O.C.T.) compounds and sectioned at 7–10 μm thickness. Tissue sections were collected on poly-l-lysine-coated glass slides (Electron Microscopy Sciences, 63478-AS).\n\nJuvenile mouse brain tissue (P21) was obtained from the C57BL/6 mice housed in the University of Pennsylvania Animal Care Facilities under pathogen-free conditions. All procedures used were approved by the Institutional Animal Care and Use Committee.\n\nMice were euthanized at P21 using CO 2 inhalation, followed by transcranial perfusion with cold Dulbecco’s PBS (DPBS). After isolation, brains were embedded in Tissue-Tek O.C.T. compound and snap frozen on dry ice and a 2-methylbutane bath. Coronal cryosections of 8–10 μm were mounted on the back of Superfrost Plus microscope slides (Fisher Scientific, 12-550-15).\n\nPreparation of transposome\n\nUnloaded Tn5 transposome (C01070010) was purchased from Diagenode and the transposome was assembled following the manufacturer’s guidelines. The oligonucleotides used for transposome assembly were: Tn5ME-B, 5′-/5Phos/CATCGGCGTACGACTAGATGTGTATAAGAGACAG-3′; Tn5MErev, 5′-/5Phos/CTGTCTCTTATACACATCT-3′.\n\nDNA barcode sequences, DNA oligonucleotides and other key reagents\n\nDNA oligonucleotides used for PCR and library construction are shown in Supplementary Table 1. All DNA barcode sequences are provided in Supplementary Tables 2 (barcode A) and 3 (barcode B) and all other chemicals and reagents are listed in Supplementary Table 4.\n\nFabrication of the polydimethylsiloxane microfluidic device\n\nChrome photomasks were purchased from Front Range Photomasks, with a channel width of either 20 or 50 μm. The moulds for polydimethylsiloxane (PDMS) microfluidic devices were fabricated using standard photolithography. The manufacturer’s guidelines were followed to spin-coat SU-8-negative photoresist (Microchem, SU-2025 and SU-2010) onto a silicon wafer (WaferPro, C04004). The heights of the features were about 20 and 50 μm for 20- and 50-μm-wide devices, respectively. PDMS microfluidic devices were fabricated using the SU-8 moulds. We mixed the curing and base agents in a 1:10 ratio and poured the mixture onto the moulds. After degassing for 30 min, the mixture was cured at 66–70 °C for 2–16 h. Solidified PDMS was extracted from the moulds for further use. The detailed protocol for the fabrication and preparation of the PDMS device can be found in our previous research24.\n\nSpatial joint profiling of DNA methylation and RNA transcription\n\nFrozen tissue slides were quickly thawed for 1 min in a 37 °C incubator. The tissue was fixed with 1% formaldehyde in PBS containing 0.05 U ml−1 RNase inhibitor (Enzymatics) for 10 min and quenched with 1.25 M glycine for another 5 min at room temperature. After fixation, tissue was washed twice with 1 ml of DPBS–RNase inhibitor and cleaned with deionized H 2 O.\n\nThe tissue was subsequently permeabilized with 100 μl of 0.5% Triton X-100 plus 0.05 U ml−1 RNase inhibitor for 30 min at room temperature, then washed twice with 200 μl DPBS–RNase inhibitor for 5 min each. After permeabilization, the tissue was treated with 100 μl of 0.1 N HCl for 5 min at room temperature to disrupt histones from the chromatin, then washed twice with 200 μl of wash buffer (10 mM Tris-HCl pH 7.4, 10 mM NaCl, 3 mM MgCl 2 , 1% BSA and 0.1% Tween 20) plus 0.05 U ml−1 RNase inhibitor for 5 min at room temperature. Next, 50 μl of transposition mixture (5 μl of assembled transposome, 16.5 μl of 1× DPBS, 25 μl of 2× Tagmentation buffer, 0.5 μl of 1% digitonin, 0.5 μl of 10% Tween 20, 0.05 U ml−1 RNase inhibitor (Enzymatics) and 1.87 μl nuclease-free water) was added and incubated at 37 °C for 60 min. After 60 min incubation, the first round of transposition mixture was removed and a second round of 50 μl of fresh transposition mixture was added and incubated for another 60 min at 37 °C. To stop the transposition, 200 μl of 40 mM EDTA with 0.05 U ml−1 RNase inhibitor was added with incubation for 5 min at room temperature. After that, 200 μl 1× NEB3.1 buffer plus 1% RNase inhibitor was used to wash the tissue for 5 min at room temperature. The tissue was then washed again with 200 μl of DPBS–RNase inhibitor for 5 min at room temperature before proceeding with the in situ reverse transcription reaction.\n\nIn situ reverse transcription\n\nFor the in situ reverse transcription, the following mixture was added: 12.5 μl 5× reverse transcription buffer, 4.05 μl RNase-free water, 0.4 μl RNase inhibitor (Enzymatics), 1.25 μl 50% PEG-8000, 3.1 μl 10 mM dNTPs, 6.2 μl 200 U μl−1 Maxima H Minus Reverse Transcriptase, 25 μl 0.5× DPBS–RNase inhibitor and 10 μl 100 μM reverse transcription primer (biotinylated-dT oligo). The tissue was incubated for 30 min at room temperature, then at 45 °C for 90 min in a humidified container. After the reverse transcription reaction, tissue was washed with 1× NEB3.1 buffer plus 1% RNase inhibitor for 5 min at room temperature.\n\nIn situ barcoding\n\nFor in situ ligation with the first barcode (barcode A), the first PDMS chip was covered at the tissue ROI. For alignment purposes, a 10× objective (KEYENCE BZ-X800 fluorescence microscope, BZ-X800 Viewer Software) was used to take the bright-field image. The PDMS device and tissue slide were clamped tightly with a custom acrylic clamp. Barcode A was first annealed with ligation linker 1 by mixing 10 μl of 100 μM ligation linker, 10 μl of 100 μM individual barcode A and 20 μl of 2× annealing buffer (20 mM Tris-HCl pH 7.5–8.0, 100 mM NaCl 2 and 2 mM EDTA). For each channel, 5 μl of ligation master mixture was prepared with 4 μl of ligation mixture (27 μl T4 DNA ligase buffer, 0.9 μl RNase inhibitor (Enzymatics), 5.4 μl 5% Triton X-100, 11 μl T4 DNA ligase and 71.43 μl RNase-free water) and 1 μl of each annealed DNA barcode A (A1–A50, 25 μM). Vacuum was applied to flow the ligation master mixture into the 50 channels of the device and cover the ROI of the tissue, followed by incubation at 37 °C for 30 min in a humidified container. Then the PDMS chip and clamp were removed after washing the tissue with 1× NEB 3.1 buffer for 5 min. The slide was then washed with deionized water and dried using compressed air.\n\nFor in situ ligation with the second barcode (barcode B), the second PDMS chip was covered at the ROI and a bright-field image was taken with the 10× objective. An acrylic clamp was applied to clamp the PDMS and tissue slide together. Annealing of barcode B (B1–B50, 25 μM) and preparation of the ligation mixture are the same as barcode A. The whole device was incubated at 37 °C for 30 min in a humidified container. The PDMS chip and clamp were then removed, and the slide was washed with deionized water and dried using compressed air. A bright-field image was then taken for further alignment.\n\nReverse crosslinking\n\nFor tissue lysis, the ROI was digested with 100 μl of the reverse crosslinking mixture (0.4 mg ml−1 proteinase K, 1 mM EDTA, 50 mM Tris-HCl pH 8.0, 200 mM NaCl and 1% SDS) at 58–60 °C for 2 h in a humidified container. The lysate was then collected in a 0.2-ml PCR tube and incubated on a 60 °C shaker overnight.\n\ngDNA and cDNA separation\n\nFor gDNA and cDNA separation, the lysate was purified with Zymo DNA Clean and Concentrator kit and eluted with 100 μl nuclease-free water. Next, 40 μl of Dynabeads MyOne Streptavidin C1 beads were used and washed three times with 1× B&W buffer containing 0.05% Tween 20 (50 μl 1 M Tris-HCl pH 8.0, 2,000 μl 5 M NaCl, 10 μl 0.5 M EDTA, 50 μl 10% Tween 20 and 7,890 μl nuclease-free water). After washing, beads were resuspended in 100 μl of 2× B&W buffer (50 μl 1 M Tris-HCl pH 8.0, 2,000 μl 5 M NaCl, 10 μl 0.5 M EDTA and 2,940 μl nuclease-free water) containing 2 μl of SUPERase In RNase inhibitor, then mixed with the gDNA–cDNA lysate and allowed to bind for 1 h with agitation at room temperature. A magnet was then used to separate the beads, which bind to the cDNA that contains dT, from the supernatant that contains the gDNA.\n\ngDNA library generation\n\nSupernatant (200 μl) was collected from the above separation process for further methylated gDNA detection and library construction. Next, 1 ml of DNA binding buffer was added to the 200 μl supernatant and purified with the Zymo DNA Clean and Concentrator kit again, then eluted in 84 μl (3 × 28 μl) nuclease-free water. The NEBNext enzymatic methyl-seq conversion module (EM-seq) was then used to detect methylated DNA in the sample by converting unmethylated cytosines to uracil; the manufacturer’s guidelines were followed. Then, 28 μl of DNA sample was aliquoted into each PCR tube, TET2 reaction mixture (10 μl TET2 reaction buffer containing reconstituted TET2 reaction buffer supplement, 1 μl oxidation supplement, 1 μl DTT, 1 μl oxidation enhancer and 4 μl TET2) was added to the DNA sample on ice. In brief, 5 μl of diluted 1:1,300 of 500 mM Fe (II) solution was added to the mixture and incubated for 1 h at 37 °C in a thermocycler. After the reaction, the sample was transferred to ice and 1 μl of stop reagent from the kit was added. The sample was then incubated for another 30 min at 37 °C. TET2 converted DNA was then purified with 90 μl of solid-phase reversible immobilization (SPRI) beads and eluted with 16 μl nuclease-free water. The thermocycler was preheated to 85 °C, 4 μl formamide was added to the converted DNA and incubated for 10 min at 85 °C in the preheated thermocycler. After the reaction, the heated sample was immediately placed on ice to maintain the open chromatin structure, then reagents from the kit were added (68 μl nuclease-free water, 10 μl APOBEC reaction buffer, 1 μl BSA and 1 μl APOBEC) to deaminate unmethylated cytosines to uracil for 3 h at 37 °C in a thermocycler. Deaminated DNA was then cleaned up using 100 μl (1:1 ratio) of SPRI beads and eluted in 20 μl nuclease-free water.\n\nSplint ligation\n\nThe gDNA tube was heat-shocked for 3 min at 95 °C and immediately put on ice for 2 min. Then, 10 μl of 0.75 μM pre-annealed Splint Ligate P5 (SLP5) adapter was added. This adapter was diluted from a 12 μM stock, which contained 6 μl of 100 μΜ SLP5RC oligo, 8.4 μl of 100 μΜ SLS5ME-A-H10 oligo, 5 μl of 10× T4 RNA Ligase Buffer and 30.6 μl nuclease-free water in a PCR tube that was incubated at 95 °C for 1 min, then gradually cooled by −0.1 °C s−1 to 10 °C on a thermocycler. Next, 80 μl of ligation master mixture was added to the gDNA tube at room temperature. The mixture contained 40 μl preheated 50% PEG-8000, 12.5 μl SCR buffer (666 mM Tris-HCl pH 8.0 and 132 mM MgCl 2 in nuclease-free water), 10 μl of 100 mM DTT, 10 μl of 10 mM ATP, 1.25 μl of 10,000 U ml−1 T4 PNK and 6.25 μl of 400,000 U ml−1 T4 ligase. The splint ligation mixture was then splinted into five 0.2-ml PCR tubes, 20 μl per tube. The tubes were shaken at 1,000 rpm for 10 s and spun down, then incubated for 45 min at 37 °C, followed by 20 min at 65 °C to inactivate the ligase. For splint ligation indexing PCR, 80 μl of the PCR reaction mixture was mixed in each splint ligated tube. The mixture contained 20 μl 5× VeraSeq GC Buffer, 4 μl 10 mM dNTPs, 3 μl VeraSeq Ultra Enzyme, 5 μl 20× EvaGreen dye, 2 μl of 10 μM N501 primer and 2 μl of 10 μM N70X-HT primer (Supplementary Table 1). The mixture was then aliquoted into a clean PCR tube with 50 μl volume and run on a thermocycler with the setting below, 98 °C for 1 min, then cycling at 98 °C for 10 s, 57 °C for 20 s and 72 °C for 30 s, for 13–19 cycles, followed by 72 °C for 10 s. The reaction was removed once the quantitative PCR (qPCR) signal began to plateau. The amplified PCR products were pooled and purified with a 0.8× volume ratio of SPRI beads (bead-to-sample ratio) and the completed DNA library was eluted in 15 μl nuclease-free water.\n\ncDNA library generation\n\nThe separated beads containing cDNA were used for cDNA library generation. In brief, 400 μl of 1× B&W buffer with 0.05% Tween 20 was used to wash the beads twice. Then, the beads were washed once with 400 μl of 10 mM Tris-HCl pH 8.0 containing 0.1% Tween 20 for 5 min at room temperature. Streptavidin beads with bound cDNA molecules were placed on a magnetic rack and washed once with 250 μl nuclease-free water before being resuspended in a template switching oligonucleotide solution (44 μl 5× Maxima reverse transcription buffer, 44 μl of 20% Ficoll PM-400 solution, 22 μl of dNTPs, 5.5 μl of 100 mM template switch oligo, 11 μl Maxima H Minus reverse transcriptase, 5.5 μl of RNase inhibitor (Enzymatics) and 88 μl nuclease-free water). Resuspended beads were then incubated for 30 min with agitation at room temperature and for 90 min at 42 °C, with gentle agitation. After the reaction, beads were washed with 400 μl of 10 mM Tris pH 8.0 containing 0.1% Tween 20 and washed without resuspension in 250 μl nuclease-free water. Water was removed on the magnetic rack and the beads were resuspended in the PCR solution (100 μl of 2× Kappa Master mix, 8.8 μl of 10 μM primers 1 and 2 and 92.4 μl nuclease-free water). Next, the beads were mixed well and 50 μl of the PCR mixture was split into four 0.2-ml PCR tubes. The PCR programme was run as follows: 95 °C for 3 min and cycling at 98 °C for 20 s, 65 °C for 45 s and 72 °C for 3 min, for a total of five cycles, followed by 4 °C on hold. After five cycles of PCR reaction, four PCR tubes were placed on a magnetic rack and 50 μl of the clear PCR solution was transferred to four optical-grade qPCR tubes, adding 2.5 μl of 20× Evagreen dye to each tube. The sample was run on a qPCR machine with the following conditions: 95 °C for 3 min, cycling at 98 °C for 20 s, 65 °C for 20 s and 72 °C for 3 min, for 13–17 cycles, followed by 72 °C for 5 min. The reaction was removed once the qPCR signal began to plateau. The amplified PCR product was purified with a 0.8× volume ratio of SPRI beads and eluted in 20 μl nuclease-free water.\n\nA Nextera XT DNA Library Prep Kit was used for cDNA library preparation. In brief, 2 μl (2 ng) of purified cDNA (1 ng μl−1), 10 μl Tagment DNA buffer, 5 μl Amplicon Tagment mix and 3 μl nuclease-free water were mixed and incubated at 55 °C for 5 min. Then, 5 μl NT buffer was added to stop the reaction with incubation at room temperature for 5 min. PCR master mix (15 μl 2× N.P.M. Master mix, 1 μl of 10 μM P5 primer (N501) and 1 μl of 10 μM indexed P7 primer (N70X) and 8 μl nuclease-free water) was added. The PCR reaction was run with the following programme: 95 °C for 30 s, cycling at 95 °C for 10 s, 55 °C for 30 s, 72 °C for 30 s and 72 °C for 5 min, for a total of 12 cycles. The PCR product was then purified with a 0.7× ratio of SPRI beads and eluted in 15 μl nuclease-free water to obtain the cDNA library.\n\nLibrary quality check and next-generation sequencing\n\nAn Agilent Bioanalyzer D5000 ScreenTape was used to determine the size distribution and concentration of the library before sequencing. Next-generation sequencing was conducted on an Illumina NovaSeq 6000 sequencer and NovaSeq X Plus system (150 bp paired-end mode).\n\nData preprocessing\n\nFor RNA-sequencing data, Read 2 was processed to extract barcode A, barcode B and the UMIs. Using the STARsolo pipeline56 (v.2.7.10b), these processed data were mapped to the mouse genome reference (mm10). This step generated a gene matrix that captures both gene-expression and spatial-positioning information, encoded through the combination of barcodes A and B. The gene matrix was then imported into R for downstream spatial transcriptomic analysis using Seurat package (v.5.1.0)57.\n\nFor DNA-methylation data, adaptor sequences were trimmed before demultiplexing the FASTQ files using the combination of barcodes A and B. We used the BISulfite-seq CUI Toolkit (BISCUIT) (v.0.3.14)58 to align the DNA sequences to the mouse reference genome (mm10). Methylation levels at individual CG and CH sites were stored as continuous values between 0 and 1, representing the fraction of methylated reads after quality filtering. These processed CG–CH files were then analysed independently using the MethSCAn pipeline18 to identify VMRs59, defined as fused genome intervals with methylation-level variance in the top 2%. We used default parameter settings when running MethSCAn, and the MethSCAn filter min-sites parameter was determined from the read coverage knee plot (Extended Data Fig. 3a). The methylation levels and residuals of VMRs were then imported into R for downstream DNA-methylation analysis.\n\nClustering and data visualization\n\nWe mapped the exact location of pixels on the bright-field tissue image using a custom Python script (https://github.com/zhou-lab/Spatial-DMT-2024/tree/main/Data_preprocess/Image), before removing additional empty barcodes on the basis of read-count thresholds determined by the knee plot (Extended Data Fig. 3a). Clustering and data visualization were conducted using R in RStudio.\n\nFor RNA data, we used the SCTtransform function in the Seurat package (v.5.1.0), built using a regularized negative binomial model, for normalization and variance stabilization. Dimensionality reduction was performed using RunPCA function with the SCTtransformed assay. We then constructed the nearest-neighbour graph using the first 30 principal components with the FindNeighbors function and identified clusters with the default Leiden method in the FindClusters function. Finally, a UMAP embedding was computed using the same principal components with RunUMAP function.\n\nOwing to the inherent sparsity of DNA-methylation data, it is impractical to analyse methylation status solely at the individual CpG level. Binary information at sparse loci cannot be used directly to construct a feature matrix suitable for downstream analysis. In our study, we adopted the VMR framework, which divides the genome into variable-sized tiles and calculates the average methylation level across CpGs in each tile for each pixel18. This approach results in a continuous-valued matrix, in which rows correspond to pixels and columns represent genomic tiles, with values ranging from 0 to 1. VMR methylation levels and residuals were then imputed using the iterative principal component analysis approach as suggested in the MethSCAn instructions. Initially, missing residual values were replaced with zero and missing methylation levels were replaced with the average values for that VMR interval. The principal component analysis approach was iteratively applied until updated values stabilized to a threshold. The imputed residual matrix for VMRs was then imported into the existing Seurat object as another modality. Similar to the RNA-clustering pipeline, dimensionality was reduced using the RunPCA function. The first ten principal components from the residual matrix were used for clustering and UMAP embedding.\n\nTo visualize clusters in their spatial locations, the SpatialDimPlot function was used after clustering on the basis of gene expression or VMR residuals. UMAP embedding was visualized with the DimPlot function. The FindMarkers function was applied to select genes and VMRs that were differentially expressed or methylated for each cluster. For spatial mapping of individual VMR methylation levels or gene expression, we applied the smoothScoresNN function from the FigR package60. The SpatialFeaturePlot function was then used to visualize VMR methylation levels and gene expression across all pixels. To illustrate the relationships between clustering results from different modalities, we generated the confusion matrix and alluvial diagram using the pheatmap and ggalluvial R package61.\n\nIntegrative analysis of DNA methylation and RNA data\n\nTo integrate spatial DNA methylation and RNA data, WNN analysis in Seurat was applied using the FindMultiModalNeighbors function19. On the basis of the WNN graph, clustering, UMAP embedding and spatial mapping of identified clusters were performed for integrated visualization.\n\nFor the integration of spatial transcriptomics data of E11 and E13 mouse embryos, the top 3,000 integration features were selected, followed by the use of PrepSCTIntegration and IntegrateData functions to generate an integrated dataset. Similarly, to integrate with public single-cell transcriptomic data25,44, we first identified anchors using the FindIntegrationAnchors function in Seurat, followed by data integration using the IntegrateData function. To integrate DNA-methylation data, common VMRs between both developmental stages were obtained and the integrated CCA method from the IntegrateLayers function was used to join the methylation data from the two developmental stages. A Wilcoxon signed-rank test was performed to compare the methylation levels and gene-expression differences between the two time points.\n\nTF motif enrichment\n\nTo perform TF motif enrichment, we first used the MethSCAn diff function on distinct groups of cells to identify differentially methylated VMRs on the basis of the clustering assignment. The HOMER62 findMotifsGenome function was then applied to analyse the enrichment of known TF motifs using its default database. We followed the same parameter settings used in MethSCAn, with motif lengths of 5, 6, 7, 8, 9, 10, 11 and 12.\n\nCpGs enrichment analysis\n\nEnrichment analysis of individual CpGs in the differential regions (Fig. 5a,b) was performed using knowYourCG (https://github.com/zhou-lab/knowYourCG), which provides a comprehensive annotation database for each CpG, including chromatin states, TF binding sites, motif occurrences, PMD annotations and more. To avoid inflated odds ratios for high-coverage data, genomic uniformity was quantified using fold enrichment, defined as the ratio of observed overlaps to expected overlaps. The expected number of overlaps was calculated as: (number of CpGs sequenced × number of CpGs in the chromatin state feature)/total number of CpGs in the genome.\n\nCorrelation and GO enrichment analysis\n\nCorrelation analysis was performed for different clusters. We first used the findOverlaps function in GenomicRanges package (v.4.4)63 to map VMRs to overlapped genes. Then, the Pearson correlation test was applied to obtain the correlation between mapped genes and corresponding VMRs. The Benjamini–Hochberg procedure was used to adjust all P values.\n\nGO enrichment analysis was conducted using the enrichGO function from clusterProfiler package64 (v.4.2). For GO enrichment in the comparative analysis of E11 and E13 mouse embryos, the FindMarkers function in Seurat package was used to find differential genes and VMRs in the same cluster from integrated data across two developmental stages. Differentially upregulated genes (false discovery rate ≤ 0.05) with demethylated VMRs were used for the GO analysis.\n\nReporting summary\n\nFurther information on research design is available in the Nature Portfolio Reporting Summary linked to this article.",
      "url": "https://www.nature.com/articles/s41586-025-09478-x",
      "source": "Nature",
      "published": "2025-09-04",
      "sentiment_score": 0.85,
      "reasoning": "The article reports a significant scientific advancement in spatial joint profiling of DNA methylation and transcriptome at near single-cell resolution, enabling detailed spatial maps of gene expression and epigenetic regulation in tissue development. This breakthrough technology has broad implications for biology and medicine, providing a powerful tool for understanding cell identity and tissue development with potential to impact health research and disease understanding on a large scale. The article provides substantial technical detail and context, demonstrating the substance and significance of the achievement.",
      "category": "Technology",
      "personality_title": "New method maps DNA and gene activity together in tissues at tiny scale",
      "personality_presentation": "**Context** – Scientists want to understand how genes work inside tissues and how chemical changes to DNA, called methylation, affect cells. These changes help control which genes are turned on or off. Studying both gene activity and DNA methylation in the exact places they happen inside tissues is very hard.\n\n**What happened** – Researchers developed a new technique that can look at both DNA methylation and gene activity in tissue samples with almost single-cell detail. They tested this method on mouse embryos and brain tissue. Using special microscopes and tiny devices, they prepared tissue slices and labeled DNA and RNA molecules with unique barcodes. Then, they used advanced machines to read both the gene activity and the DNA methylation from the same spots in the tissue.\n\n**Impact** – This method creates detailed maps showing how gene activity and DNA methylation work together in space. It helps scientists see how cells develop and what makes them different from each other inside tissues. This is important because it can improve our understanding of development and diseases, like cancer, where gene control goes wrong.\n\n**What's next step** – The researchers plan to use this method on more tissue types and at different stages of development. It could also be applied to human samples to better understand diseases. The technique might help find new targets for treatments by revealing how gene regulation changes in illness.\n\n**One-sentence takeaway** – Scientists have created a powerful new way to study gene activity and DNA methylation together in tissues, revealing how cells develop and function with great detail.\n",
      "personality_title_fr": "Une nouvelle méthode cartographie l'ADN et l'activité génétique ensemble dans les tissus à très petite échelle",
      "personality_presentation_fr": "**Contexte** – Les scientifiques cherchent à comprendre comment les gènes fonctionnent dans les tissus et comment les modifications chimiques de l'ADN, appelées méthylation, influencent les cellules. Ces modifications contrôlent quels gènes sont activés ou désactivés. Étudier à la fois l'activité génétique et la méthylation de l'ADN dans leurs emplacements précis dans les tissus est très difficile.\n\n**Ce qui s'est passé** – Des chercheurs ont développé une nouvelle technique capable d'examiner à la fois la méthylation de l'ADN et l'activité génétique dans des échantillons de tissus avec une précision proche de la cellule unique. Ils ont testé cette méthode sur des embryons et des tissus cérébraux de souris. Grâce à des microscopes spéciaux et des dispositifs microscopiques, ils ont préparé des tranches de tissu et marqué les molécules d'ADN et d'ARN avec des codes-barres uniques. Ensuite, ils ont utilisé des machines avancées pour lire à la fois l'activité des gènes et la méthylation de l'ADN à partir des mêmes endroits dans le tissu.\n\n**Impact** – Cette méthode crée des cartes détaillées montrant comment l'activité génétique et la méthylation de l'ADN fonctionnent ensemble dans l'espace. Elle aide les scientifiques à voir comment les cellules se développent et ce qui les différencie dans les tissus. C'est important car cela améliore la compréhension du développement et des maladies, comme le cancer, où le contrôle des gènes est perturbé.\n\n**Prochaine étape** – Les chercheurs prévoient d'utiliser cette méthode sur plus de types de tissus et à différents stades du développement. Elle pourrait aussi être appliquée à des échantillons humains pour mieux comprendre les maladies. La technique pourrait aider à trouver de nouvelles cibles pour des traitements en révélant comment la régulation des gènes change dans la maladie.\n\n**Phrase clé** – Les scientifiques ont créé une nouvelle méthode puissante pour étudier ensemble l'activité génétique et la méthylation de l'ADN dans les tissus, révélant avec précision comment les cellules se développent et fonctionnent.\n",
      "personality_title_es": "Nuevo método mapea ADN y actividad genética juntos en tejidos a escala muy pequeña",
      "personality_presentation_es": "**Contexto** – Los científicos quieren entender cómo funcionan los genes dentro de los tejidos y cómo los cambios químicos en el ADN, llamados metilación, afectan a las células. Estos cambios controlan qué genes se activan o desactivan. Estudiar tanto la actividad genética como la metilación del ADN en los lugares exactos donde ocurren dentro de los tejidos es muy difícil.\n\n**Qué pasó** – Investigadores desarrollaron una nueva técnica que puede observar tanto la metilación del ADN como la actividad genética en muestras de tejido con un detalle casi a nivel de célula única. Probaron este método en embriones y tejido cerebral de ratón. Usaron microscopios especiales y dispositivos muy pequeños para preparar rebanadas de tejido y etiquetar las moléculas de ADN y ARN con códigos de barras únicos. Luego, usaron máquinas avanzadas para leer tanto la actividad genética como la metilación del ADN en los mismos lugares del tejido.\n\n**Impacto** – Este método crea mapas detallados que muestran cómo la actividad genética y la metilación del ADN trabajan juntas en el espacio. Ayuda a los científicos a ver cómo se desarrollan las células y qué las hace diferentes dentro de los tejidos. Esto es importante porque puede mejorar la comprensión del desarrollo y de enfermedades como el cáncer, donde el control de los genes falla.\n\n**Próximo paso** – Los investigadores planean usar este método en más tipos de tejidos y en diferentes etapas del desarrollo. También podría aplicarse a muestras humanas para entender mejor las enfermedades. La técnica podría ayudar a encontrar nuevos objetivos para tratamientos al mostrar cómo cambia la regulación genética en las enfermedades.\n\n**Conclusión en una frase** – Los científicos han creado una forma poderosa de estudiar juntos la actividad genética y la metilación del ADN en tejidos, revelando con gran detalle cómo las células se desarrollan y funcionan.\n",
      "image_url": "public/images/news_image_Spatial-joint-profiling-of-DNA-methylome-and-trans.png",
      "image_prompt": "A detailed, warm-toned painting of a delicate mouse embryo and juvenile mouse brain represented as softly glowing, translucent layers of tissue slides, overlaid with intricate, flowing patterns of DNA strands and RNA ribbons weaving together, all framed by a subtle grid of microfluidic channels symbolizing precise scientific mapping and spatial profiling."
    },
    {
      "title": "3D-printing could make it easier to make large quantum computers",
      "summary": "As quantum computers get larger, they may become truly useful – 3D-printing a key component of some quantum computers may make it easier to build larger arrays of qubits to make them more powerful",
      "content": "As quantum computers get larger, they may become truly useful – 3D-printing a key component of some quantum computers may make it easier to build larger arrays of qubits to make them more powerful\n\nAn ion trap used to corral two beryllium ions above a gold microchip Y. Colombe/NATIONAL INSTITUTE OF STANDARDS AND TECHNOLOGY/SCIENCE PHOTO LIBRARY\n\nTo make some quantum computers larger, and therefore more powerful, we may have to 3D-print them.\n\nCurrently, there is no consensus on the single best design for quantum computers, but researchers agree that to become unambiguously useful, quantum computers will have to be made larger. For those that use ions as quantum bits, or qubits, a key building block is called an “ion trap”. Hartmut Häffner at the University of California, Berkeley, and his colleagues have now developed a 3D-printing technique for miniaturised ion traps, which could make it easier to combine many of them into one large computer.\n\nAdvertisement\n\nThe purpose of an ion trap is right in its name: it confines ions in place and helps control their quantum states with electromagnetic fields, an essential condition for using ions to run calculations.\n\nFor their version, the researchers 3D-printed traps that were just a few hundred microns across. In extensive laboratory tests, these beat more conventional designs. They captured ions up to 10 times more efficiently and did so with shorter wait times from when the trap is turned on to when the ions can be used, says Häffner. “You can scale to an order of magnitude more qubits, and you can speed up things,” he says.\n\nTeam member Xiaoxing Xia at Lawrence Livermore National Laboratory in California says that 3D-printing is a perfect match for the problem at hand, because it can make small and complex objects with fewer restraints than methods more akin to chip manufacturing. This means the researchers could follow the success of their tiny ion trap with more innovative and novel designs. Team member Shuqi Xu, also at the University of California, Berkeley, says some are already in the works. “3D-printing lets you reimagine things to a large degree,” says Xia.\n\nSubscriber-only newsletter Sign up to Lost in Space-Time Untangle mind-bending physics, maths and the weirdness of reality with our monthly, special-guest-written newsletter. Sign up to newsletter\n\nThe methods currently used to make ion traps “suffer from complexity, inherent limitations and sometimes from low yield, high costs and bad reproducibility. It appears to me that the 3D-printing scheme could eventually overcome all these issues… which is in turn a key prerequisite for scalability quantum computing with trapped ions”, says Ulrich Poschinger at the Johannes Gutenberg University Mainz in Germany.\n\nXia says the team now wants to integrate optical components into their 3D-printed designs, such as miniaturised lasers that are necessary for quantum computing. Häffner adds that their tiny traps could help redesign mass spectrometers, which are ubiquitous tools in chemistry.",
      "url": "https://www.newscientist.com/article/2494821-3d-printing-could-make-it-easier-to-make-large-quantum-computers/?utm_campaign=RSS%7CNSNS&utm_source=NSNS&utm_medium=RSS&utm_content=home",
      "source": "New Scientist - Home",
      "published": "2025-09-03",
      "sentiment_score": 0.85,
      "reasoning": "The article reports a significant technological breakthrough in quantum computing by using 3D-printing to create more efficient ion traps, which could enable larger and more powerful quantum computers. This advancement has broad implications for the future of computing technology and related scientific fields, with detailed context and expert commentary provided.",
      "category": "Technology",
      "personality_title": "3D-printing boosts quantum computer design with better ion traps",
      "personality_presentation": "**Context** – Quantum computers use tiny units called qubits to process information in new ways. To become more useful, these computers need to get bigger and handle more qubits. One type of quantum computer uses ions, or charged atoms, held in place by devices called ion traps.\n\n**What happened** – Scientists at the University of California, Berkeley, developed a way to 3D-print very small ion traps. These new traps are just a few hundred microns wide and work better than traditional ones. They can catch ions up to 10 times more efficiently and get ready faster for quantum calculations.\n\n**Impact** – Making ion traps with 3D-printing allows for more flexible and complex designs. This could help build larger quantum computers with many more qubits, making them more powerful. The new traps also cost less and are easier to produce consistently. Experts say this method could solve many problems in scaling up quantum computers.\n\n**What's next step** – The research team plans to add tiny lasers and other optical parts directly into their 3D-printed ion traps. This integration will bring the technology closer to fully working quantum computers. The new traps might also improve other tools, like devices used in chemistry labs.\n\n**One-sentence takeaway** – Using 3D-printing to make better ion traps could speed up the creation of larger, more powerful quantum computers.\n",
      "personality_title_fr": "L’impression 3D améliore la conception des ordinateurs quantiques grâce à de meilleurs pièges à ions",
      "personality_presentation_fr": "**Contexte** – Les ordinateurs quantiques utilisent de petites unités appelées qubits pour traiter l’information différemment. Pour être plus utiles, ces ordinateurs doivent devenir plus grands et gérer plus de qubits. Un type d’ordinateur quantique utilise des ions, ou atomes chargés, maintenus en place par des dispositifs appelés pièges à ions.\n\n**Ce qui s’est passé** – Des scientifiques de l’Université de Californie à Berkeley ont développé une méthode pour imprimer en 3D des pièges à ions très petits. Ces nouveaux pièges font quelques centaines de microns de large et fonctionnent mieux que les modèles traditionnels. Ils peuvent capturer les ions jusqu’à 10 fois plus efficacement et sont prêts plus rapidement pour les calculs quantiques.\n\n**Impact** – L’impression 3D permet de créer des pièges à ions avec des formes plus complexes et flexibles. Cela pourrait aider à construire des ordinateurs quantiques plus grands avec beaucoup plus de qubits, les rendant plus puissants. Ces pièges coûtent aussi moins cher et sont plus faciles à produire de manière régulière. Des experts disent que cette méthode pourrait résoudre beaucoup de problèmes liés à l’agrandissement des ordinateurs quantiques.\n\n**Prochaine étape** – L’équipe de recherche prévoit d’intégrer des petits lasers et d’autres composants optiques directement dans leurs pièges à ions imprimés en 3D. Cette intégration rapprochera la technologie des ordinateurs quantiques entièrement fonctionnels. Ces nouveaux pièges pourraient aussi améliorer d’autres outils, comme des appareils utilisés dans les laboratoires de chimie.\n\n**Résumé en une phrase** – Utiliser l’impression 3D pour fabriquer de meilleurs pièges à ions pourrait accélérer la création d’ordinateurs quantiques plus grands et plus puissants.\n",
      "personality_title_es": "La impresión 3D mejora el diseño de computadoras cuánticas con mejores trampas de iones",
      "personality_presentation_es": "**Contexto** – Las computadoras cuánticas usan pequeñas unidades llamadas qubits para procesar información de nuevas maneras. Para ser más útiles, estas computadoras deben ser más grandes y manejar más qubits. Un tipo de computadora cuántica usa iones, o átomos cargados, que se mantienen en su lugar con dispositivos llamados trampas de iones.\n\n**Qué pasó** – Científicos de la Universidad de California en Berkeley desarrollaron una forma de imprimir en 3D trampas de iones muy pequeñas. Estas nuevas trampas tienen solo unos pocos cientos de micrones de ancho y funcionan mejor que las tradicionales. Pueden capturar iones hasta 10 veces más eficientemente y estar listas más rápido para cálculos cuánticos.\n\n**Impacto** – Hacer trampas de iones con impresión 3D permite diseños más flexibles y complejos. Esto podría ayudar a construir computadoras cuánticas más grandes con muchos más qubits, haciéndolas más potentes. Las nuevas trampas también cuestan menos y son más fáciles de producir de forma constante. Expertos dicen que este método podría resolver muchos problemas para aumentar el tamaño de las computadoras cuánticas.\n\n**Próximo paso** – El equipo planea añadir pequeños láseres y otras partes ópticas directamente en sus trampas de iones impresas en 3D. Esta integración acercará la tecnología a computadoras cuánticas completas. Las nuevas trampas también podrían mejorar otras herramientas, como aparatos usados en laboratorios de química.\n\n**Resumen en una frase** – Usar impresión 3D para hacer mejores trampas de iones podría acelerar la creación de computadoras cuánticas más grandes y potentes.\n",
      "image_url": "public/images/news_image_3D-printing-could-make-it-easier-to-make-large-qua.png",
      "image_prompt": "A warm, detailed painting of a delicate, intricately 3D-printed golden microchip structure gently holding two glowing blue ions suspended above it, surrounded by soft, flowing electromagnetic field lines symbolized as translucent ribbons of light, all set against a simple, natural-colored background that emphasizes innovation and precision in quantum technology."
    }
  ]
}