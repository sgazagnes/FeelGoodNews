{
  "personality": null,
  "timestamp": "2025-11-06T04:44:58.689074",
  "category": "Technology",
  "news_summary": "Today's technology news highlights advancements in brain-inspired AI, ethical and human-centered data development, and promising progress in quantum computing for superconductivity research.",
  "news_summary_fr": "L'actualité technologique d'aujourd'hui met en lumière les progrès de l'IA inspirée par le cerveau, le développement de données éthiques et centrées sur l'humain, ainsi que les avancées prometteuses de l'informatique quantique pour la recherche sur la supraconductivité.",
  "news_summary_es": "Las noticias tecnológicas de hoy destacan los avances en IA inspirada en el cerebro, el desarrollo de datos éticos y centrados en el ser humano y los prometedores progresos en computación cuántica para la investigación de la superconductividad.",
  "articles": [
    {
      "title": "Artificial neurons that behave like real brain cells",
      "summary": "USC researchers built artificial neurons that replicate real brain processes using ion-based diffusive memristors. These devices emulate how neurons use chemicals to transmit and process signals, offering massive energy and size advantages. The technology may enable brain-like, hardware-based learning systems. It could transform AI into something closer to natural intelligence.",
      "content": "Scientists at the USC Viterbi School of Engineering and the School of Advanced Computing have created artificial neurons that reproduce the intricate electrochemical behavior of real brain cells. The discovery, published in Nature Electronics, marks a major milestone in neuromorphic computing, a field that designs hardware modeled after the human brain. This advancement could shrink chip sizes by orders of magnitude, cut energy use dramatically, and push artificial intelligence closer to achieving artificial general intelligence.\n\nUnlike digital processors or earlier neuromorphic chips that only simulate brain activity through mathematical models, these new neurons physically reproduce how real neurons operate. Just as natural brain activity is triggered by chemical signals, these artificial versions use actual chemical interactions to start computational processes. This means they are not just symbolic representations but tangible recreations of biological function.\n\nA New Class of Brain-Like Hardware\n\nThe research, led by Professor Joshua Yang of USC's Department of Computer and Electrical Engineering, builds on his earlier pioneering work on artificial synapses more than a decade ago. The team's new approach centers on a device called a \"diffusive memristor.\" Their findings describe how these components could lead to a new generation of chips that both complement and enhance traditional silicon-based electronics. While silicon systems rely on electrons to perform computations, Yang's diffusive memristors use the motion of atoms instead, creating a process that more closely resembles how biological neurons transmit information. The result could be smaller, more efficient chips that process information the way the brain does and potentially pave the way toward artificial general intelligence (AGI).\n\nIn the brain, both electrical and chemical signals drive communication between nerve cells. When an electrical impulse reaches the end of a neuron at a junction called a synapse, it converts into a chemical signal to transmit information to the next neuron. Once received, that signal is converted back into an electrical impulse that continues through the neuron. Yang and his colleagues have replicated this complex process in their devices with striking accuracy. A major advantage of their design is that each artificial neuron fits within the footprint of a single transistor, whereas older designs required tens or even hundreds.\n\nIn biological neurons, charged particles known as ions help create the electrical impulses that enable activity in the nervous system. The human brain relies on ions such as potassium, sodium, and calcium to make this happen.\n\nUsing Silver Ions to Recreate Brain Dynamics\n\nIn the new study, Yang -- who also directs the USC Center of Excellence on Neuromorphic Computing -- used silver ions embedded in oxide materials to generate electrical pulses that mimic natural brain functions. These include fundamental processes like learning, movement, and planning.\n\n\"Even though it's not exactly the same ions in our artificial synapses and neurons, the physics governing the ion motion and the dynamics are very similar,\" says Yang.\n\nYang explains, \"Silver is easy to diffuse and gives us the dynamics we need to emulate the biosystem so that we can achieve the function of the neurons, with a very simple structure.\" The new device that can enable a brain-like chip is called the \"diffusive memristor\" because of the ion motion and the dynamic diffusion that occurs with the use of silver.\n\nHe adds, the team chose to utilize ion dynamics for building artificial intelligent systems \"because that is what happens in the human brain, for a good reason and since the human brain, is the 'winner in evolution-the most efficient intelligent engine.\"\n\n\"It's more efficient,\" says Yang.\n\nWhy Efficiency Matters in AI Hardware\n\nYang emphasizes that the issue with modern computing isn't lack of power but inefficiency. \"It's not that our chips or computers are not powerful enough for whatever they are doing. It's that they aren't efficient enough. They use too much energy,\" he explains. This is especially important given how much energy today's large-scale artificial intelligence systems consume to process massive datasets.\n\nYang goes on to explain that unlike the brain, \"Our existing computing systems were never intended to process massive amounts of data or to learn from just a few examples on their own. One way to boost both energy and learning efficiency is to build artificial systems that operate according to principles observed in the brain.\"\n\nIf you are looking for pure speed, electrons that run modern computing would be the best for fast operations. But, he explains, \"Ions are a better medium than electrons for embodying principles of the brain. Because electrons are lightweight and volatile, computing with them enables software-based learning rather than hardware-based learning, which is fundamentally different from how the brain operates.\"\n\nIn contrast, he says, \"The brain learns by moving ions across membranes, achieving energy-efficient and adaptive learning directly in hardware, or more precisely, in what people may call 'wetware'.\"\n\nFor example, a young child can learn to recognize handwritten digits after seeing only a few examples of each, whereas a computer typically needs thousands to achieve the same task. Yet, the human brain accomplishes this remarkable learning while consuming only about 20 watts of power, compared to the megawatts required by today's supercomputers.\n\nPotential Impact and Next Steps\n\nYang and his team see this technology as a major step toward replicating natural intelligence. However, he acknowledges that the silver used in these experiments is not yet compatible with standard semiconductor manufacturing processes. Future work will explore other ionic materials that can achieve similar effects.\n\nThe diffusive memristors are efficient in both energy and size. A typical smartphone may contain around ten chips, each with billions of transistors switching on and off to perform calculations.\n\n\"Instead [with this innovation], we just use a footprint of one transistor for each neuron. We are designing the building blocks that eventually led us to reduce the chip size by orders of magnitude, reduce the energy consumption by orders of magnitude, so it can be sustainable to perform AI in the future, with similar level of intelligence without burning energy that we cannot sustain,\" says Yang.\n\nNow that we have demonstrated capable and compact building blocks, artificial synapses and neurons, the next step is to integrate large numbers of them and test how closely we can replicate the brain's efficiency and capabilities. \"Even more exciting,\" says Yang, \"is the prospect that such brain-faithful systems could help us uncover new insights into how the brain itself works.\"",
      "url": "https://www.sciencedaily.com/releases/2025/11/251105050723.htm",
      "source": "Latest Science News -- ScienceDaily",
      "published": "2025-11-05",
      "sentiment_score": 0.9,
      "reasoning": "The article reports a significant breakthrough in neuromorphic computing where artificial neurons physically replicate real brain cell processes using ion-based diffusive memristors. This advancement promises major improvements in energy efficiency and chip size reduction, potentially transforming AI to be more brain-like and sustainable. The impact is broad, affecting AI development and computing technology at large, with detailed context and explanation of the innovation and its future implications.",
      "category": "Technology",
      "personality_title": "USC researchers create artificial neurons that work like real brain cells",
      "personality_presentation": "**Context** – Scientists are trying to build computer systems that work more like the human brain. These systems, called neuromorphic computers, aim to process information more efficiently by copying how brain cells communicate.\n\n**What happened** – Researchers at the University of Southern California (USC) built artificial neurons that mimic the way real brain cells send signals using chemicals. They used a special device called a diffusive memristor, which moves tiny particles called ions to create electrical pulses. This process closely copies how natural neurons work, unlike older computer chips that only simulate brain activity with math.\n\n**Impact** – This new technology can make computer chips much smaller and use far less energy. For example, each artificial neuron fits in the space of a single transistor, while older designs needed many times more room. Because it copies the brain’s chemical signals, it could help create smarter and more energy-efficient artificial intelligence that learns more like humans do.\n\n**What's next step** – The team plans to build larger systems using many of these artificial neurons to see how well they can replicate the brain’s abilities. They will also look for new materials that work better with existing chip-making methods to make this technology easier to produce.\n\n**One-sentence takeaway** – USC’s new artificial neurons use chemical signals like the brain to create smaller, more efficient chips that could improve future artificial intelligence systems.",
      "personality_title_fr": "Des chercheurs de l'USC créent des neurones artificiels qui fonctionnent comme de vraies cellules cérébrales",
      "personality_presentation_fr": "**Contexte** – Les scientifiques cherchent à construire des systèmes informatiques qui fonctionnent davantage comme le cerveau humain. Ces systèmes, appelés ordinateurs neuromorphiques, visent à traiter l'information plus efficacement en imitant la communication entre les cellules cérébrales.\n\n**Ce qui s'est passé** – Des chercheurs de l'Université de Californie du Sud (USC) ont fabriqué des neurones artificiels qui reproduisent la façon dont les cellules cérébrales réelles transmettent des signaux chimiques. Ils ont utilisé un dispositif spécial appelé memristor diffusif, qui déplace de minuscules particules appelées ions pour créer des impulsions électriques. Ce procédé imite de près le fonctionnement des neurones naturels, contrairement aux anciennes puces qui simulaient seulement l'activité cérébrale grâce aux mathématiques.\n\n**Impact** – Cette nouvelle technologie peut rendre les puces informatiques beaucoup plus petites et consommer beaucoup moins d'énergie. Par exemple, chaque neurone artificiel tient dans l'espace d'un seul transistor, alors que les anciens modèles en nécessitaient beaucoup plus. En copiant les signaux chimiques du cerveau, elle pourrait aider à créer une intelligence artificielle plus intelligente et plus économe en énergie, capable d'apprendre comme les humains.\n\n**Prochaine étape** – L'équipe prévoit de construire des systèmes plus grands avec plusieurs de ces neurones artificiels pour tester leur capacité à reproduire les fonctions du cerveau. Ils chercheront aussi de nouveaux matériaux compatibles avec les méthodes de fabrication actuelles pour faciliter la production.\n\n**Résumé en une phrase** – Les nouveaux neurones artificiels de l'USC utilisent des signaux chimiques comme le cerveau pour créer des puces plus petites et plus efficaces, capables d'améliorer les futures intelligences artificielles.",
      "personality_title_es": "Investigadores de USC crean neuronas artificiales que funcionan como células cerebrales reales",
      "personality_presentation_es": "**Contexto** – Los científicos intentan construir sistemas informáticos que funcionen más parecido al cerebro humano. Estos sistemas, llamados computadoras neuromórficas, buscan procesar la información de manera más eficiente imitando cómo se comunican las células cerebrales.\n\n**Qué pasó** – Investigadores de la Universidad del Sur de California (USC) crearon neuronas artificiales que imitan la forma en que las células cerebrales reales envían señales químicas. Usaron un dispositivo especial llamado memristor difusivo, que mueve pequeñas partículas llamadas iones para crear pulsos eléctricos. Este proceso copia muy bien el funcionamiento de las neuronas naturales, a diferencia de los chips antiguos que solo simulaban la actividad cerebral con matemáticas.\n\n**Impacto** – Esta nueva tecnología puede hacer que los chips sean mucho más pequeños y consuman mucha menos energía. Por ejemplo, cada neurona artificial cabe en el espacio de un solo transistor, mientras que los diseños anteriores necesitaban mucho más espacio. Al copiar las señales químicas del cerebro, podría ayudar a crear una inteligencia artificial más inteligente y eficiente en energía, que aprenda de manera similar a los humanos.\n\n**Próximo paso** – El equipo planea construir sistemas más grandes usando muchas de estas neuronas artificiales para ver qué tan bien pueden imitar las capacidades del cerebro. También buscarán nuevos materiales que funcionen mejor con los métodos actuales de fabricación de chips para facilitar su producción.\n\n**Resumen en una frase** – Las nuevas neuronas artificiales de USC usan señales químicas como el cerebro para crear chips más pequeños y eficientes que podrían mejorar los futuros sistemas de inteligencia artificial.",
      "image_url": "public/images/news_image_Artificial-neurons-that-behave-like-real-brain-cel.png",
      "image_prompt": "A detailed painting of a glowing network of delicate silver ion pathways weaving through a compact microchip shaped like a stylized brain, where tiny atomic particles flow like streams of light between neuron-shaped transistor outlines, all rendered in soft natural hues of silver, warm amber, and muted blues, evoking the harmony of biological and artificial intelligence working together."
    },
    {
      "title": "AI 'godmother' Fei-Fei Li says she is 'proud to be different'",
      "summary": "AI pioneer Professor Fei-Fei Li is set to receive a top engineering prize from the King for her contributions to the field.",
      "content": "AI 'godmother' Fei-Fei Li says she is 'proud to be different'\n\nProfessor Fei-fei Li is being honoured for her pioneering contributions to the advancement of artificial intelligence (AI)\n\nThey are being recognised for their contributions to the development of modern machine learning, a field that underpins the rapid advancement of AI.\n\nThose honoured alongside her are Prof Yoshua Bengio, Dr Bill Dally, Dr Geoffrey Hinton, Prof John Hopfield, Nvidia founder Jensen Huang and Meta's chief AI scientist Dr Yann LeCun.\n\nThe King will present the 2025 Queen Elizabeth Prize for Engineering to Prof Li and six others during a ceremony at St James's Palace.\n\nThe 'godmother' of AI, Professor Fei-Fei Li has told the BBC that being the only woman amongst seven pioneers of artificial intelligence being presented with a top engineering prize by the King today makes her \"proud to be different\".\n\nWho are the godparents of AI?\n\nDr Hinton, Prof Bengio and Yann LeCun, currently chief AI scientist at Meta have widely been recognised as the \"godfathers of AI\" since they were jointly awarded the 2018 Turing Award.\n\nThere is however only one so-called \"godmother\" of AI and Prof Li told the BBC she has grown to accept the moniker.\n\n\"I would not call myself godmother of anything,\" she said.\n\nShe said a few years ago when people started calling her that, she had to \"pause and recognise if I rejected this, it would miss an opportunity for women scientists and technologists to be recognised this way\".\n\n\"Because men are pretty easily called godfathers or founding fathers.\"\n\n\"For all the young women I work with and the generations of girls to come, I'm okay now accepting this title,\" she added.\n\nBorn in China, Prof Li emigrated to the US as a teenager and went on to excel in computer science. She is co-director for Stanford's Human-Centered AI Institute and co-founder and CEO of World Labs.\n\nIt is her work on ImageNet a project which enabled major advances in computer vision for which she is recognised.\n\nShe and her students created large-scale image recognition datasets upon which a lot of artificial intelligence technology is now built. It paved the way for computer vision – working out how computers could 'see'.\n\nShe says the importance of that data set \"open the floodgate of data-driven AI\".\n\nShe thinks the next AI milestone will come when it is able to interact with the world around it.\n\nThis ability was is \"innately important and native to animals and humans\", and if this could be unlocked in AI, it could \"superpower\" humans in many ways, \"including creativity, robotic learning, design and architecture\".\n\nThis will be the first time all seven laureates have come together in person.\n\nThe three \"godfathers\" have publicly stated opposing views on how dangerous AI could be.\n\nDr Hinton has repeatedly expressed serious concerns about the potential for AI to pose an \"extinction-level threat\". But Prof LeCun, who also works at Meta has written that apocalyptic warnings are overblown.\n\nProf Li says she takes a more \"pragmatic approach\" and says the disagreement amongst scientists is \"healthy\".\n\n\"We're used to even disagreement, and I think that's healthy. A topic as profound and impactful as AI requires a lot of healthy debate and public discourse.\n\n\"I think in the case of AI, both extreme rhetorics concern me…I have always advocated for a much more science based, pragmatic method in communicating and educating the public.\n\n\"So, yes, I would like to see our communication of AI to be much more moderated and grounded in facts and science instead of the extreme rhetorics\".\n\nThe Queen Elizabeth prize is awarded annually to engineers responsible for groundbreaking innovations which globally benefit humanity. Previous recipients include Sir Tim Berners Lee, the creator of the World Wide Web.\n\nLord Vallance, chair of the Queen Elizabeth Prize for Engineering Foundation, said the winners \"represent the very best of engineering,\" adding that their work \"demonstrates how engineering can both sustain our planet and transform the way we live and learn.\"\n\nAdditional reporting by Philippa Wain",
      "url": "https://www.bbc.com/news/articles/c5yp2vj92e9o?at_medium=RSS&at_campaign=rss",
      "source": "BBC News",
      "published": "2025-11-05",
      "sentiment_score": 0.85,
      "reasoning": "The article highlights Professor Fei-Fei Li receiving a prestigious engineering prize for pioneering contributions to AI, specifically in computer vision and large-scale datasets that have advanced the field significantly. This recognition underscores a major technological breakthrough with broad societal impact, including potential future benefits in creativity, robotics, and design. The story also emphasizes diversity and representation in STEM, inspiring future generations of women in technology.",
      "category": "Technology",
      "personality_title": "AI pioneer Fei-Fei Li honored for breakthrough work in computer vision",
      "personality_presentation": "**Context** – Artificial intelligence (AI) is a technology that helps computers learn and understand information like humans do. One important part of AI is computer vision, which allows machines to 'see' and recognize images. Professor Fei-Fei Li has played a key role in this area.\n\n**What happened** – In 2025, Professor Fei-Fei Li will receive the Queen Elizabeth Prize for Engineering, a top award given by the King. She is being recognized for creating ImageNet, a large collection of labeled images that helped computers learn to identify objects. She is the only woman among seven AI experts receiving this prize.\n\n**Impact** – Professor Li’s work on ImageNet opened new possibilities for AI by providing the data needed for machines to understand pictures. This has helped improve technologies like facial recognition, self-driving cars, and medical image analysis. Her recognition also highlights the importance of women in science and technology fields.\n\n**What's next step** – Professor Li believes the next big step for AI is teaching machines to interact with the world around them, like animals and humans do. This could improve creativity, learning, and robotics. The prize ceremony will bring together all seven winners, sparking more conversations about AI’s future.\n\n**One-sentence takeaway** – Professor Fei-Fei Li’s pioneering work in teaching computers to see has earned her a top engineering prize and inspires more diversity in AI innovation.",
      "personality_title_fr": "La pionnière de l'IA Fei-Fei Li récompensée pour ses avancées en vision par ordinateur",
      "personality_presentation_fr": "**Contexte** – L'intelligence artificielle (IA) est une technologie qui aide les ordinateurs à apprendre et comprendre comme les humains. La vision par ordinateur permet aux machines de 'voir' et reconnaître des images. La professeure Fei-Fei Li a joué un rôle clé dans ce domaine.\n\n**Ce qui s'est passé** – En 2025, la professeure Fei-Fei Li recevra le prix d'ingénierie Queen Elizabeth, une récompense prestigieuse remise par le roi. Elle est reconnue pour avoir créé ImageNet, une grande base d’images annotées qui a aidé les ordinateurs à identifier des objets. Elle est la seule femme parmi sept experts en IA récompensés.\n\n**Impact** – Le travail de Fei-Fei Li sur ImageNet a ouvert de nouvelles possibilités en IA en fournissant les données nécessaires à la compréhension des images par les machines. Cela a amélioré des technologies comme la reconnaissance faciale, les voitures autonomes et l'analyse médicale. Sa reconnaissance souligne aussi l'importance des femmes dans les sciences et technologies.\n\n**Prochaine étape** – La professeure Li pense que la prochaine grande avancée sera d’apprendre aux machines à interagir avec leur environnement, comme les humains et les animaux. Cela pourrait améliorer la créativité, l’apprentissage et la robotique. La cérémonie rassemblera les sept lauréats et favorisera les discussions sur l’avenir de l’IA.\n\n**Résumé en une phrase** – Le travail pionnier de Fei-Fei Li pour apprendre aux ordinateurs à voir lui vaut un grand prix d’ingénierie et encourage plus de diversité dans l’innovation en IA.",
      "personality_title_es": "La pionera de la IA Fei-Fei Li recibe premio por avances en visión por computadora",
      "personality_presentation_es": "**Contexto** – La inteligencia artificial (IA) es una tecnología que ayuda a las computadoras a aprender y entender como las personas. La visión por computadora permite que las máquinas 'vean' y reconozcan imágenes. La profesora Fei-Fei Li ha sido clave en este campo.\n\n**Qué pasó** – En 2025, la profesora Fei-Fei Li recibirá el Premio de Ingeniería Queen Elizabeth, un galardón importante entregado por el rey. Se la reconoce por crear ImageNet, una gran base de datos de imágenes etiquetadas que ayudó a las computadoras a identificar objetos. Es la única mujer entre siete expertos en IA premiados.\n\n**Impacto** – El trabajo de Fei-Fei Li con ImageNet abrió nuevas oportunidades en IA al proporcionar los datos necesarios para que las máquinas comprendan imágenes. Esto ha mejorado tecnologías como el reconocimiento facial, los autos autónomos y el análisis médico. Su reconocimiento también resalta la importancia de las mujeres en ciencia y tecnología.\n\n**Próximo paso** – La profesora Li cree que el siguiente gran avance será enseñar a las máquinas a interactuar con el mundo que las rodea, como hacen los humanos y animales. Esto podría mejorar la creatividad, el aprendizaje y la robótica. La ceremonia reunirá a los siete ganadores, promoviendo más conversaciones sobre el futuro de la IA.\n\n**Resumen en una frase** – El trabajo pionero de Fei-Fei Li para enseñar a las computadoras a ver le ha ganado un premio de ingeniería y fomenta mayor diversidad en la innovación de IA.",
      "image_url": "public/images/news_image_AI-godmother-Fei-Fei-Li-says-she-is-proud-to-be-di.png",
      "image_prompt": "A warm, detailed painting of a radiant, stylized lotus flower blooming at the center of a softly glowing neural network pattern, its petals subtly shaped like interconnected human eyes symbolizing computer vision, surrounded by seven gently illuminated orbs representing the seven AI pioneers, set against a calm, natural-toned background of flowing data streams resembling woven silk threads."
    },
    {
      "title": "Fair human-centric image dataset for ethical AI benchmarking",
      "summary": "Nature, Published online: 05 November 2025; doi:10.1038/s41586-025-09716-2The Fair Human-Centric Image Benchmark (FHIBE, pronounced ‘Feebee’)—an image dataset that implements best practices for consent, privacy, compensation, safety, diversity and utility—can be used responsibly as a fairness evaluation dataset for many human-centric computer vision applications.",
      "content": "Ethics statement: participants and consent/recruitment procedures\n\nData collection commenced after 23 April 2023, following Institutional Review Board approval from WCG Clinical (study number 1352290). All of the participants have provided their informed consent to the use of their data, and those who were image subjects further consented to have their identifiable images published.\n\nWe developed an informed consent form designed to comply with the EU’s GDPR46 and other similarly comprehensive data privacy regulations. Vendors were required to ensure that all image subjects (that is, both primary and secondary) provided signed informed consent forms when contributing their data. Vendors were also required to ensure that each image was associated with a signed copyright agreement to obtain the necessary IP rights in the images from the appropriate rightsholder. Only individuals above the age of majority in their country of residence and capable of entering into contracts were eligible to submit images.\n\nAll of the image subjects, regardless of their country of residence, have the right to withdraw their consent to having their images included in the dataset, with no impact to the compensation that they received for the images. This is a right that is not typically provided in pay-for-data arrangements nor in many data privacy laws beyond GDPR and GDPR-inspired regimes.\n\nData annotators involved in labelling or QA were given the option to disclose their demographic information as part of the study and were similarly provided informed consent forms giving them the right to withdraw their personal information. Some data annotators and QA personnel were crowdsourced workers, while others were vendor employees.\n\nTo validate English language proficiency, which was needed to understand the project’s instructions, terms of participation, and related forms, participants (that is, image subjects, annotator crowdworkers and QA annotator crowdworkers) were required to answer at least two out of three randomly selected multiple-choice English proficiency questions correctly from a question bank, with questions presented before project commencement. The questions were randomized to minimize the likelihood of sharing answers among participants. An example question is: “Choose the word or phrase which has a similar meaning to: significant” (options: unimportant, important, trivial).\n\nTo avoid possibly coercive data-collection practices, we instructed data vendors not to use referral programs to incentivize participants to recruit others. Moreover, we instructed them not to provide participants support (beyond platform tutorials and general technical support) in signing up for or submitting to the project. The motivation was to avoid scenarios in which the participants could feel pressured or rushed through key stages, such as when reviewing consent forms. We further reviewed project description pages to ensure that important disclosures about the project (such as the public sharing and use of the data collected, risks, compensation and participation requirements) were provided before an individual invested time into the project.\n\nImage collection guidelines\n\nImages and annotations were crowdsourced through external vendors according to extensive guidelines that we provided. Vendors were instructed to only accept images captured with digital devices released in 2011 or later, equipped with at least an 8-megapixel camera and capable of recording Exif metadata. Accepted images had to be in JPEG or TIFF format (or the default output format of the device) and free from post-processing, digital zoom, filters, panoramas, fisheye effects and shallow depth-of-field. Images were also required to have an aspect ratio of up to 2:1 and be clear enough to allow for the annotation of facial landmarks, with motion blur permitted only if it resulted from subject activity (for example, running) and did not compromise the ability to annotate them. Each subject was allowed to submit a maximum of ten images, which had to depict actual subjects, not representations such as drawings, paintings or reflections.\n\nSubmissions were restricted to images featuring one or two consensual image subjects, with the requirement that the primary subject’s entire body be visible (including the head, and a minimum of 5 body landmarks and 3 facial landmarks identifiable) in at least 70% of the images delivered by each vendor, and the head visible (with at least 3 facial landmarks identifiable) in all images. Vendors were also directed to avoid collecting images with third-party IP, such as trademarks and landmarks.\n\nTo increase image diversity, we requested that images ideally be taken at least 1 day apart and recommended that images submitted of a subject were taken over as wide a time span as possible, preferably at least 7 days apart. If images were captured less than 7 days apart, the subject had to be wearing different clothing in each image, and the images had to be taken in different locations and at different times of day. Our instructions to vendors requested minimum percentages for different poses to enhance pose diversity, but we did not instruct subjects to submit images with specific poses. Participants were permitted to submit previously captured images provided that they met all requirements.\n\nAnnotation categories and guidelines\n\nWe provided extensive annotation guidelines to data vendors that included examples and explanations. A complete list of the annotations, their properties (including whether they were multiple-choice), categories and annotation methods is provided in Supplementary Information A.\n\nA key component of our project was that most annotations were self-reported by the image subjects as they were best suited to provide accurate information about subject demographics and physical characteristics, interactions depicted and scene context. The only annotations that were not self-reported were those that could be objectively observed from the image itself and would benefit from the consistency offered by professional annotators (that is, pixel-level annotations, head pose and camera distance, as defined by the size of an image subject’s face bounding box). We also provided examples and guidance for subject–subject interactions, subject–object interactions and head pose based on the request of our data vendors due to ambiguities in those labels.\n\nWe included open-ended, free text options alongside closed-ended responses, enabling subjects to provide input beyond predefined categories. These open-ended responses were coded as ‘Not Listed’. For privacy reasons, we do not report the specific text provided by the subjects. This approach enabled subjects to express themselves more fully79,80, resulting in more accurate data and informing better question design for future data collection. Given the mutability of most attributes, annotations were collected on a per-image basis, except for ancestry.\n\nFor the pixel-level annotations, face bounding boxes were annotated following the protocol used for the WIDER FACE dataset81, a commonly used face detection dataset. Keypoint annotations were based on the BlazePose topology82, a composite of the COCO40, BlazePalm83 and BlazeFace84 topologies. While the 17-keypoint COCO topology is widely used in computer vision, it lacks definitions for hand and foot keypoints, making it less suitable for applications such as fitness compared to BlazePose. For person segmentation, we defined 28 semantic segmentation categories based on the most comprehensive categorical schemas for this task, including MHP (v.2.0)85, CelebAMask-HQ86 and Face Synthetics87. Finally, person bounding boxes were automatically derived from human segmentation masks by enclosing the minimum-sized box that contained the entirety of each person’s segmentation mask.\n\nEach annotator, QA annotator and QA specialist was assigned a unique identifier to link them to each annotation that they provided or reviewed, as well as any demographic information they chose to disclose. For annotation tasks involving multiple annotators, we provided the individual annotations from each annotator, rather than aggregated data. These annotations included those made before any vendor QA and those generated during each stage of vendor QA.\n\nFor our analyses, images with multiple annotations within a single attribute category (for example, ancestry subregion) are included in all relevant attribute value categories. For example, if an image subject is annotated with multiple ancestry subregions, the subject is counted in each of those subregions during analyses. Nested annotations—such as when a broad category is selected (for example, ‘Africa’ for ancestry)—are handled by counting the image subject in all corresponding subregions (for example, each subregion of ‘Africa’).\n\nQuality control and data filtering\n\nQuality control for images and annotations was conducted by both the vendors and our team. Vendor QA annotators handled the first round of checking images, annotations, and consent and IPR forms. For non-self-reported annotations, vendor QA workers were permitted to modify the annotation if incorrect. For imageable attributes (such as apparent eye colour, facial marks, apparent head hair type), they could provide their own annotations if they believed the annotations were incorrect, but this would not overwrite the original self-reported annotation (we report both annotations). Vendors were instructed not to QA non-imageable attributes (such as pronouns, nationality, natural hair colour), with the exception of height and weight if there were significant differences in the numbers for the same subject in images taken 48 h or less apart.\n\nMoreover, we developed and ran various automated and manual checks to further examine the images and annotations delivered by the vendors. Our automated checks verified image integrity (for example, readability), resolution, lack of post-processing artifacts and sufficient diversity among images of the same subject. They also assessed annotation reliability by comparing annotations to inferred data (for example, verifying that a scene labelled as ‘outdoor’ corresponds with outdoor characteristics in the image), checked for internal consistency (for example, ensuring body keypoints are correctly positioned within body masks), identified duplicates and checked the images against existing images available on the Internet. Moreover, the automated testing checked for CSAM by comparing image hashes against the database of known CSAM maintained by the National Center for Missing & Exploited Children (NCMEC).\n\nImages containing logos were automatically detected using a logo detector88 and the commercial logo detection API from Google Cloud Vision89. They were then excluded from FHIBE to avoid trademark issues. We used a detection score threshold of 0.6 to eliminate identified bounding boxes with low confidence, and the positive detection results were reviewed and filtered manually to avoid false positives. However, despite these efforts, logo detection remains a complex challenge due to the vast diversity of global designs, spatial orientation, partial occlusion, background artifacts and lighting variations. Even manual review can be inherently limited, as QA teams cannot be familiar with every logo worldwide and often face difficulty distinguishing between generic text and logos. Our risk-based approach to logo detection and removal was informed by the relatively low risk of IP harms posed by the inclusion of logos in our dataset. The primary concern is that individuals might mistakenly perceive a relationship between our dataset and the companies whose logos appear. However, this is mitigated by the academic nature of this publication and the clear disclosure of author and contributor affiliations.\n\nManual checks on the data were conducted predominantly by our team of QA specialists, as well as by authors. The QA specialists were a team of four contractors who worked with the authors to evaluate the quality of vendor-delivered data, and conduct corrections where needed. The QA specialists had a background in ML data annotation and QA work, and received training and extensive documentation regarding the quality standards and requirements for images and annotations for this project. Furthermore, they remained in direct contact with our team throughout the project, ensuring that they could clarify quality standards as needed.\n\nThe manual checks focused on ensuring the accuracy of annotations for imageable attributes, such as hair colour, scene context and subject interactions. Non-imageable attributes, representing social constructs, such as pronouns or ancestry, were not part of the visual content verification. Moreover, even though the probability of objectionable content (for example, explicit nudity, violence, hate symbols) was low given our sourcing method, instructions to data subjects and QA from vendors, we took the additional step of manually reviewing each image for such content given the public nature of the dataset.\n\nOverall, to arrive at the 10,318 images for the initial launch of FHIBE, we collected a total of 28,703 images from three data vendors. As the result of initial internal assessments, a set of 6,868 images were excluded due to issues with data quality and adherence to project specifications. Another 5,855 images were excluded for consent or copyright form issues. Of the remaining 15,980 images collected from vendors, approximately 0.07% were excluded for minor annotation errors (for example, missing skin colour annotations), 0.17% for offensive content (in free-text or visual content) and 0.01% for other reasons (for example, duplicated subject IDs) before the suspicious-pattern exclusions described in the following section.\n\nDetection and removal of suspicious images\n\nIt was difficult to determine whether the people who submitted the images were the same as the subjects in the image while respecting the privacy of the subjects. There can be fraudulent actors who submit images of other people without their consent to be compensated by data vendors. Given the public and consent-driven nature of our dataset, we did not rely exclusively on vendors to detect and remove suspicious images. We used a combination of automated and manual checks to detect and remove images where we had reason to suspect the data subject(s) might not be the individual who submitted the image. Combining automated and manual checks, we removed 3,848 images from 1,718 subjects from the dataset.\n\nFor automated checks, we used Web Detect from Google Cloud Vision API89 to identify and exclude images that could have been scraped from the Internet. This was a conservative check as images found online could still have been consensually submitted to our project by the image subject. However, given the importance of consent for our project, and the use of the dataset for evaluation, we excluded these images out of an abundance of caution.\n\nThis check resulted in removing 321 images, across 70 subjects, as we removed all the images for a given subject, as long as a single image was found online. However, there were some limitations to this automated approach. Vision API had a high false-positive rate: 62% for our task (that is, images that are visually similar, due to scene elements or popular landmarks). Google Web Detect returned limited results for images containing people and, in some cases, the returned matches focused on clothing items or the landmark. Furthermore, some social media images may not have been indexed by the Vision API because the websites required authentication.\n\nWe therefore also performed manual review methods for removing potentially suspicious images. Manual reviewers were instructed to track potentially suspicious patterns during their review of images and consent/copyright forms. For example, they were instructed to examine inconsistencies between self-reported and image metadata (for example, landmarks that contradicted the self-reported location). These patterns were later reviewed for exclusion by the research team.\n\nMoreover, one of our QA specialists developed a manual process to find additional online image matches. The QA specialist used Google Lens to identify the location of the image. For images with distinctive locations (for example, not generic indoor locations or extremely popular tourist locations), the QA specialist performed a time-limited manual search to try to find image matches online. While we were not able to apply this time-intensive process to every image, using this approach, we were able to assess the risk level of different qualitative suspicious patterns and make additional exclusions.\n\nAfter these exclusions, 2,017 subjects remained. From these subjects, we randomly sampled a set of 400 subjects and conducted the above manual QA process. In total, 14 subjects were found online while inspecting this sample, and we excluded them from the dataset. On the basis of this analysis, we estimated a baseline level of suspiciousness of 3.5 ± −1.7% with 95% confidence.\n\nIt is important to note that removing suspicious images also had an impact on the demographic distribution of subjects in the dataset (Supplementary Information I). We found that excluded images were more likely to feature individuals of older ages, with lighter skin tones and of Europe/Americas/Oceania ancestry. While it is not possible for us to determine the true underlying reason why some people might have submitted fraudulent images, we can speculate that some of the ethical design choices of our dataset may have inadvertently incentivized fraudulent behaviours. For example, requiring vendors to pay at least the applicable local minimum wage may have encouraged people to falsely claim to be from regions with higher wages, submitting images from the Internet taken in those locations. Similarly, in our pursuit of diversity, our vendors found certain demographics were more difficult to obtain images of (for example, people of older ages). As such, higher compensation was offered for those demographics, increasing the incentives to fraudulently submit images featuring those demographics.\n\nThe priorities of our data collection project also made fraud more feasible and difficult to detect. Given that FHIBE is designed for fairness evaluation, we sought to maximize visual diversity and collect naturalistic (rather than staged) images. As a result, we opted for a crowd-sourcing approach and allowed subjects to submit past photos. Compared with in-person data collection or bespoke data collection in which the setting, clothing, poses or other attributes might be fixed or specified, it was more difficult for our project to verify that the images were intentionally submitted by the data subject for our project. We therefore encourage dataset curators to consider how their ethical goals may inadvertently attract fraudulent submissions.\n\nAnnotation QA\n\nWe verified the quality of both pixel-level annotations and imageable categorical attribute annotations using two methods. First, we compared the vendor-provided annotations with the average annotations from three of our QA specialists on a randomly sampled set of 500 images for each annotation type. For pixel-level annotations, agreement between the collected annotations and the QA specialist annotations was above 90% (Supplementary Information E), at a similar or higher level as related works90,91,92, showing the robustness and quality of our collected annotations.\n\nSecond, we assessed intra- and inter-vendor annotation consistency by obtaining three sets of annotations for the same 70 images from each vendor. Within each vendor, each image was annotated and reviewed three times by different annotators. To ensure independent assessments, no individual annotator reviewed the same annotation for a given image instance, resulting in mutually exclusive outputs from each labelling pipeline. For dense prediction annotations, intra- and inter-vendor agreement is above 90%, confirming a high quality of collected annotations. For attribute annotations, intra-vendor agreement is above 80% and inter-vendor agreement is at 70%, which indicates that they are more noisy labels than the dense prediction ones (Supplementary Information E).\n\nRegarding metrics for these comparisons, for bounding boxes, we computed the mean intersection over union between the predicted and ground truth bounding boxes. For keypoints, we computed object keypoint similarity93. For segmentation masks, we computed the Sørensen–Dice coefficient94,95. For categorical attributes (for example, hair type, hairstyle, body pose, scene, camera position), we computed the pairwise Jaccard similarity coefficient96 and then the average. Using these analyses, we were able to verify the consistency of the annotations between vendors and our QA specialists, within individual vendors and between different vendors.\n\nPrivacy assurance\n\nWe used a text-guided, fine-tuned stable diffusion model47 from the HuggingFace Diffusers library97 to inpaint regions identified by annotator-generated bounding boxes and segmentation masks containing incidental, non-consensual subjects or personally identifiable information (for example, license plates, identity documents). The model was configured with the following parameters: (1) text prompt: “a high-resolution image with no humans or people in it”; (2) negative text prompt: “human, people, person, human body parts, android, animal”; (3) guidance scale: randomly sampled from a uniform distribution, w ~ U(12, 16); (4) denoising steps: 20; and (5) variance control: η = 0, enabling the diffusion model to function as a denoising diffusion implicit model98.\n\nWe also manually reviewed the images to ensure the correct removal of personally identifiable information and identified any redaction artifacts. Around 10% of images had some content removed and in-painted. To evaluate any potential loss in data use, we compared performance on a subset of tasks (i.e., pose estimation, person segmentation, person detection and face detection) before and after removal and in-painting. No significant performance differences were observed.\n\nTo further address possible privacy concerns with the public disclosure of personal information, a subset of the attributes of consensual image subjects (that is, biological relationships to other subjects in a given image, country of residence, height, weight, pregnancy and disability/difficulty status) are reported only in aggregate form. Moreover, the date and time of image capture were coarsened to the approximate time and month of the year. Subject and annotator identifiers were anonymized, and Exif metadata from the images were stripped.\n\nConsent revocation\n\nWe are committed to upholding the right of human participants to revoke consent at any time and for any reason. As long as FHIBE is publicly available, we will remove images and other data when consent is revoked. If possible, the withdrawn image will be replaced with one that most closely matches key attributes, such as pronoun, age group and regional ancestry. To the extent possible, we will also consider other features that could impact the complexity of the image for relevant tasks when selecting the closest match.\n\nFHIBE derivative datasets\n\nWe release both the original images and downsampled versions in PNG format. The downsampled images were resized to have their largest side set to 2,048 pixels while maintaining the original aspect ratio. These downsampled versions were used in our analyses to prevent memory overflows when feeding images to the downstream models.\n\nFHIBE also includes two face datasets created from the original images (that is, not the downsampled versions), both in PNG format: a cropped-only set and a cropped-and-aligned set. These datasets feature both primary and secondary subjects. For the cropped-and-aligned set, we followed a procedure similar to existing datasets99,100 by cropping oriented rectangles based on the positions of two eye landmarks and two mouth landmarks. These rectangles were first resized to 4,096 × 4,096 pixels using bilinear filtering and then downsampled to 512 × 512 pixels using Lanczos filtering101. Only faces with visible eye and mouth landmarks were included in the final cropped-and-aligned set.\n\nFor the cropped-only set, facial regions were directly cropped based on the face bounding box annotations, with each bounding box enlarged by a factor of two to capture all necessary facial pixels. This set includes images with resolutions ranging from 85 × 144 to 5,820 × 8,865 pixels. If facial regions extended beyond the original image boundaries, padding was applied using the mean value along each axis for both face derivative datasets.\n\nDatasets for fairness evaluation\n\nWe evaluated FHIBE’s effectiveness as a fairness benchmarking dataset by comparing it against several representative human-centric datasets commonly used in the computer vision literature. These datasets were selected based on their relevance to fairness evaluation, the availability of demographic annotations, and/or their use in previous fairness-related studies. Our analysis is limited to datasets that are publicly available; we did not include datasets that have been discontinued, like the JANUS program datasets (IJB-A, IJB-B, IJB-C, IJB-D)102. The results are shown in Supplementary Information F.\n\nCOCO is constructed from the MS-COCO 2014 validation split40, COCO Caption Bias103 and COCO Whole Body104 datasets. We used the images and annotations from the MS-COCO 2014 validation set, and added the perceived gender and skin tone (dark, light) annotations from COCO Caption Bias, excluding entries for which the label was ‘unsure’. We then used COCO Whole Body to filter the dataset for images containing at least one person bounding box. After filtering, this dataset contained 1,355 images with a total of 2,091 annotated person bounding boxes.\n\nFACET24 is a benchmark and accompanying dataset for fairness evaluation, consisting of 32,000 images and 50,000 subjects, with annotations for attributes like perceived skin tone (using the Monk scale105), age group and perceived gender. For our evaluations, we used 49,500 person bounding box annotations and 17,000 segmentation masks, spread across 31,700 images.\n\nOpen Images MIAP42 is a set of annotations for 100,000 images from the Open Images Dataset, including attributes such as age presentation and gender presentation. In our evaluations, we used the test split, excluding images for which the annotations of age or gender are unknown, as well as the ‘younger’ category—to ensure that only adults were included in the evaluation. With this filtering, we used a set of 13,700 images with 36,000 associated bounding boxes and masks.\n\nWiderFace81 is a face detection benchmark dataset containing images and annotations for faces, including the attributes perceived gender, age, skin tone, hair colour and facial hair. We used the validation split in our evaluations after excluding annotations for which perceived gender, age and skin tone were marked as ‘Not Sure’. After the filtering, we used a set of 8,519 face annotations across 2,856 files.\n\nCelebAMask-HQ86 consists of 30,000 high-resolution face images of size 512 × 512 from the CelebA-HQ dataset, which were annotated with detailed segmentation of facial components across 19 classes. From this dataset, we used the test split in our evaluations, consisting of 2,824 images with binarized attributes for age, skin colour and gender.\n\nCCv1106 contains 45,186 videos from 3,011 participants across five US cities. Self-reported attributes include age and gender, with trained annotators labelling apparent skin tone using the Fitzpatrick scale. For dataset statistics, we extracted a single frame per video. For Vendi score computation, we used 10 frames per video.\n\nCCv226 contains 26,467 videos from 5,567 participants across seven countries. Self-reported attributes include age, gender, language, disability status and geolocation, while annotators labelled skin tone (Fitzpatrick and Monk scales), voice timbre, recording setups and per-second activity. For dataset statistics, we extract a single frame per video. For Vendi score computation, we use three frames per video.\n\nIMDB-WIKI107 is a dataset of public images of actors crawled from IMDB and Wikipedia. The images were captioned with date taken such that age could be labelled. From this dataset, we randomly sampled 10% to use for face verification task, resulting in 17,000 images.\n\nNarrow models for evaluation\n\nTo assess the use of FHIBE and FHIBE face datasets, we compared the performance of specialized narrow models (spanning eight classic computer vision tasks) using both FHIBE and pre-existing benchmark datasets as listed above. As FHIBE is designed only for fairness evaluation and mitigation, we did not train any models from scratch. Instead, we evaluated existing, pretrained state-of-the-art models on our dataset to assess their performance and fairness. The results are shown in Supplementary Information F.\n\nPose-estimation models aim to locate face and body landmarks in cropped and resized images derived from ground truth person bounding boxes, following108,109,110. For this task, we used Simple Baseline108, HRNet109 and ViTPose110, all of which were pretrained on the MS-COCO dataset40.\n\nPerson-segmentation models generate segmentation masks that label each pixel of the image with specific body parts or clothing regions of a person. For this task, we used Mask RCNN111, Cascade Mask RCNN112 and Mask2Former113, all of which were trained on MS-COCO dataset40.\n\nPerson-detection models identify individuals from images by relying on object detection models, retaining only the outputs for the class ‘person’. For this task, we used DETR114, Faster RCNN115, Deformable DETR116 and DDOD117 with the ResNet-50 FPN115 backbone, all of which were trained on MS-COCO dataset40.\n\nFace-detection models locate faces in images by predicting bounding boxes that encompass each detected face. For this task, we used the MTCNN118 model trained on VGGFaces2119 and the RetinaFace120 model trained on WiderFace81 using publicly available source code121,122.\n\nFace-segmentation models generate pixel-level masks that classify facial regions into specific facial features (such as eyes, nose, mouth or skin) or background, enabling detailed facial analysis and manipulation. For this task, we used the DML CSR123 model trained on CelebAMask-HQ86.\n\nFace-verification models determine whether two face images belong to the same person by comparing their facial features against a preset similarity threshold. For extracting facial features, we used FaceNet62 trained on VGGFaces2119, and ArcFace60 and CurricularFace61, both trained on refined MS-Celeb-1M124, using publicly available implementations61,121,125.\n\nFace-reconstruction models encode facial images into latent codes and decode these codes back into images, enabling controlled manipulation of facial attributes. For this task, we used ReStyle126 applied over e4e127 and pSp128, and trained on FFHQ99.\n\nFace super-resolution models generate high-resolution facial images from low-resolution inputs, enhancing facial details and overall image quality. For this task, we used GFP-GAN129 and GPEN130, trained on FFHQ99.\n\nNarrow model evaluation metrics\n\nWe used the standard metrics reported in the literature to assess the performance of the narrow models on different tasks.\n\nFor pose estimation, we reported the percentage correct keypoints at a normalized distance of 50% of head length (PCK@0.5)131, which measures the portion of predicted landmarks (keypoints) falling within 0.5 × head-length radius from their true positions.\n\nFor person segmentation, person detection, and face detection, we reported the average recall across intersection over union (IoU) thresholds ranging [0.5, 0.95] with step size 0.05, to assess the average detection completeness of the models across multiple IoU thresholds.\n\nFor face segmentation, we reported the average F1 score (that is, the Sørensen–Dice coefficient94,95) across all segmentation mask categories, where F1 measures the intersection between the predicted and ground truth masks relative to their average size.\n\nFor face verification, we sampled image pairs of the same person (positive) and different people (negative) within each demographic subgroup. For each subgroup, we reported true acceptance rate (TAR) at a false acceptance rate (FAR) of 0.001. TAR@FAR = 0.001 measures the proportion of correctly accepted positive pairs when classification threshold is set to allow only 0.1% incorrectly accepted negative pairs.\n\nFor face reconstruction and face super-resolution, we reported learned perceptual image patch similarity132, which evaluates the perceived visual similarity between reference image I ref and generated image I gen by comparing their feature representations extracted by a pretrained VGG16133 model.\n\nFor face reconstruction, we also assessed perceptual quality using peak signal-to-noise ratio and measured identity preservation using cosine similarity between facial embeddings of I ref and I gen extracted by a CurricularFace model61.\n\nDataset diversity\n\nTo compare FHIBE’s visual diversity with other datasets, we used the Vendi Score134,135, which quantifies diversity using a similarity function.\n\nTo construct the similarity matrix K, we first extracted image features (embeddings) using the self-supervised SEER136 model, which exhibits strong expressive power for vision tasks. We then constructed K by computing the cosine similarity between every feature pair. For extracting feature embeddings with SEER, all images are pre-processed using the ImageNet protocol: rescaling to 224 × 224 and applying z-score normalization using the ImageNet per-channel mean and s.d.\n\nBias discovery in narrow models\n\nWe tested and compared FHIBE’s capabilities for bias diagnosis using a variety of methods.\n\nBenchmarking analysis\n\nFor this analysis, we evaluated FHIBE on seven (note that for this analysis we excluded face verification owing to the inability to compute per-image scores for that task) different downstream computer vision tasks: pose estimation, person segmentation, person detection, face detection, face parsing, face reconstruction and face super-resolution. For each task and its respective models, we obtained a performance score for each image and subject, enabling us to conduct a post hoc analysis to explore the relationship between labelled attributes and performance.\n\nFor every task and model, we performed the following analyses. For each annotation attribute (for example, hair colour), we first isolated individual attribute groups (for example, blond, red, white). For each group, we compiled a set of performance scores (for example, scores for all subjects with blond hair, red hair or white hair). Only groups with at least ten subjects were considered in the analysis. We next performed pairwise comparisons (for example, blond versus red, blond versus white) using the Mann–Whitney U-test to determine whether the groups had similar median performance scores (null hypothesis, two-tailed). To control for multiple comparisons, we applied the Bonferroni correction58 by adjusting the significance threshold based on the number of pairwise tests. For pairs with a statistically significant difference (\\(P < \\frac{0.05}{{\\rm{number}}\\,{\\rm{of}}\\,{\\rm{pairwise}}\\,{\\rm{tests}}}\\)), we identified the groups with the lowest and highest median scores as the worst group and best group, respectively, and computed the min–max group disparity, D, between them:\n\n$$D=1-\\frac{{\\rm{MED}}({\\rm{worst}}\\,{\\rm{group}})}{{\\rm{MED}}({\\rm{best}}\\,{\\rm{group}})},\\,\\,D\\in [0,1],$$\n\nwhere MED(g) denotes the median performance score for group g. A value D → 0 indicates minimal disparity, while D → 1 indicates maximal disparity. We repeated this process for each attribute, identifying group pairs with statistically significant disparities and their corresponding values. For each attribute, we selected the pair with the highest disparity.\n\nDirect error modelling\n\nUsing this approach, we aimed to examine which features were associated with reduced model performance using regression analysis. Although regression analysis is widely used to identify underlying relationships within datasets, its application to image data has traditionally been limited due to the lack of extensive structured annotations. However, the comprehensive scope and detail of the FHIBE annotations enabled us to effectively apply this method and achieve meaningful results. For each task and model, we predicted the model’s performance on individual images as the target variable. To this end, we collected, processed and extracted a range of annotations related to both images and subjects, including features derived from pixel-level annotations, such as the number of visible keypoints or visible head hair, or the absence of it (categorized as the binary attribute ‘bald’), which served as predictor variables. We used decision trees and random forests—an ensemble of decision trees—due to their interpretability, modelling power and low variance. We used the available implementation in the scikit-learn v.1.5.1 library for both of these models. Feature importance was obtained from the random forests model by assessing how each variable (for example, body pose) contributed to reducing variance when constructing decision trees, helping to identify the most predictive features. We then identified the most significant features (top six in most experiments) using the elbow method137. These selected features were then used in a decision tree model to assess the direction of their contribution to prediction—determining whether higher feature values are associated with better or worse model performance. To assess the robustness and statistical significance of observed differences across subgroups, we conducted bootstrap resampling with 5,000 iterations estimating standard errors. This approach enabled us to evaluate differences across groups even within smaller intersectional subgroups.\n\nError pattern recognition\n\nWe used association rule mining, a method frequently used in data mining to identify relationships between variables within a dataset. We applied association rule mining to identify attribute values that frequently co-occur with low performance. This approach enabled us to systematically identify and analyse patterns of bias within the model’s outputs. We used the FP-growth algorithm138. After obtaining the frequently occurring rules, we identified the attributes that are potential modes of error and investigated them further. We did this by studying the error disparities across the unique values of the attribute and evaluating its effect in conjunction with the sensitive attributes.\n\nFor face verification, we modified the protocol described above in the ‘Narrow model evaluation metrics’ section. Given that we wanted to look at the whole dataset, unconstrained to specific attributes, positive and negative pairs were computed using all face images from the FHIBE face dataset. All possible positive pairs were computed (15,474 pairs), while all negative pairs were sampled with the constraint as described previously139 to extract hard pairs: the gallery and probe images had the same pronoun, and their skin colour differed by no more than one of the six possible levels, yielding 4,945,896 pairs.\n\nBias discovery in foundation models\n\nOur analysis focuses on two foundation models: CLIP and BLIP-2. CLIP74 is a highly influential vision-language model that is widely recognized for its applications in zero-shot classification and image search. BLIP-275 advances vision–language alignment by using a captioning and filtering mechanism to refine noisy web-scraped training data, thereby enhancing performance in image captioning, VQA and instruction following.\n\nCLIP\n\nWe used the official OpenAI CLIP model74. We analysed CLIP in an open-vocabulary zero-shot setting to examine the model’s biases towards different image concepts, such as demographic attributes or image concepts (for example, scene). For each value of the given attribute, we presented four distinct text prompts. These prompts were intentionally varied in wording to reduce potential bias or sensitivity to specific phrasing. The prompts were standardized, clear and consistent across various values to minimize the influence of prompt engineering (the set of prompts is provided in Supplementary Information H). We further encoded FHIBE images using the CLIP image encoder. For pre-processing, we used the same pre-processing function as the official implementation. We analysed different variants of the FHIBE dataset to control for various effects related to the human subject and image background. These variants included the original images, images with individuals masked in black, images with individuals blurred with Gaussian noise of radius 100 and images with the background blacked out.\n\nFor the zero-shot classification analysis, we calculated the cosine similarity between the image embeddings and the text embeddings for each attribute. For example, for the scene attribute, we used two sets of prompts, each consisting of four text descriptions for indoor and outdoor environments. We computed the similarity between each text description and the image, selecting the description with the highest similarity as the assigned label for the image.\n\nBLIP-2\n\nThe analysis protocol for the BLIP-275 model consisted of specifying a prompt pair composed of an image and a text string in the form of a request or a question (the prompts are provided in Supplementary Information H). In our experiments, we instantiated the model to perform VQA tasks. We used the HuggingFace open-source BLIP-2 implementation (Salesforce/blip2-opt-2.7b) for all our bias analysis experiments. We defined a set of prompts that will be used to probe the model along with images drawn from the FHIBE dataset. Each prompt from the prompt set was used to probe the model and the final set of prompts was chosen to maximize the model response rate (that is, the prompts for which the model provided the most non-empty responses). The full set of prompts, their corresponding model response rate and the chosen prompts are provided in Supplementary Information H.\n\nEach prompt from this chosen set was paired with every image from the FHIBE dataset and the pair was used to prompt the BLIP-2 model. Depending on the type of prompt, the model response was filtered to extract the relevant information. The filtering mechanism varied according to the prompt tone.\n\nThe positive prompts asked the model to determine the reason for likeability, while the neutral prompts asked the model for the person’s occupation based only on the provided image input. The model responses were in the form of ‘(S)he is likable because (s)he is a good person or (S)he’s a teacher’. To filter responses like these, we used regex expressions to isolate the predicted gender pronoun and the predicted occupation/likability reason from the model response.\n\nWhen the model was prompted with the negatively toned prompt about convictions, the model responses included toxic and discriminatory language. The model response was labelled toxic if it contained any of the words contained in our keyword set as shown in the Supplementary Information H.\n\nFor this analysis, we prompted BLIP-2 with questions about individuals’ social attributes, but we do not condone the use of these tasks outside of bias detection. Predicting social attributes from facial images has long been a popular but problematic task in computer vision. For example, the ChaLearn First Impressions Challenge140 tasked participants with predicting personality traits like warmth and trustworthiness from videos or images. Deep learning models have been used to map facial features to social judgements141,142. With the rise of foundational models, such uses have also emerged for VQA models, which have been employed to predict personality traits of individuals from a single image of them143.\n\nSuch tasks are highly problematic due to their reliance on physiognomic beliefs that personality traits or social attributes can be inferred from appearance alone144. We use such tasks in our paper solely to identify biases in the model, not to use the model’s inferences themselves. While VQA models should in theory refuse to answer such questions, BLIP-2 generally did answer them, with its answers revealing learned societal biases. Building on recent efforts to identify biases in VQA models by using targeted questions to identify biases145,146,147 (for example, “Does this person like algebra?” and “Is this person peaceful or violent?”), our work shows how FHIBE can reveal biases in foundation models and cautions against the flawed assumptions they may promote.\n\nReporting summary\n\nFurther information on research design is available in the Nature Portfolio Reporting Summary linked to this article.",
      "url": "https://www.nature.com/articles/s41586-025-09716-2",
      "source": "Nature",
      "published": "2025-11-06",
      "sentiment_score": 0.85,
      "reasoning": "The article reports the creation and release of FHIBE, a human-centric image dataset designed with rigorous ethical standards including consent, privacy, compensation, and diversity, to enable responsible fairness evaluation in computer vision. This dataset addresses significant issues in AI fairness and bias detection, with broad implications for improving AI systems impacting many people. The article provides detailed context on data collection, annotation, privacy safeguards, quality control, and bias analysis, demonstrating substantial substance and significance.",
      "category": "Technology",
      "personality_title": "New ethical image dataset advances fairness in AI vision systems",
      "personality_presentation": "**Context** – Many AI systems that analyze images of people have problems with fairness and bias. To improve this, researchers need large sets of images that are collected and used responsibly, respecting privacy, consent, and diversity.\n\n**What happened** – Scientists created the Fair Human-Centric Image Benchmark (FHIBE), a dataset of over 10,000 images collected with strict ethical rules. These rules included getting clear permission from people in the photos, protecting their privacy, paying them fairly, and ensuring a wide range of ages, skin tones, and backgrounds. The images were carefully checked for quality and any suspicious or non-consensual photos were removed. Annotators also helped label details like body pose and facial features.\n\n**Impact** – FHIBE is one of the first large image datasets designed specifically to test fairness in AI vision tools. Unlike many older datasets, it follows strong privacy laws and lets people remove their photos anytime. This makes it a trustworthy resource for researchers to find and fix biases in AI models that recognize faces, body movements, and more. By using FHIBE, developers can build AI systems that treat all people more fairly.\n\n**What's next step** – Researchers will use FHIBE to evaluate existing AI models and identify where they perform unevenly across different groups. The dataset also helps test newer, more advanced AI systems called foundation models. Over time, FHIBE can be updated and expanded, and the lessons learned can guide the creation of other ethical datasets.\n\n**One-sentence takeaway** – The FHIBE dataset sets a new standard for ethically collected, diverse images to help make AI vision technology fairer and more respectful of people’s rights.\n",
      "personality_title_fr": "Un nouveau jeu de données éthique améliore l’équité des systèmes d’IA visuelle",
      "personality_presentation_fr": "**Contexte** – De nombreux systèmes d’IA qui analysent des images de personnes souffrent de problèmes d’équité et de biais. Pour y remédier, les chercheurs ont besoin de grands ensembles d’images collectées et utilisées de manière responsable, en respectant la vie privée, le consentement et la diversité.\n\n**Ce qui s’est passé** – Des scientifiques ont créé le Fair Human-Centric Image Benchmark (FHIBE), un jeu de données de plus de 10 000 images recueillies selon des règles éthiques strictes. Ces règles incluent l’obtention d’une permission claire des personnes sur les photos, la protection de leur vie privée, une rémunération équitable et une grande diversité d’âges, de tons de peau et d’origines. Les images ont été soigneusement vérifiées, et les photos suspectes ou sans consentement ont été retirées. Des annotateurs ont aussi aidé à marquer des détails comme la pose du corps et les traits du visage.\n\n**Impact** – FHIBE est l’un des premiers grands jeux de données conçus spécialement pour tester l’équité des outils d’IA visuelle. Contrairement à beaucoup d’anciens ensembles, il respecte des lois strictes sur la vie privée et permet aux personnes de retirer leurs photos à tout moment. Cela en fait une ressource fiable pour les chercheurs qui veulent détecter et corriger les biais dans les modèles d’IA qui reconnaissent les visages, les mouvements du corps, et plus encore. En utilisant FHIBE, les développeurs peuvent créer des systèmes d’IA qui traitent toutes les personnes plus équitablement.\n\n**Prochaine étape** – Les chercheurs utiliseront FHIBE pour évaluer les modèles d’IA existants et repérer où ils fonctionnent de manière inégale selon les groupes. Le jeu de données aide aussi à tester des systèmes d’IA plus avancés appelés modèles fondamentaux. Avec le temps, FHIBE pourra être mis à jour et agrandi, et les enseignements tirés guideront la création d’autres jeux de données éthiques.\n\n**Résumé en une phrase** – Le jeu de données FHIBE établit une nouvelle norme pour la collecte éthique et diversifiée d’images afin de rendre la technologie d’IA visuelle plus juste et respectueuse des droits des personnes.\n",
      "personality_title_es": "Nuevo conjunto de datos ético mejora la equidad en sistemas de visión por IA",
      "personality_presentation_es": "**Contexto** – Muchos sistemas de IA que analizan imágenes de personas tienen problemas de equidad y sesgos. Para mejorar esto, los investigadores necesitan grandes conjuntos de imágenes que se recojan y usen de forma responsable, respetando la privacidad, el consentimiento y la diversidad.\n\n**Qué pasó** – Científicos crearon el Fair Human-Centric Image Benchmark (FHIBE), un conjunto de datos con más de 10,000 imágenes recogidas con estrictas reglas éticas. Estas reglas incluyen obtener permiso claro de las personas en las fotos, proteger su privacidad, pagarles justamente y asegurar una amplia variedad de edades, tonos de piel y orígenes. Las imágenes fueron cuidadosamente revisadas y se eliminaron las fotos sospechosas o sin consentimiento. Los anotadores también ayudaron a marcar detalles como la pose corporal y rasgos faciales.\n\n**Impacto** – FHIBE es uno de los primeros grandes conjuntos de datos diseñados específicamente para probar la equidad en herramientas de visión por IA. A diferencia de muchos conjuntos antiguos, cumple con estrictas leyes de privacidad y permite que las personas retiren sus fotos en cualquier momento. Esto lo convierte en un recurso confiable para que los investigadores detecten y corrijan sesgos en modelos de IA que reconocen rostros, movimientos corporales y más. Usando FHIBE, los desarrolladores pueden crear sistemas de IA que traten a todas las personas de manera más justa.\n\n**Próximo paso** – Los investigadores usarán FHIBE para evaluar modelos de IA existentes y encontrar dónde funcionan de forma desigual entre diferentes grupos. El conjunto también ayuda a probar sistemas de IA más avanzados llamados modelos base. Con el tiempo, FHIBE podrá actualizarse y ampliarse, y las lecciones aprendidas guiarán la creación de otros conjuntos de datos éticos.\n\n**Conclusión en una frase** – El conjunto de datos FHIBE establece un nuevo estándar para la recolección ética y diversa de imágenes que ayuda a hacer la tecnología de visión por IA más justa y respetuosa de los derechos de las personas.\n",
      "image_url": "public/images/news_image_Fair-human-centric-image-dataset-for-ethical-AI-be.png",
      "image_prompt": "A warm, detailed painting of a balanced scale made of intertwined human silhouettes in soft earth tones, symbolizing ethical fairness and consent, set against a calm abstract backdrop of interconnected puzzle pieces representing data diversity and careful quality control."
    },
    {
      "title": "New quantum computer is on the path to unravelling superconductivity",
      "summary": "Using the Helios-1 quantum computer, researchers have used a record-breaking number of error-proof qubits to run the first and biggest quantum simulation of a model for perfect conductivity",
      "content": "Using the Helios-1 quantum computer, researchers have used a record-breaking number of error-proof qubits to run the first and biggest quantum simulation of a model for perfect conductivity\n\nThe Helios-1 quantum computing chip Quantinuum\n\nResearchers at the quantum computing firm Quantinuum used a new Helios-1 quantum computer to simulate a mathematical model that has long been used to study superconductivity. These simulations are not out of reach for conventional computers, but this advance sets the stage for quantum computers to become useful tools for materials science.\n\nSuperconductors conduct electricity with perfect efficiency, but they currently only work at temperatures too low to be practical. For decades, physicists have been trying to understand how to tweak their structure to make them work at room temperature, and many believe answers will come from a mathematical framework called the Fermi-Hubbard model. This potential makes it one of the most important models in all condensed matter physics, says Quantinuum’s Henrik Dreyer.\n\nAdvertisement\n\nConventional computers can run exceptional simulations of the Fermi-Hubbard model but struggle with very large samples or cases where the materials it describes change over time. Quantum computers stand a chance to eventually do better. Now, Dreyer and his colleagues have run the biggest-yet simulation of the Fermi-Hubbard model on a quantum computer.\n\nThey used Helios-1, which has 98 qubits made from barium ions, each of which is controlled with lasers and electromagnetic fields. To run a simulation, the researchers manipulated the qubits through a sequence of quantum states, then read the output by measuring their properties. Their simulations included 36 particles called fermions, which are exactly the type of particle that exists in real superconductors and are mathematically described by the Fermi-Hubbard model.\n\nFor a superconductor to work, fermions must pair up, and experiments have found such pairing can sometimes be initiated by hitting a material with a laser. Quantinuum’s team simulated this scenario – they hit their qubits with a laser pulse, then measured the resulting states, finding signs of the simulated particles’ pairing. The simulation did not exactly replicate experiments, but it captured a dynamical process, which is difficult for conventional computer methods when applied to more than a few particles.\n\nFree newsletter Sign up to The Daily The latest on what’s new in science and why it matters each day. Sign up to newsletter\n\nDreyer says the new experiment is not a rigorous proof Helios-1 has an advantage over every possible traditional computing approach, but exploring classical simulation methods convinced his team that a quantum computer could compete. “For the methods that we tried, it was impossible to reliably get the same results, we were looking at a couple hours on a quantum computer and a big question mark on the classical side of things,” he says. In other words, the team’s estimates of classical computation times were so much longer it was difficult to tell when they would be comparable to Helios’ work.\n\nTrapped ions serve as qubits in the Helios-1 chip Quantinuum\n\nOther quantum computers have not yet tackled simulations of fermions pairing to achieve superconductivity, and the team credits their success to Helios’ hardware. David Hayes, also at Quantinuum, says Helios’ qubits are exceptionally reliable and excel at benchmarking tasks common across the quantum computing industry. In preliminary tests, it could also sustain experiments with error-proof qubits, including connecting 94 of these special qubits through quantum entanglement, which is a record across all quantum computers. Using such qubits in future simulations could make them more accurate.\n\nEduardo Ibarra García Padilla at Harvey Mudd College in California says the new results are promising, but still need to be carefully benchmarked against state-of-the-art classical computer simulations. He says the Fermi-Hubbard model has been of great interest to physicists since the 1960s, so it is exciting to have a new tool for studying it.\n\nPrecisely when approaches like those used with Helios-1 will become true competitors to the best conventional computers is anyone’s guess, because many more details need to be ironed out, says Steve White at the University of California, Irvine. For instance, he says there are challenges in making sure the quantum computer simulation starts with just the right set of qubit properties. Yet, White says quantum simulations could become complementary to classical, especially for dynamic, or changing, behaviour in materials.\n\n“They are on the way to becoming useful simulating tools in condensed matter [physics],” he says. “But they’re still in the early stages, there are computational barriers still to come.”\n\nReference: arXiv, DOI: 10.48550/arXiv.2511.02125",
      "url": "https://www.newscientist.com/article/2502688-new-quantum-computer-is-on-the-path-to-unravelling-superconductivity/?utm_campaign=RSS%7CNSNS&utm_source=NSNS&utm_medium=RSS&utm_content=home",
      "source": "New Scientist - Home",
      "published": "2025-11-05",
      "sentiment_score": 0.85,
      "reasoning": "The article reports a significant breakthrough in quantum computing with the Helios-1 quantum computer running the largest simulation of a key superconductivity model, potentially enabling advances in materials science and energy efficiency. This achievement has broad scientific and technological implications beyond a niche audience and is well-detailed, highlighting a meaningful step toward practical quantum simulations that could impact future superconductor development.",
      "category": "Technology",
      "personality_title": "Quantum computer runs largest simulation of superconductivity model",
      "personality_presentation": "**Context** – Scientists have long studied superconductors, materials that carry electricity without losing any energy. These materials only work at very cold temperatures, and researchers want to understand how to make them work at room temperature. A mathematical idea called the Fermi-Hubbard model helps explain how particles inside superconductors behave.\n\n**What happened** – A company called Quantinuum used their new quantum computer, Helios-1, to run the biggest simulation ever of the Fermi-Hubbard model. They used 98 special units called qubits, made from barium ions, controlled by lasers. The simulation involved 36 particles similar to those in real superconductors and showed signs of how these particles pair up, which is important for superconductivity.\n\n**Impact** – This is the first time a quantum computer has simulated this kind of particle pairing on such a large scale. The simulation captured changes over time that regular computers find very hard to handle. Helios-1’s qubits are very reliable, helping make this detailed simulation possible. This work brings quantum computers closer to being useful tools for studying and designing new materials.\n\n**What's next step** – Researchers will compare these quantum simulations more carefully with the best traditional computer models to check accuracy. They will also work on improving quantum computers to handle even bigger and more complex simulations. Over time, quantum and classical computers may work together to better understand superconductors and other materials.\n\n**One-sentence takeaway** – Using a powerful quantum computer, scientists ran the largest simulation yet of a key superconductivity model, marking progress toward better materials research.\n",
      "personality_title_fr": "Un ordinateur quantique réalise la plus grande simulation d’un modèle de supraconductivité",
      "personality_presentation_fr": "**Contexte** – Les scientifiques étudient depuis longtemps les supraconducteurs, des matériaux qui conduisent l’électricité sans aucune perte d’énergie. Ces matériaux ne fonctionnent qu’à des températures très basses, et les chercheurs veulent comprendre comment les faire fonctionner à température ambiante. Une idée mathématique appelée modèle de Fermi-Hubbard aide à expliquer le comportement des particules à l’intérieur des supraconducteurs.\n\n**Ce qui s’est passé** – Une entreprise nommée Quantinuum a utilisé son nouvel ordinateur quantique, Helios-1, pour réaliser la plus grande simulation jamais faite du modèle de Fermi-Hubbard. Ils ont utilisé 98 unités spéciales appelées qubits, fabriquées à partir d’ions baryum et contrôlées par des lasers. La simulation impliquait 36 particules semblables à celles des supraconducteurs réels et a montré des signes de leur appariement, important pour la supraconductivité.\n\n**Impact** – C’est la première fois qu’un ordinateur quantique simule ce type d’appariement de particules à cette échelle. La simulation a capturé des changements dans le temps, difficiles à gérer pour les ordinateurs classiques. Les qubits d’Helios-1 sont très fiables, ce qui a rendu cette simulation détaillée possible. Ce travail rapproche les ordinateurs quantiques d’un usage utile pour étudier et concevoir de nouveaux matériaux.\n\n**Prochaine étape** – Les chercheurs vont comparer ces simulations quantiques avec les meilleurs modèles classiques pour vérifier leur précision. Ils travailleront aussi à améliorer les ordinateurs quantiques pour gérer des simulations encore plus grandes et complexes. Avec le temps, ordinateurs quantiques et classiques pourront collaborer pour mieux comprendre les supraconducteurs et d’autres matériaux.\n\n**Résumé en une phrase** – Grâce à un ordinateur quantique puissant, des scientifiques ont réalisé la plus grande simulation d’un modèle clé de la supraconductivité, un pas en avant pour la recherche sur les matériaux.\n",
      "personality_title_es": "Ordenador cuántico realiza la simulación más grande de un modelo de superconductividad",
      "personality_presentation_es": "**Contexto** – Los científicos han estudiado durante mucho tiempo los superconductores, materiales que conducen electricidad sin perder energía. Estos materiales solo funcionan a temperaturas muy bajas, y los investigadores quieren entender cómo hacer que funcionen a temperatura ambiente. Una idea matemática llamada modelo de Fermi-Hubbard ayuda a explicar cómo se comportan las partículas dentro de los superconductores.\n\n**Qué pasó** – Una empresa llamada Quantinuum usó su nueva computadora cuántica, Helios-1, para ejecutar la simulación más grande hasta ahora del modelo Fermi-Hubbard. Usaron 98 unidades especiales llamadas qubits, hechas de iones de bario y controladas por láseres. La simulación incluyó 36 partículas similares a las de superconductores reales y mostró señales de cómo estas partículas se emparejan, algo importante para la superconductividad.\n\n**Impacto** – Es la primera vez que una computadora cuántica simula este tipo de emparejamiento de partículas a gran escala. La simulación capturó cambios en el tiempo que las computadoras tradicionales encuentran muy difíciles de manejar. Los qubits de Helios-1 son muy confiables, lo que hizo posible esta simulación detallada. Este trabajo acerca a las computadoras cuánticas a ser herramientas útiles para estudiar y diseñar nuevos materiales.\n\n**Próximo paso** – Los investigadores compararán estas simulaciones cuánticas con los mejores modelos tradicionales para verificar su precisión. También trabajarán en mejorar las computadoras cuánticas para manejar simulaciones aún más grandes y complejas. Con el tiempo, las computadoras cuánticas y clásicas podrían trabajar juntas para entender mejor los superconductores y otros materiales.\n\n**Resumen en una frase** – Usando una computadora cuántica poderosa, los científicos realizaron la simulación más grande hasta ahora de un modelo clave de superconductividad, marcando un avance en la investigación de materiales.\n",
      "image_url": "public/images/news_image_New-quantum-computer-is-on-the-path-to-unravelling.png",
      "image_prompt": "A detailed, warm-toned painting of a glowing, intricate quantum chip made of softly glowing barium ion spheres connected by delicate beams of light, with pairs of luminous fermion-like particles gently intertwining above it, all set against a calm, abstract background symbolizing the flow of perfect electrical conductivity."
    }
  ]
}