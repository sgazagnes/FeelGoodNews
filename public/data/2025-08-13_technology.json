{
  "personality": null,
  "timestamp": "2025-08-13T04:43:39.232142",
  "category": "Technology",
  "news_summary": "Innovative AI techniques are being developed to combat unauthorized data collection by bots, enhancing digital security and privacy.",
  "news_summary_fr": "Des techniques d'IA innovantes sont mises au point pour lutter contre la collecte non autorisée de données par des robots, afin d'améliorer la sécurité numérique et la protection de la vie privée.",
  "news_summary_es": "Se están desarrollando técnicas innovadoras de IA para combatir la recopilación no autorizada de datos por parte de bots, mejorando la seguridad y la privacidad digitales.",
  "articles": [
    {
      "title": "How AI poisoning is fighting bots that hoover data without permission",
      "summary": "The web is awash with bots that scrape data without permission. Now content creators are poisoning the well of artificial intelligence – but similar technology can also be used to spread misinformation",
      "content": "The web is awash with bots that scrape data without permission. Now content creators are poisoning the well of artificial intelligence – but similar technology can also be used to spread misinformation\n\nRenaud Vigourt\n\nGone are the days when the web was dominated by humans posting social media updates or exchanging memes. Earlier this year, for the first time since the data has been tracked, web-browsing bots, rather than humans, accounted for the bulk of web traffic.\n\nWell over half of that bot traffic is from malicious bots, hoovering up personal data left unprotected online, for instance. But an increasing proportion comes from bots sent out by artificial intelligence companies to gather data for their models or respond to user prompts. Indeed, ChatGPT-User, a bot powering OpenAI’s ChatGPT, is now responsible for 6 per cent of all web traffic, while ClaudeBot, an automated system developed by AI company Anthropic, accounts for 13 per cent.\n\nThe AI companies say such data scraping is vital to keep their models up to date. Content creators feel differently, however, seeing AI bots as tools for copyright infringement on a grand scale. Earlier this year, for example, Disney and Universal sued AI company Midjourney, arguing that the tech firm’s image generator plagiarises characters from popular franchises like Star Wars and Despicable Me.\n\nFew content creators have the money for lawsuits, so some are adopting more radical methods of fighting back. They are using online tools that make it harder for AI bots to find their content – or that manipulate it in a way that tricks bots into misreading it, so that the AI begins to confuse images of cars with images of cows, for example. But while this “AI poisoning” can help content creators protect their work, it might also inadvertently make the web a more dangerous place.\n\nCopyright infringement\n\nFor centuries, copycats have made a quick profit by mimicking the work of artists. It is one reason why we have intellectual property and copyright laws. But the arrival in the past few years of AI image generators such as Midjourney or OpenAI’s DALL-E has supercharged the issue.\n\nA central concern in the US is what is known as the fair use doctrine. This allows samples of copyrighted material to be used under certain conditions without requesting permission from the copyright holder. Fair use law is deliberately flexible, but at its heart is the idea that you can use an original work to create something new, provided it is altered enough and doesn’t have a detrimental market effect on the original work.\n\nMany artists, musicians and other campaigners argue that AI tools are blurring the boundary between fair use and copyright infringement to the cost of content creators. For instance, it isn’t necessarily detrimental for someone to draw a picture of Mickey Mouse in, say, The Simpsons’ universe for their own entertainment. But with AI, it is now possible for anyone to spin up large numbers of such images quickly and in a manner where the transformative nature of what they have done is questionable. Once they have made these images, it would be easy to produce a range of T-shirts based on them, for example, which would cross from personal to commercial use and breach the fair use doctrine.\n\nKeen to protect their commercial interests, some content creators in the US are taking legal action. The Disney and Universal lawsuit against Midjourney, launched in June, is just the latest example. Others include an ongoing legal battle between The New York Times and OpenAI over alleged unauthorised use of the newspaper’s stories.\n\nDisney sued AI company Midjourney over its image generator, which they say plagiarises Disney characters Photo 12/Alamy\n\nThe AI companies strongly deny any wrongdoing, insisting that data scraping is permissible under the fair use doctrine. In an open letter to the US Office of Science and Technology Policy in March, OpenAI’s chief global affairs officer, Chris Lehane, warned that rigid copyright rules elsewhere in the world, where there have been attempts to provide stronger copyright protections for content creators, “are repressing innovation and investment”. OpenAI has previously said it would be “impossible” to develop AI models that meet people’s needs without using copyrighted work. Google takes a similar view. In an open letter also published in March, the company said, “Three areas of law can impede appropriate access to data necessary for training leading models: copyright, privacy, and patents.”\n\nHowever, at least for the moment, it seems the campaigners have the court of public opinion on their side. When the site IPWatchdog analysed public responses to an inquiry about copyright and AI by the US Copyright Office, it found that 91 per cent of comments contained negative sentiments about AI.\n\nWhat may not help AI firms gain public sympathy is a suspicion that their bots are sending so much traffic to some websites that they are straining resources and perhaps even forcing some websites to go offline – and that content creators are powerless to stop them. For instance, there are techniques content creators can use to opt out of having bots crawl their websites, including reconfiguring a small file at the heart of the website to say that bots are banned. But there are indications that bots can sometimes disregard such requests and continue crawling anyway.\n\nAI data poisoning\n\nIt is little wonder, then, that new tools are being made available to content creators that offer stronger protection against AI bots. One such tool was launched this year by Cloudflare, an internet infrastructure company that provides its users protection against distributed denial-of-service (DDoS) attacks, in which an attacker floods a web server with so much traffic that it knocks the site itself offline. To combat AI bots that may pose their own DDoS-like risk, Cloudflare is fighting fire with fire: it produces a maze of AI-generated pages full of nonsense content so that AI bots expend all their time and energy looking at the nonsense, rather than the actual information they seek.\n\nThe tool, known as AI Labyrinth, is designed to trap the 50 billion requests a day from AI crawlers that Cloudflare says it encounters on the websites within its network. According to Cloudflare, AI Labyrinth should “slow down, confuse, and waste the resources of AI crawlers and other bots that don’t respect ‘no crawl’ directives”. Cloudflare has since released another tool, which asks AI companies to pay to access websites, or else be blocked from crawling its content.\n\nAn alternative is to allow the AI bots access to online content – but to subtly “poison” it in such a way that it renders the data less useful for the bot’s purposes. The tools Glaze and Nightshade, developed at the University of Chicago, have become central to this form of resistance. Both are free to download from the university’s website and can run on a user’s computer.\n\nGlaze, released in 2022, functions defensively, applying imperceptible, pixel-level alterations, or “style cloaks”, to an artist’s work. These changes, invisible to humans, cause AI models to misinterpret the art’s style. For example, a watercolour painting might be perceived as an oil painting. Nightshade, published in 2023, is a more offensive tool that poisons image data – again, imperceptibly as far as humans are concerned – in a way that encourages an AI model to make an incorrect association, such as learning to link the word “cat” with images of dogs. Both tools have been downloaded more than 10 million times.\n\nThe Nightshade tool gradually poisons AI bots so that they represent dogs as cats Ben Y. Zhao\n\nThe AI poisoning tools put power back in the hands of artists, says Ben Zhao at the University of Chicago, who is the senior researcher behind both Glaze and Nightshade. “These are trillion-dollar market-cap companies, literally the biggest companies in the world, taking by force what they want,” he says.\n\nUsing tools like Zhao’s is a way for artists to exert the little power they have over how their work is used. “Glaze and Nightshade are really interesting, cool tools that show a neat method of action that doesn’t rely on changing regulations, which can take a while and might not be a place of advantage for artists,” says Jacob Hoffman-Andrews at the Electronic Frontier Foundation, a US-based digital rights non-profit.\n\nThe idea of self-sabotaging content to try to ward off alleged copycats isn’t new, says Eleonora Rosati at Stockholm University in Sweden. “Back in the day, when there was a large unauthorised use of databases – from telephone directories to patent lists – it was advised to put in some errors to help you out in terms of evidence,” she says. For instance, a cartographer might deliberately include false place names on their maps. If those false names then appear later in a map produced by a rival, it would provide clear evidence of plagiarism. The practice still makes headlines today: music lyrics website Genius claimed to have inserted different types of apostrophes into its content, which it alleged showed that Google had been using its content without permission. Google denies the allegations, and a court case filed by Genius against Google was dismissed.\n\nEven calling it “sabotage” is debatable, according to Hoffman-Andrews. “I don’t think of it as sabotage necessarily,” he says. “These are the artist’s own images that they are applying their own edits to. They’re fully free to do what they want with their data.”\n\nIt is unknown to what extent AI companies are taking their own countermeasures to try to combat this poisoning of the well, either by ignoring any content that is marked with the poison or trying to remove it from the data. But Zhao’s attempts to break his own system showed that Glaze was still 85 per cent effective against all countermeasures he could think of taking, suggesting that AI companies may conclude that dealing with poisoned data is more trouble than it’s worth.\n\nSpreading fake news\n\nHowever, it isn’t just artists with content to protect who are experimenting with poisoning the well against AI. Some nation-states may be using similar principles to push false narratives. For instance, US-based think tank the Atlantic Council claimed earlier this year that Russia’s Pravda news network – whose name means “truth” in Russian – has used poisoning to trick AI bots into disseminating fake news stories.\n\nPravda’s approach, as alleged by the think tank, involves posting millions of web pages, sort of like Cloudflare’s AI Labyrinth. But in this case, the Atlantic Council says the pages are designed to look like real news articles and are being used to promote the Kremlin’s narrative about Russia’s war in Ukraine. The sheer volume of stories could lead AI crawlers to over-emphasise certain narratives when responding to users, and an analysis published this year by US technology firm NewsGuard, which tracks Pravda’s activities, found that 10 major AI chatbots outputted text in line with Pravda’s views in a third of cases.\n\nThe relative success in shifting conversations highlights the inherent problem with all things AI: technology tricks used by good actors with good intentions can always be co-opted by bad actors with nefarious goals.\n\nThere is, however, a solution to these problems, says Zhao – although it may not be one that AI companies are willing to consider. Instead of indiscriminately gathering whatever data they can find online, AI companies could enter into formal agreements with legitimate content providers and ensure that their products are trained using only reliable data. But this approach carries a price, because licensing agreements can be costly. “These companies are unwilling to license these artists’ works,” says Zhao. “At the root of all this is money.”",
      "url": "https://www.newscientist.com/article/2490598-how-ai-poisoning-is-fighting-bots-that-hoover-data-without-permission/?utm_campaign=RSS%7CNSNS&utm_source=NSNS&utm_medium=RSS&utm_content=home",
      "source": "New Scientist - Home",
      "published": "2025-08-12",
      "sentiment_score": 0.75,
      "reasoning": "The article describes innovative technological tools developed to empower content creators to protect their work against unauthorized AI data scraping, representing a meaningful advancement with broad implications for digital rights and AI ethics. It focuses on the development and deployment of AI poisoning tools that help artists and creators defend their intellectual property, offering a tangible, scalable solution to a significant societal challenge posed by AI data misuse. While it also acknowledges risks and misuse by bad actors, the core story is about a positive technological breakthrough that benefits many creators and the broader public.",
      "category": "Technology",
      "personality_title": "New AI tools help artists protect their work from unauthorized data scraping",
      "personality_presentation": "**Context** – The internet is full of automated bots that collect information from websites without asking permission. Many of these bots belong to artificial intelligence (AI) companies that gather data to improve their AI models. This has raised concerns among artists and content creators about copyright and misuse of their work.\n\n**What happened** – To fight back, some creators are using new technology called \"AI poisoning.\" This involves adding small, hidden changes to their digital work that confuse AI bots. For example, one tool called Glaze changes the style of an image just enough to trick AI, while another called Nightshade makes AI mix up objects, like seeing dogs as cats. These tools have been downloaded millions of times and help slow down or mislead AI bots trying to copy content.\n\n**Impact** – These AI poisoning tools give artists more control over how their work is used online. Instead of relying only on slow legal battles, creators can now protect their content directly. This is important because AI companies often collect huge amounts of data, sometimes ignoring website rules against bots. The tools also show a new way to balance the power between big AI firms and individual creators.\n\n**What's next step** – AI companies might try to find ways around these protections, but early tests show the tools are still mostly effective. Experts suggest a better long-term solution would be for AI firms to make formal agreements with content creators to use data fairly. However, such agreements can be expensive and are not yet common.\n\n**One-sentence takeaway** – New AI poisoning tools offer artists a practical way to protect their digital work from unauthorized AI data collection, changing how creators defend their rights online.",
      "personality_title_fr": "De nouveaux outils d’IA aident les artistes à protéger leurs œuvres contre la collecte non autorisée de données",
      "personality_presentation_fr": "**Contexte** – Internet est rempli de robots automatisés qui collectent des informations sur les sites web sans demander la permission. Beaucoup de ces robots appartiennent à des entreprises d’intelligence artificielle (IA) qui rassemblent des données pour améliorer leurs modèles. Cela inquiète les artistes et créateurs à cause des droits d’auteur et de l’utilisation abusive de leurs œuvres.\n\n**Ce qui s’est passé** – Pour riposter, certains créateurs utilisent une nouvelle technologie appelée « empoisonnement de l’IA ». Cela consiste à ajouter de petites modifications cachées à leurs œuvres numériques qui embrouillent les robots d’IA. Par exemple, un outil nommé Glaze change légèrement le style d’une image pour tromper l’IA, tandis qu’un autre, Nightshade, fait confondre à l’IA des objets, comme voir des chiens comme des chats. Ces outils ont été téléchargés des millions de fois et aident à ralentir ou à induire en erreur les robots d’IA qui essaient de copier du contenu.\n\n**Impact** – Ces outils d’empoisonnement redonnent aux artistes plus de contrôle sur l’utilisation de leurs œuvres en ligne. Plutôt que de compter uniquement sur des procédures juridiques longues, les créateurs peuvent maintenant protéger directement leur contenu. Cela est important car les entreprises d’IA collectent souvent d’énormes quantités de données, parfois en ignorant les règles des sites web contre les robots. Ces outils montrent aussi une nouvelle façon d’équilibrer le pouvoir entre les grandes firmes d’IA et les créateurs individuels.\n\n**Prochaine étape** – Les entreprises d’IA pourraient chercher à contourner ces protections, mais les premiers tests montrent que les outils restent efficaces. Des experts suggèrent qu’une meilleure solution à long terme serait que les entreprises d’IA concluent des accords formels avec les créateurs pour utiliser les données de manière équitable. Cependant, ces accords peuvent coûter cher et ne sont pas encore courants.\n\n**Résumé en une phrase** – Les nouveaux outils d’empoisonnement de l’IA offrent aux artistes un moyen concret de protéger leurs œuvres numériques contre la collecte non autorisée de données par l’IA, changeant la manière dont les créateurs défendent leurs droits en ligne.",
      "personality_title_es": "Nuevas herramientas de IA ayudan a artistas a proteger sus obras contra la recopilación no autorizada de datos",
      "personality_presentation_es": "**Contexto** – Internet está lleno de bots automáticos que recopilan información de sitios web sin pedir permiso. Muchos de estos bots pertenecen a empresas de inteligencia artificial (IA) que reúnen datos para mejorar sus modelos. Esto ha generado preocupación entre artistas y creadores sobre los derechos de autor y el mal uso de sus obras.\n\n**Qué pasó** – Para defenderse, algunos creadores están usando una nueva tecnología llamada \"envenenamiento de IA\". Esto consiste en agregar pequeños cambios ocultos en sus obras digitales que confunden a los bots de IA. Por ejemplo, una herramienta llamada Glaze cambia ligeramente el estilo de una imagen para engañar a la IA, mientras que otra llamada Nightshade hace que la IA confunda objetos, como ver perros como gatos. Estas herramientas se han descargado millones de veces y ayudan a ralentizar o engañar a los bots que intentan copiar contenido.\n\n**Impacto** – Estas herramientas de envenenamiento de IA dan a los artistas más control sobre cómo se usa su trabajo en línea. En lugar de depender solo de largos procesos legales, los creadores ahora pueden proteger directamente su contenido. Esto es importante porque las empresas de IA a menudo recopilan grandes cantidades de datos, a veces ignorando las reglas de los sitios web contra bots. Las herramientas también muestran una nueva forma de equilibrar el poder entre las grandes empresas de IA y los creadores individuales.\n\n**Próximo paso** – Las empresas de IA podrían intentar encontrar formas de evitar estas protecciones, pero las primeras pruebas muestran que las herramientas siguen siendo efectivas. Los expertos sugieren que una mejor solución a largo plazo sería que las empresas de IA hagan acuerdos formales con los creadores para usar los datos de manera justa. Sin embargo, estos acuerdos pueden ser costosos y aún no son comunes.\n\n**Resumen en una frase** – Las nuevas herramientas de envenenamiento de IA ofrecen a los artistas una forma práctica de proteger sus obras digitales contra la recopilación no autorizada de datos, cambiando cómo los creadores defienden sus derechos en línea.",
      "image_url": "public/images/news_image_How-AI-poisoning-is-fighting-bots-that-hoover-data.png",
      "image_prompt": "A detailed warm-toned painting of a serene digital garden where glowing, translucent data streams flow into a protective maze of shimmering, puzzle-like walls, with subtle pixelated distortions gently swirling around vibrant, stylized artworks and images, symbolizing AI bots being playfully tricked and guided away from creators’ content."
    }
  ]
}