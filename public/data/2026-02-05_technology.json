{
  "personality": null,
  "timestamp": "2026-02-05T05:29:12.067160",
  "category": "Technology",
  "news_summary": "Advancements in quantum simulation, AI-driven scientific synthesis, efficient battery life prediction, and innovative bacterial immune activation showcase significant technological progress today.",
  "news_summary_fr": "Les progrès réalisés dans les domaines de la simulation quantique, de la synthèse scientifique basée sur l'IA, de la prédiction efficace de la durée de vie des batteries et de l'activation innovante du système immunitaire bactérien témoignent aujourd'hui d'avancées technologiques significatives.",
  "news_summary_es": "Los avances en simulación cuántica, síntesis científica impulsada por IA, predicción eficiente de la vida útil de las baterías y activación inmunitaria bacteriana innovadora muestran un importante progreso tecnológico en la actualidad.",
  "articles": [
    {
      "title": "Large-scale analogue quantum simulation using atom dot arrays",
      "summary": "Nature, Published online: 04 February 2026; doi:10.1038/s41586-025-10053-7A new platform comprising large-scale 2D arrays of quantum dots patterned with sub-nanometre precision, with each quantum dot defined by tens of phosphorus atoms doped into silicon, allows for analogue simulation of quantum materials on arbitrary lattices.",
      "content": "Strongly correlated quantum states, ubiquitous in quantum systems with complex band topology and strong interactions, are one of the most active areas of research in quantum materials and chemistry3,20,21,22,23. Analogue quantum simulations are a useful tool for investigating these systems, particularly in regimes in which the applicability of numerical techniques is limited. For different simulator platforms1,8,9,10,11,12,13,14,15,16,17,18,19, figures of merit include the electron bandwidth and interaction strength, temperature and the number of simulated lattice sites. Their use is further underscored by the ability to realize distinct lattice geometries, on-site degrees of freedom and by the physical observables that are accessible to experimental measurement.\n\nRecent works on topological states in 1D chains24 and Fermi–Hubbard physics in small 2D arrays25 have shown that atom-based quantum dots, precision-manufactured using scanning tunnelling microscope (STM) lithography26,27 (Supplementary Information Section S1A), have many unique qualities advantageous to analogue quantum simulation. The strong Coulomb potential of the donor nuclei naturally creates strong local and long-range interactions that play a key role in many complex phenomena in quantum materials and chemistry and the precision and flexibility of STM lithography enables the patterning of arbitrary quantum dot and lattice geometries. Furthermore, atom-based quantum dots do not require confinement electrodes as in gate-defined quantum dots, creating a simpler system that is amenable to scaling up to thousands of lattice sites. These unique qualities of atom-based quantum dot arrays make them attractive for reaching simulation regimes that are challenging to access for other platforms such as gate-defined quantum dots28,29,30, ultracold atoms8,9,10,11 and van der Waals materials16,17,18,19, in which physical limitations in temperature12 and lattice geometry16 reduce the scope of systems that can be simulated. So far, atom-based quantum dot arrays have not yet been demonstrated at a scale useful for analogue quantum simulation owing to the challenge of maintaining sub-nanometre precision control in STM fabrication over the scale of micrometres.\n\nIn this work, we greatly expand the quantum simulation capabilities of atom-based quantum dots in terms of both precision and size (see Figs. 1 and 2a) and simulate a MI transition driven by Mott–Hubbard and Anderson physics in a series of 2D quantum dot arrays, each containing 15,000 atom-based quantum dots on a 100 × 150 square lattice. This array size eclipses the previous largest reported arrays of about ten quantum dots24,25 and reflects a notable step up in STM-based atomic fabrication. To achieve uniformity across such large arrays, in both the size of and the spacing between the atom-based quantum dots, we incorporated and optimized the use of an advanced STM controller (Zyvex Labs) that can correct for piezo creep and hysteresis across micron-sized areas. This method allows us to pattern arbitrary lattice geometries, as shown in Fig. 1b,d. The quantum dot array is integrated into a fully epitaxial Hall bar architecture to enable charge-transport measurements (Fig. 1c and Supplementary Information Section S4). Here the source/drain and Hall probes are also patterned using STM lithography, yielding metallic, highly phosphorus-doped silicon leads31. Ohmic contacts to the leads along with a metallic (Ti/Pd) global top gate for electrostatic control are added using semiconductor processing techniques. The behaviour of the system is determined by the energy scales engineered into the lattice (see Fig. 2b,c): the on-site interaction U i (energy cost to add/remove an electron on site i), inter-site interaction V ij (cost to add an electron on site i as a result of one on site j), tunnel coupling t ij (hopping energy between sites i and j) and electrochemical potential μ i (single-particle energy of site i). For next-nearest neighbour sites, we find typical V ij and t ij that are 50–70% and ≱5% of their nearest neighbour values, respectively (see Supplementary Information Section S2 and ref. 25). Next-nearest neighbour interactions are therefore substantial, whereas next-nearest neighbour tunnelling is negligible. We simulate a MI transition by manufacturing four different quantum dot arrays (labelled A–D) with increasing inter-dot separation a (7.2 nm, 9.1 nm, 10.8 nm and 15.5 nm), quenching the inter-dot tunnel coupling t from 1.54 ± 0.28 meV to 0.10 ± 0.02 meV. The quantum dot area was kept in the range A = 22–28 nm2 corresponding to about 50 phosphorus atoms in each dot and an on-site energy U = 20.63 ± 0.94 meV. As a result of the increasing dot separation, the effective interaction strength U/t increases from 14 to 203. Disorder in these energies is estimated at <1 meV; see Supplementary Information Section S3D. Scanning tunnelling micrographs of sections of the lithographic masks used to define each array are shown in Fig. 3a, along with the ratio U/t calculated from a combination of continuum and atomistic modelling techniques (Supplementary Information Section S3). The collective behaviour of charge carriers in the array is first examined by measuring the longitudinal conductance σ xx , plotted in Fig. 3b as a function of temperature. We observe that the conductance at base temperature (approximately 100 mK) decreases exponentially with increasing inter-dot separation a (that is, larger U/t). Theories of transport in granular metals, in which both interactions and disorder are present32,33,34,35,36, predict a MI transition at a critical conductance σ c , determined by the interaction strength U and level spacing δ of the quantum dots, with the latter estimated independently from the high-temperature conductance in each sample (Supplementary Information Section S4 and Fig. 4e). We obtain a value of the critical conductance \\({\\sigma }_{{\\rm{c}}}\\approx 1.11\\frac{2{e}^{2}}{h}\\), indicated by the dashed line in Fig. 3b. Consistent with this prediction, arrays A and B are metallic, whereas arrays C and D show weakly and strongly insulating low-temperature behaviour, respectively.\n\nFig. 1: Large-scale quantum simulators using precision-engineered atom-based quantum dots in silicon. a, Illustration of the STM hydrogen lithography technique used to fabricate the quantum dot arrays. An atomically sharp metallic tip scans a hydrogen-terminated silicon surface and selectively removes individual hydrogen atoms to create a lithographic mask of dangling silicon bonds. Subsequent phosphine dosing is used to selectively dope the lithographic region, embedding arbitrary 2D geometries of atom-based quantum dots. b, Examples of STM-defined lattices: hexagonal lattice with circular quantum dots (top left); honeycomb structure (top right); Lieb lattice with rectangular and cross-shaped quantum dots (bottom right). c, Schematic of the quantum analogue simulator. The STM-defined array and Hall probes are encapsulated in about 80 nm of epitaxial silicon and a metallic top gate (Ti/Pd) is patterned directly over the array on the silicon surface. Charge-transport measurements are performed by applying a current or voltage through the source/drain contacts and reading out the two-point or four-point voltages at the source/drain or Hall contacts. d, A large-scale STM image of a square-lattice array, showing about 700 out of its 15,000 quantum dots. The terraces visible in the image have no impact on the physics of the dot array, as they are removed when the device is encapsulated in epitaxial silicon. Scale bars, 100 nm (b top left, d); 50 nm (b top right); 40 nm (b bottom right). Full size image\n\nFig. 2: Simulation of strongly interacting physics on a 2D square-lattice quantum dot array. a, Approximate range of the interaction strength U/t and number of lattice sites N in different platforms of analogue quantum simulation. For atom-based quantum dots, we plot the range for both previous 2D simulations25 and this work. b, Top, schematic of a Hubbard model on a 2D square lattice. The sites (grey) hold up to two electrons (red arrows) with electron hopping terms t and on-site (inter-site) electron–electron interaction U (V). Bottom, zoomed-in image of a quantum dot array showing the equivalent Hubbard parameters realized in our quantum simulators; see also Fig. 1. c, The energy term in the Hubbard model, the corresponding quantity in the quantum analogue simulator and the respective method of control. Full size image\n\nFig. 3: Engineering U/t across the MI transition. a, Scanning tunnelling micrographs that show close-up sections of the lithographic masks used to fabricate the quantum dot arrays. For arrays A–D (top to bottom), the inter-dot distance is increased, reducing the tunnel coupling. The predicted interaction strengths U/t are as indicated. b, Temperature dependence of the conductance σ xx , exhibiting metallic behaviour for arrays A and B and a weak and strong insulating behaviour in arrays C and D, respectively. Open circles indicate that the conductance at base temperature (0.1 K) decreases exponentially as the inter-dot distance a is increased (see inset). Error bars obtained from fits to I–V curves are smaller than the symbols for all but one data point. The horizontal dashed line indicates the critical conductance σ c of the MI transition predicted for these four devices. Full size image\n\nFig. 4: Investigating the interaction-driven nature of the insulating state. a, STM micrographs of arrays F, D and E (left to right), with inter-dot separation a ≈ 15 nm and distinct quantum dot areas A. The predicted interaction strength U is shown for each array. b, Bias spectroscopy for the three arrays, with bias voltage V dc normalized by the Mott–Hubbard gap \\({\\Delta }_{{\\rm{c}}}^{{\\rm{th}}}\\). A total depletion of charge transport occurs in the Coulomb gap \\({\\Delta }_{{\\rm{eh}}}^{{\\rm{th}}}\\), indicated by the coloured arrows. The stronger-interacting devices D and E show a coherence peak at the Mott–Hubbard value \\(e{V}_{{\\rm{dc}}}\\approx \\pm 2{\\Delta }_{{\\rm{c}}}^{{\\rm{th}}}\\), indicated by the vertical dashed lines, whereas device F exhibits one just outside the Coulomb gap. c, Increase of the charge gap Δ c in an applied magnetic field, normalized by the interaction U. The linear dependence suggests electron exchange as the underlying mechanism; an effective Landé factor g eff is extracted from a linear fit for each device. The data are offset by 0.3 for better visibility. d, Thermal activation of the low-bias conductance σ xx in the three arrays. Transport is driven by incoherent or coherent electron co-tunnelling, with a switch at the cross-over scale T c indicated by the vertical dashed lines. In both regimes, the conductance follows an Efros–Shklovskii (ES) law and depends exponentially on the inverse square root of temperature, but with distinct activation temperatures \\({T}_{0}^{{\\rm{in}},{\\rm{el}}}\\) (slopes of fitted dashed grey lines). e, The quantum dot level spacing δ (in meV) and the spin susceptibility χ s (dimensionless), extracted from analysis of the data in Figs. 3b and 4c, versus the quantum dot area A. Full size image\n\nTo further examine the nature of the observed insulating state, we manufactured two more quantum dot arrays with inter-dot separations a ≃ 15.1 nm and 17.1 nm (similar to the insulating array D in Fig. 3) but changing the quantum dot area A to 9 nm2 and 62 nm2 (arrays E and F, respectively; Fig. 4a). Electron interactions are weaker in larger quantum dots and stronger in smaller quantum dots, allowing us to control the interaction energy scales U and V by changing the quantum dot area A (Supplementary Information Section S3A,B). Figure 4a shows close-up STM micrographs of the lithographic masks used to define the three insulating arrays, with the corresponding dot area A and interaction strength U indicated. For a Mott–Hubbard system in the strongly interacting limit, theory predicts a single-particle charge gap of the order \\({\\Delta }_{{\\rm{c}}}^{{\\rm{th}}}\\simeq \\frac{1}{2}U+4V+\\cdots \\) (refs. 20,21,35) with extra terms owing to long-range interactions. Conversely, the onset of bulk charge transport in the dot arrays is governed by the Mott–Coulomb gap for electron–hole excitations \\({\\Delta }_{{\\rm{eh}}}^{{\\rm{th}}}=U-c{g}_{{\\rm{T}}}{E}_{{\\rm{eh}}}\\), in which E eh = 2U − V is the bare excitation energy cost, g T the inter-dot conductance and c ≈ 0.281 (refs. 35,37) (see Supplementary Information Section S4C). The bias spectroscopy measurements in Fig. 4b are taken at base temperature (T ≈ 100 mK), for which we use the longitudinal voltage measured between the Hall probes as a proxy for the presence of current-carrying states in the array. For arrays E and D, these indicate a hard insulating gap at low bias within the Coulomb gap, \\(e|{V}_{{\\rm{dc}}}|\\le {\\Delta }_{{\\rm{eh}}}^{{\\rm{th}}}\\), and a coherence peak (boost of spectral weight) with subsequent saturation of the signal as the source/drain bias overcomes the Mott–Hubbard excitation energy (at \\(e{V}_{{\\rm{dc}}}=\\pm 2{\\Delta }_{{\\rm{c}}}^{{\\rm{th}}}\\) in Fig. 4b, with V dc = V S − V D the total source–drain bias). For the largest-dot array F, we find a coherence peak just above the Coulomb gap \\({\\Delta }_{{\\rm{eh}}}^{{\\rm{th}}}\\) and only faint features at the Mott–Hubbard scale \\({\\Delta }_{{\\rm{c}}}^{{\\rm{th}}}\\) (Supplementary Information Section S4C,D). We attribute the different behaviours of the devices to the weaker interactions, smaller dot level spacing and larger inter-dot conductance going from devices E, D to F. The bias spectroscopy data show that the strength of interactions in our devices can be tuned using the size of the quantum dots.\n\nNext we subject the insulating devices to a perpendicular magnetic field, for which we observe a distinct interaction effect in quantum dots: the enhancement of the charge addition energy (and hence charge gap) owing to the electron exchange mechanism38. Here electron spin states in an applied magnetic field are split by a large Zeeman energy that can be framed in terms of an interaction-enhanced Landé g-factor g eff . The measurements of the field-enhanced gap for devices D, E and F are in shown in Fig. 4c, with g eff extracted from the linear fit indicated by the solid lines (for the full field-dependent voltage-bias spectroscopy data, see Supplementary Information Section S4D). The exchange mechanism acts locally on each quantum dot and we take g eff ≃ g 0 (1 + χ s ) with the bare value g 0 = 2. Using the data in Fig. 4c, we can extract the ‘excess’ spin susceptibility χ s and in Fig. 4e see that χ s ~ U with increasing dot size A and decreasing on-site interaction strength U (Supplementary Information Section S4D). This scaling is consistent with predictions for granular metallic systems and quantum dots in which the wavefunction is spread across the donor sites within each individual dot38,39,40,41.\n\nAs well as bias spectroscopy, we take measurements of the low-bias longitudinal conductance σ xx in devices D, E and F, shown in Fig. 4d, and observe a distinct two-step thermal activation. According to the theory of granular metals (Supplementary Information Section S4A,B), a transition from inelastic to elastic co-tunnelling of electrons in the arrays is set by the cross-over scale \\({k}_{{\\rm{B}}}{T}_{{\\rm{c}}}\\approx 0.2\\sqrt{\\delta U}\\) (refs. 35,37,42). Both regimes lead to a conductance described by the Efros–Shklovskii law \\({\\sigma }_{{\\rm{ES}}}(T)\\simeq {\\sigma }_{0}\\exp (\\,-\\sqrt{{T}_{0}/T})\\) (refs. 35,37,42,43,44) but with different activation temperatures \\({T}_{0}^{{\\rm{in}}}\\) and \\({T}_{0}^{{\\rm{el}}}\\) in the inelastic and elastic co-tunnelling regimes, respectively. Our data (Fig. 4d) show that we observe this two-step thermal activation in devices F and E and that the observed transition temperature (kink in the experimental data) closely coincides with the predicted cross-over temperature T c (vertical dashed lines). The observed activation temperatures \\({T}_{0}^{{\\rm{in}},{\\rm{el}}}\\) extracted from fits to the data are larger in the devices with stronger interactions and broadly conform with theory expectations35,37,42 (see Supplementary Information Section S4B).\n\nWithin the Mott–Hubbard/Anderson picture, the findings in Figs. 3 and 4 provide evidence of a MI transition and that the atom-based quantum dots in our arrays host electron states spread across the geometric area of individual dots, with wavefunction characteristics, inter-dot tunnelling and interaction energies that match design expectations. In Fig. 4e, we plot the dot level spacing and spin susceptibility extracted for the various devices, which served as important metrics to understand the underlying physics. Going forward, the ability to precision-engineer the size, shape and lattice spacing of the dots demonstrated in Fig. 1 provides an intricate level of control, to interpolate from few donors per dot to the metallic limit or to promote the impact of different electron orbitals. Likewise, we can target future devices to fall close to or far from the MI transition, while hosting coherent electrons up to a high temperature T c .\n\nFinally, we present measurements of the temperature-dependent Hall coefficient R H , which can be used to investigate subtle changes in the nature of charge transport in interacting electron systems16,17,18,19,20,21,33,34,45,46. Hall coefficient data for the conductive arrays A–C of Fig. 3 are plotted in Fig. 5, with arrays D, E and F being too resistive to reliably perform Hall measurements. At temperatures >20 K, the Hall coefficient saturates at a value that, for array A, matches a non-interacting picture with carrier density n H = 1/eR H close to the number of dopants in the device. The dashed lines in Fig. 5 indicate the expected non-interacting Hall coefficients assuming a doping density of 2 nm−2 within the quantum dots. For arrays B and C, the saturated Hall coefficient reflects a lower carrier density than expected from the number of dopants, with a stronger discrepancy the larger the inter-dot spacing a. This is probably because of the weaker tunnelling t, for which some electron states in the quantum dots split off and stop contributing to transport. In the context of analogue quantum simulation, we may equate this to a trend towards band insulation or electron localization in the artificial atom lattice.\n\nFig. 5: Hall coefficient data at low temperatures. The temperature-dependent Hall coefficient R H obtained from magneto-transport measurements for the three conductive devices A–C of Fig. 3. Horizontal dashed lines denote the value expected if each donor contributes one free electron, ignoring dot lattice and interaction effects. Vertical lines indicate temperatures T c,t,J for device C at which the physics of the electrons are predicted to undergo notable changes. Full size image\n\nMore pronounced effects are observed in the coherent tunnelling regime in the weakly insulating array C, at temperatures below T c ≈ 17 K. We find a sharp increase in the magnitude of the Hall coefficient at temperature T < 6 K and a distinct turnaround at a lower T < 2 K. Similar trends are predicted to arise owing to Fermi-surface reconstruction for correlated electrons33,34,45,46,47,48,49,50 or the formation of mini-bands in moiré superlattices16,17,18,19. The temperature scales T t ≈ 5.7 K and T J ≈ 0.7 K indicated in Fig. 5 are related to the inter-dot hopping t and exchange coupling J exc , respectively (Supplementary Information Section S4E). Here we refrain from drawing too close a comparison with either such systems. The devices studied by us exhibit several electron states per dot, chemical potential disorder, inter-site interactions and other characteristics that may affect the Hall coefficient in unknown ways. Hence it is difficult to conclusively link the data in Fig. 5 to a Fermi-surface reconstruction51,52. Note that we also observe differences between forward and backward magnetic field sweeps that hint at magnetic hysteresis in devices B and C (raw data and analysis in Supplementary Information Section S2B). Yet the behaviour of the Hall coefficient below the coherence temperature T c in device C (Fig. 5, pink) clearly differs from what is expected for a charge carrier freeze-out owing to non-correlated effects such as chemical potential disorder, for which we would find a gradual logarithmic increase of the Hall coefficient32,33,34,35,36,37,53. This indeed is what seems to happen in device B and has been observed in many granular metals or disordered doped semiconductors, including in continuously doped silicon phosphorus delta layers54,55,56.\n\nIn summary, we have presented a new class of analogue quantum simulators using precision-engineered atom-based quantum dots in silicon for simulating strongly interacting electron systems. Using the nanometre accuracy of STM hydrogen lithography, we pattern arrays containing 15,000 quantum dots with varying inter-dot separations (7.2 nm to 17.1 nm) and quantum dot areas (9 nm2 to 62 nm2) to simulate a Mott–Anderson MI transition. Embedding the quantum dot arrays in a Hall bar device geometry allows us to characterize their physics through a comprehensive suite of magneto-transport measurements. The observations reported here include a MI transition, the dot size dependence and exchange-interaction enhancement of the charge-transport gap, a temperature-driven cross-over from incoherent to coherent electron co-tunnelling in the insulating dot arrays and, finally, promising signatures of a Fermi-surface reconstruction witnessed by the temperature-dependent Hall coefficient. The presented data paints an encouraging picture for the use of atom-based quantum dots as a platform for large-scale analogue quantum simulation.\n\nHaving extensively tested and characterized our atom-based quantum dot arrays, we believe that this system has the potential to shed light on contemporary challenges in physics such as quantum spin liquids, interacting topological quantum matter and unconventional superconductivity. This is because of the unique capabilities of the physical platform, such as the ability to engineer large values of the interaction strengths U and V with a varying ratio U/t. As demonstrated in Fig. 1, we can implement arbitrary 2D lattice geometries as long as the dot edge length and inter-dot separation are larger than the silicon lattice constant (\\(a,\\sqrt{A}\\gg {a}_{0}=0.54\\,{\\rm{nm}}\\)) and engineer specific orbitals as valence states of individual quantum dots or multidot unit cells. This includes geometries that are hard to realize or altogether inaccessible in the ultracold atom and twisted moiré material platforms, such as the Lieb lattice or quasicrystals. An immediate next step in our experiments is the systematic exploration of electron correlation and Fermi-surface reconstruction effects hinted at by the Hall coefficient data in Fig. 5. On tuning the electron density by means of the top gate (see Fig. 1), we should be able to observe various band-filling or correlation effects16,17,18,19. Further, it may be possible to simulate a correlated state such as superconductivity that has very distinct transport signatures2,3 or to observe various kinds of magnetic order in the arrays57,58. Another interesting direction is the 2D Lieb lattice, as shown in Fig. 1b, with p-type and d-type ‘atomic’ orbitals on distinct sites and adjustable level detunings in the presence of strong interactions. Such a set-up mimics the Hubbard model description of a copper–oxygen plane in the cuprates, which is believed to lead to its coveted charge-transfer insulator and high-temperature superconducting states2,3,20,21,46.",
      "url": "https://www.nature.com/articles/s41586-025-10053-7",
      "source": "Nature",
      "published": "2026-02-05",
      "sentiment_score": 0.9,
      "reasoning": "The article reports a significant scientific breakthrough in analogue quantum simulation using large-scale, precision-engineered atom-based quantum dot arrays in silicon. This platform enables simulation of complex quantum materials and phenomena such as the Mott–Anderson metal-insulator transition with unprecedented scale (15,000 quantum dots) and precision. The work has broad implications for advancing understanding of strongly correlated electron systems, quantum materials, and potentially unconventional superconductivity, which are important for future quantum technologies and materials science. The article provides detailed context, experimental results, and future outlook, demonstrating substantial real-world impact and significance beyond a niche audience.",
      "category": "Technology",
      "personality_title": "Scientists create large, precise quantum dot arrays to simulate complex materials",
      "personality_presentation": "**Context** – Scientists study quantum materials, where tiny particles called electrons interact strongly and create unusual behaviors. To understand these materials better, researchers use quantum simulators, devices that mimic how electrons act. Making large, accurate simulators has been very challenging.\n\n**What happened** – A team developed a new quantum simulator using 15,000 tiny quantum dots made from phosphorus atoms placed precisely in silicon. They used a special microscope technique to build these dots in exact patterns on a flat surface. This large array lets them study how electrons move and interact, including a key change called the metal-insulator transition, where a material switches from conducting electricity to blocking it.\n\n**Impact** – This is the largest and most precise simulator of its kind, allowing scientists to explore complex quantum effects that were hard to study before. By changing the size and spacing of the dots, they can control how strongly electrons interact and observe different behaviors. This helps us understand materials that could lead to new technologies, like better electronics or superconductors.\n\n**What's next step** – Researchers plan to explore more complex patterns and behaviors, such as magnetic effects and superconductivity, by adjusting the simulator’s design. They also want to study changes in electron behavior hinted at in their experiments. This work opens the door to simulating many quantum materials that are difficult to study with other methods.\n\n**One-sentence takeaway** – A new quantum simulator built from thousands of precisely placed atom-sized dots offers a powerful way to study complex materials and electron behaviors on a large scale.\n",
      "personality_title_fr": "Des scientifiques créent de grands réseaux précis de points quantiques pour simuler des matériaux complexes",
      "personality_presentation_fr": "**Contexte** – Les scientifiques étudient des matériaux quantiques où de petites particules appelées électrons interagissent fortement et créent des comportements inhabituels. Pour mieux comprendre ces matériaux, ils utilisent des simulateurs quantiques, des dispositifs qui imitent le comportement des électrons. Construire de grands simulateurs précis est très difficile.\n\n**Ce qui s'est passé** – Une équipe a développé un nouveau simulateur quantique utilisant 15 000 petits points quantiques faits d'atomes de phosphore placés avec précision dans du silicium. Ils ont utilisé une technique de microscope spéciale pour construire ces points en motifs exacts sur une surface plane. Ce grand réseau leur permet d’étudier comment les électrons se déplacent et interagissent, notamment une transition clé appelée transition métal-isolant, où un matériau passe de conducteur à isolant.\n\n**Impact** – C’est le plus grand et le plus précis simulateur de ce type, permettant aux scientifiques d’explorer des effets quantiques complexes difficiles à étudier auparavant. En changeant la taille et l’espacement des points, ils peuvent contrôler la force des interactions entre électrons et observer différents comportements. Cela aide à mieux comprendre des matériaux qui pourraient mener à de nouvelles technologies, comme de meilleurs composants électroniques ou des supraconducteurs.\n\n**Prochaine étape** – Les chercheurs prévoient d’explorer des motifs et comportements plus complexes, comme les effets magnétiques et la supraconductivité, en ajustant la conception du simulateur. Ils veulent aussi étudier les changements dans le comportement des électrons suggérés par leurs expériences. Ce travail ouvre la voie à la simulation de nombreux matériaux quantiques difficiles à étudier autrement.\n\n**Résumé en une phrase** – Un nouveau simulateur quantique composé de milliers de points atomiques placés avec précision offre un moyen puissant d’étudier à grande échelle les matériaux complexes et les comportements électroniques.\n",
      "personality_title_es": "Científicos crean grandes y precisas matrices de puntos cuánticos para simular materiales complejos",
      "personality_presentation_es": "**Contexto** – Los científicos estudian materiales cuánticos donde pequeñas partículas llamadas electrones interactúan fuertemente y crean comportamientos inusuales. Para entender mejor estos materiales, usan simuladores cuánticos, dispositivos que imitan cómo actúan los electrones. Construir simuladores grandes y precisos ha sido muy difícil.\n\n**Qué pasó** – Un equipo desarrolló un nuevo simulador cuántico usando 15,000 pequeños puntos cuánticos hechos de átomos de fósforo colocados con precisión en silicio. Usaron una técnica especial de microscopio para construir estos puntos en patrones exactos sobre una superficie plana. Esta gran matriz les permite estudiar cómo se mueven e interactúan los electrones, incluyendo un cambio clave llamado transición metal-aislante, donde un material pasa de conductor a aislante.\n\n**Impacto** – Este es el simulador más grande y preciso de su tipo, que permite a los científicos explorar efectos cuánticos complejos que antes eran difíciles de estudiar. Cambiando el tamaño y la separación de los puntos, pueden controlar la fuerza de las interacciones electrónicas y observar diferentes comportamientos. Esto ayuda a entender materiales que podrían llevar a nuevas tecnologías, como mejores componentes electrónicos o superconductores.\n\n**Próximo paso** – Los investigadores planean explorar patrones y comportamientos más complejos, como efectos magnéticos y superconductividad, ajustando el diseño del simulador. También quieren estudiar cambios en el comportamiento electrónico que sugieren sus experimentos. Este trabajo abre la puerta a simular muchos materiales cuánticos difíciles de estudiar con otros métodos.\n\n**Resumen en una frase** – Un nuevo simulador cuántico hecho de miles de puntos atómicos colocados con precisión ofrece una forma poderosa de estudiar materiales complejos y comportamientos electrónicos a gran escala.\n",
      "image_url": "public/images/news_image_Large-scale-analogue-quantum-simulation-using-atom.png",
      "image_prompt": "A detailed, warm-toned painting of a vast, intricate grid of glowing, interconnected atom-sized dots arranged in various precise 2D lattice patterns—such as square, hexagonal, and Lieb lattices—set against a softly illuminated silicon crystal background, with subtle abstract waves symbolizing electron interactions flowing between the dots, conveying the precision and scale of quantum dot arrays simulating complex quantum states."
    },
    {
      "title": "Synthesizing scientific literature with retrieval-augmented language models",
      "summary": "Nature, Published online: 04 February 2026; doi:10.1038/s41586-025-10072-4A specialized, open-source, retrieval-augmented language model is introduced for answering scientific queries and synthesizing literature, the responses of which are shown to be preferred by human evaluations over expert-written answers.",
      "content": "OpenScholar\n\nOpenScholar (detailed in Extended Data Fig. 1) is a new retrieval-augmented LM designed to ensure reliable, high-quality responses to a range of information-seeking queries about scientific literature.\n\nTask formulation and challenges\n\nGiven a scientific query x, the task is to identify relevant papers, synthesize their findings and generate a response y that effectively addresses the query. This response should be accompanied by a set of citations, C = c 1 , c 2 ,…, c K , in which each citation c i corresponds to an existing scientific paper. Each c i in C corresponds to specific passages from scientific literature and should be provided as an in-line citation, linked to the relevant spans of text in y, following standard practice in scientific writing. These citations allow researchers to trace the output back to the original literature, ensuring transparency and verifiability.\n\nHowever, this task presents several challenges: (1) retrieving high-recall, high-precision scientific content from a vast, domain-specific corpus; (2) synthesizing accurate, non-hallucinated responses grounded in the retrieved evidence; and (3) producing citation-aware outputs that align generated text with appropriate references at a fine-grained level. A further challenge lies in the scarcity of resources: to our knowledge, there is limited availability of large-scale, up-to-date scientific corpora, especially those suitable for dense retrieval, as well as a lack of supervised training data for both retrieval and generation in scientific domains.\n\nOverview of OpenScholar\n\nTo address these challenges, OpenScholar introduces several key innovations that extend the standard RAG (refs. 1,5) model for scientific literature synthesis. Specifically, OpenScholar combines domain-specialized retrieval, citation-aware generation and a new self-feedback inference mechanism, all built on top of a fully open and large-scale scientific data store.\n\nFormally, OpenScholar consists of three key components: a data store D, a retriever \\({R}\\) and a generator LM \\({G}\\). In standard retrieval-augmented inference pipelines, the process begins with \\({R}\\), which retrieves a set of passages P = {p 1 , p 2 ,…, p N } from D—a large-scale corpus of previously published scientific papers—based on semantic relevance to the input query x. These passages serve as context for the next step. The generator LM \\({G}\\) then takes both the retrieved passages P and the input query x to produce the output y along with corresponding citations C. Formally, this process can be represented as:\n\n$$y,{\\bf{C}}={G}(x,{R}(x,{\\bf{D}})),$$\n\nin which each c i in C corresponds to a specific passage from P.\n\nOpenScholar introduces new technical contributions to address the aforementioned challenges. (1) To address the lack of large-scale, up-to-date scientific corpora, we construct OSDS, a database of 45 million scientific papers with precomputed dense embeddings, representing, to our knowledge, the largest and most up to date scientific paper data store available. (2) To enable high-recall, high-precision retrieval and support LM training in scientific domains, we design a retrieval pipeline that integrates both our trained OpenScholar retriever and OpenScholar reranker, optimized on scientific data to select the top N passages for the generator \\({G}\\)—and complementary retrieval APIs—ensuring broader coverage and improved relevance. (3) To improve factuality and evidence grounding, we introduce iterative self-feedback inference with retrieval and citation verification, in which the LM first produces an initial draft y 0 with \\({G}\\) and then iteratively refines it using retrieval-augmented self-feedback. (4) To enhance citation accuracy and overall output quality, we use this inference pipeline to generate high-quality training data, enabling the training of specialized LMs that produce more accurate and citation-aware long-form answers.\n\nOpenScholar retrieval pipeline\n\nExtended Data Fig. 1 (top left) shows our retrieval pipeline, consisting of a data store D, a bi-encoder retriever θ bi and a cross-encoder reranker θ cross . We first select initial candidate paragraphs using D and θ bi , as well as external APIs, and then refine and identify the top N relevant paragraphs using θ cross .\n\nScientific paper collection and data store construction\n\nAlthough previous work often used a small subset of scientific papers, such as arXiv papers from 2023 to 2024 (ref. 9), it is important to have a diverse set of papers to improve the quality and coverage of model generation23. For this, we use peS2o (ref. 24) as our retrieval source, which consists of open-access academic papers from S2ORC (ref. 25). We built our data store using peS2o v3, which includes 45 million papers up to October 2024. For evaluations, we use peS2o v2, which consists of papers up to January 2023, as our main benchmarks and models were constructed before the curation of peS2o v3. Our data store, which we call OSDS, consists of 236 million passages. To our knowledge, this is the largest open-sourced data store for scientific literature.\n\nInitial paragraph retrieval\n\nWe retrieve passages from three sources: (1) the OSDS using our trained retriever; (2) publicly available abstracts from papers returned through the Semantic Scholar API (ref. 26) based on search keywords; and (3) publicly available texts from papers retrieved through a web search engine using the original query x.\n\nFor (1), we first generate embeddings of each passage in the OSDS D using the passage bi-encoder θ bi , which processes text chunks (for example, queries or passages) into dense vectors27 offline. Off-the-shelf retrieval models often struggle in out-of-domain scenarios28. To overcome this limitation, we develop θ bi by continually pre-training Contriever29 on the peS2o data store in an unsupervised fashion to improve domain-specific retrieval performance. During inference, we encode the query using θ bi and retrieve the top 70 passages through a nearest-neighbour search27. Following previous work23, we split the main text of each paper into discrete, 256-word text blocks (as determined by white space) and concatenate the paper title to each block to formulate passages in D. Although semantic segmentation can be used to split scientific articles into meaningful sections, we found that not all papers in our data store consistently retain such semantic or discourse structures. Furthermore, applying segmentation models post hoc would be computationally expensive at this scale. Therefore, following common practice in this area27,29, we divide articles into fixed-length chunks to ensure scalability and simplicity. Therefore, several text chunks from the same paper can be retrieved at inference time.\n\nFor (2), we first generate keywords from the query x using a generator LM. These keywords are then used to retrieve the top 10 papers for each, as ranked by citation count, through the Semantic Scholar search API. This approach addresses a limitation of the Semantic Scholar API, which cannot effectively handle long, question-like search queries. If the full text is available in HTML format (for example, ar5iv), we retrieve the entire text and include all passages from the paper as candidate documents. Otherwise, we only consider the abstract.\n\nFor (3), we obtain the top 10 search results using the You.com retrieval API, restricting the search to academic platforms such as arXiv and PubMed. Similarly to (2), if the papers are open access, we extract and add their full texts to the candidate pool; otherwise, we include only their abstracts.\n\nTop N paragraph reranking and finalization\n\nAfter the initial stage, we have gathered more than a hundred or even a thousand relevant passages per query. However, passages retrieved by the bi-encoder may include unhelpful context owing to deep interactions between a query and passages, as they are encoded separately30. Feeding a large number of documents that might include irrelevant content to LLMs can cause efficiency and performance issues, even with state-of-the-art models31,32. To overcome these challenges, we use a cross-encoder reranker33,34, denoted as θ cross . For each candidate paragraph, the cross-encoder reranker jointly encodes and computes the relevance score between the input query and each of the passages. We then use the relevance score to rank the passages accordingly. To train θ cross for scientific domains, we fine-tune a BGE reranker34 using synthetic data generated by Llama-3-70B-Instruct. Specifically, we randomly generate queries based on abstracts from peS2o and retrieve the top 10 passages. For each passage, Llama-3-70B-Instruct assigns a relevance score from 1 to 5, for which we consider scores of 4 or 5 as positive and scores of 1 or 2 as negative. Passages with a score of 3 are discarded. More details of θ cross training are in Supplementary Information Section 3.3. During reranking and finalization of the top N passages, we also implement extra meta-filtering, which includes: (1) limiting the number of passages per paper to three passages and (2) incorporating normalized citation counts into relevance scores predicted by the cross-encoder.\n\nInference: self-reflective iterative RAG\n\nIn standard RAG (refs. 5,35), a generator LM takes in the original input x and top N retrieved passages P and generates the output y 0 . Although effective for tasks such as question answering2, this one-step generation can lead to unsupported claims36 or incomplete output owing to missing information7,37. To address these challenges, in OpenScholar, we introduce an iterative generation approach with self-feedback, which involves three steps: (1) initial response and feedback generation to output the initial draft y 0 and a set of feedback on y 0 ; (2) iterative refinement with further retrieval to improve y 0 using the feedback; and (3) citation verification. Our inference is detailed in Extended Data Fig. 1, top right.\n\nInitial response and feedback generation\n\nGiven the input x and retrieved passages P, the generator LM first produces an initial response y 0 with citation markers tied to the corresponding passages in P. After generating y 0 , the LM generates a set of feedback on y 0 , F = f 1 , f 2 ,…, f T , that is aimed at improving the initial response, in which each feedback f t is a natural language sentence that describes potential improvements. Although the model can generate an arbitrary number of feedback (T), we set a maximum limit of three feedback sentences for efficient inference. Unlike previous work that relies on a predefined set of feedback signals7, our approach allows the LM to generate flexible natural language feedback on various aspects of the response, such as organization, completeness or further required information. If the feedback sequence identifies missing content (for example, “The answer only includes empirical results on QA tasks. Add results from other task types.”), the LM also generates a retrieval query for further retrieval using the pipeline.\n\nIterative refinement\n\nWe then iterate over the feedback F to incrementally refine the output. If f k indicates that further retrieval is needed, the query q k is used to retrieve extra passages, which are appended to P before producing y k . Although we could iteratively regenerate the output each time feedback is provided, doing so introduces more latency. Empirically, we found that feedback is often diverse, addressing different aspects of generation. As a result, sequentially incorporating feedback from the initial output remains effective. The LM uses the previous output y k−1 , the retrieved passages P and newly retrieved passages, if any, to generate a revised output y k . This process is repeated until all feedback has been addressed, resulting in a final output y T by time step T.\n\nCitation verification\n\nFinally, we instruct the generator LM to verify the citations in y t . Specifically, the generator ensures that all citation-worthy statements—scientific claims requiring justification—are adequately supported by references from the retrieved passages. If any claims lack proper citations, the LM performs a post-hoc insertion to ensure that citation-worthy statements are supported by passages. In our pipeline, we do not remove sentences that lack citation-worthy statements.\n\nSynthetic training data generation with inference pipeline\n\nBuilding powerful LMs that can effectively synthesize scientific literature is challenging because of the lack of training data for this problem. Although there are some resources to train scientific LMs38, most tasks do not require open-retrieval settings and are single-paper tasks. As a result, most previous work in this area10 relies on proprietary LMs, which poses challenges for reproducibility and inference costs.\n\nWe use our inference-time pipeline to synthetically generate high-quality training data through self-feedback, so that the resulting model can get better at generating higher-quality output without going through the self-feedback process (Extended Data Fig. 1, bottom).\n\nQuestion and response generations\n\nOur data generation process involves three steps: first, selecting the top-cited papers from D; second, generating information-seeking queries based on their abstracts; and third, using the OpenScholar inference-time pipeline to produce high-quality responses. We generate data using Llama 3.1 70B (ref. 17). Specifically, we begin by sampling 1 million paper abstracts from the peS2o dataset and gathering their corresponding metadata, such as publication year or citation count. We then randomly select 10,000 papers that were published after 2017 and prompt a LM to generate literature review questions or information-seeking queries based on each abstract that require several papers to answer. Next, we use our OpenScholar pipeline to produce the final output y T , along with intermediate generations such as feedback F and initial outputs.\n\nData filtering\n\nDespite its effectiveness and scalability, synthetic data may also contain issues such as hallucinations, repetitive writing or limited instruction-following39. To address this, we introduce a two-step data filtering process: pairwise filtering and rubric filtering, using the same LM as for data generation. In pairwise filtering, we compare the quality of model outputs y T (output at the final step) and y 0 (initial output) and retain the output that is judged to be higher quality. We find that y 0 is preferred over y T around 20% of the time, owing to over-editing or increased redundancy after several iteration steps. We then evaluate the quality of the chosen response on a five-point scale across two aspects: organization and factual precision and citation accuracy. A valid model output must achieve a score of 4.5 or higher in both categories and we discard instances whose outputs do not meet this requirement.\n\nData mixing and training\n\nFrom this synthetic pipeline, we generate three types of training data: answer generation (x → y), feedback generation (y 0 → F) and feedback incorporation (y t−1 , f t → y t ). We found that incorporating both final and intermediate outputs during training helps smaller LMs learn to generate more effective feedback. We further blend this synthetic training data with existing general-domain instruction-tuning data40 and scientific instruction-tuning data38, ensuring that 50% of the training data come from scientific domains, whereas the remaining 50% is sourced from general-domain data. We also generate synthetic fact verification and Boolean QA data based on sampled abstract data from peS2o. For this, we sort the papers based on citation count and select the top 100,000 papers. After data mixing, we train generator LMs on our large-scale synthetic training data. We train Llama-3.1-8B-Instruct on the generated training data.\n\nOpenScholar experimental details\n\nWe use peS2o v2 as D, our default data store. For θ bi and θ cross in OpenScholar, we use our trained bi-encoder and cross-encoder models, which consist of 110 million and 340 million parameters, respectively. We analysed various cross-encoder and bi-encoder models on a customized synthetic benchmark and found that OpenScholar retriever (bi-encoder) and OpenScholar reranker (cross-encoder) achieved the highest normalized discounted cumulative gain among models of comparable size (Supplementary Information Section 5.2. We set the maximum number of papers from web search and Semantic Scholar to 10. For the generator LMs, we set the temperature to 0.7 and limit the maximum token count to 3,000 for response generation and 1,000 for feedback generation and use the vLLM package for faster inference. We trained Llama 3.1 8B for two epochs on 130,000 training instances for two epochs. For all models, we set the number of passages input into the generator LM to five for single-paper tasks and ten for multi-paper tasks. No few-shot demonstrations are provided, except for SciFact and PubMed, for which we include one-shot demonstrations. OpenScholar responses are marked with special decorators Response_Start and Response_End and citations are indicated as reference numbers (for example, [1]), which correspond to the reference documents provided in the context. We do not add any new special tokens to the model vocabulary; instead, we use these decorators as regular strings. After training, we observe that the model can generate the correct tokens as intended.\n\nScholarQABench\n\nChallenges and overview\n\nPrevious studies on building LMs to synthesize scientific literature use either small-scale, single-domain human evaluation8,9 or oversimplified multiple-choice QA set-ups10. Building high-quality benchmarks for literature review has two main challenges. First, creating such datasets is resource-intensive, as it requires PhD-level domain expertise and research experience, particularly when annotating realistic questions and high-quality answers. Second, even when high-quality data are available, reliably evaluating long-form natural language responses presents a notable challenge, especially in expert domains13,14. This contrasts with benchmarks for other scientific processes, such as automated experimental code generation, for which clearer evaluation criteria, such as pass@1, are more readily available45.\n\nTo address these gaps, we introduce ScholarQABench, a benchmark that supports diverse formats of scientific literature synthesis tasks, including closed-form classification, multiple-choice and long-form generation, as shown in Extended Data Table 1. We use three existing single-paper datasets and then construct a suite of high-quality, expert-annotated datasets for computer science, biomedicine, physics and neuroscience. We also built a reliable automatic evaluation pipeline. Extended Data Fig. 2 shows an example and an overview of the evaluation pipeline.\n\nData curation\n\nScholarQABench is designed to evaluate model capabilities in automating scientific literature review. The curation process is guided by three key factors. Diversity of tasks: ScholarQABench includes tasks with a range of input-output formats. Diversity of disciplines: unlike previous analyses that often focus on a single discipline such as computer science, ScholarQABench spans four scientific disciplines. Inclusion of multi-paper tasks: unlike previous work that focuses on understanding single, preselected papers, all tasks require retrieving from the entire open-access collection of full texts of papers and four datasets specifically require reasoning over several retrieved papers. As a result, ScholarQABench is the first multidisciplinary literature synthesis benchmark that requires long-form generation grounded in several recent papers, with all examples annotated by PhD-level experts. This sets it apart from previous datasets that focus on short-form or multiple-choice answers or rely on static scientific knowledge reasoning10,11,12,46, as well as those that lack expert-annotated refs. 13,47.\n\nNote that our benchmark is designed for single-turn set-ups and does not include multi-turn follow-up questions and answers in dynamic evaluations48. Evaluating multi-turn LM–human interactions remains challenging49, so we begin with a single-turn, static evaluation set-up as a first step towards more realistic assessments of such systems.\n\nSingle-paper tasks\n\nSciFact\n\nSciFact42 is a dataset of 1,400 expert-written scientific claims in the biomedical domain, paired with gold evidence from existing PubMed paper abstracts annotated with labels and rationales. We include validation set queries labelled as either ‘supports’ (true) or ‘contradicts’ (false), discarding the original gold evidence, and reformulate the task as binary open retrieval, in which a system needs to identify relevant papers from a large collection of papers.\n\nPubMedQA\n\nPubMedQA41 has expert-annotated (yes/no/maybe) QA data on PubMed paper abstracts. Similarly to SciFact, we only keep instances with yes or no labels and discard the original abstract passage to formulate the task as an open-retrieval set-up.\n\nQASA\n\nQASA43 is a single-paper QA dataset that consists of question answering pairs, requiring reasoning over scientific articles in artificial intelligence and machine learning. We evaluate the ability of the model to sufficiently answer a detailed question about the target paper. Although the original dataset provides three subtasks (answer selection, rationale generation and answer compositions) as well as end-to-end QA, we evaluate the performance of the models based on an end-to-end QA set-up.\n\nMulti-paper tasks\n\nSingle-paper, closed-set tasks may provide reliable evaluations. However, they may not be reflective of realistic scenarios, in which complex, open-ended questions are asked independently from existing papers and require multi-paper retrieval and reasoning. Few datasets13,47 explore multi-paper set-ups with realistic queries and most lack a reliable evaluation pipeline or human-written references. We address this gap by recruiting expert-level annotators across several scientific disciplines and curating three new long-form QA datasets for this challenging setting. All answers are written by PhD-level experts, with each taking approximately one hour to compose, reflecting the demanding nature of the task. Details of our annotation process, including compensation (US$30–45 per hour on average), are provided in Supplementary Information Section 2.3. The process was approved by the ethics board (institutional review board) as exempt research. Data collection took place between April and October 2024 and all reference answers (where applicable) are grounded in scientific literature published up to October 2024. Below, we discuss each subset of the four multi-paper tasks, which span four broad scientific disciplines.\n\nScholar-CS\n\nWe collected 100 questions along with detailed answer rubrics for each question across various computer science disciplines by recruiting expert annotators holding PhDs in the field (professors, postdoctoral researchers and research scientists). Annotators were tasked with writing literature review questions that require several research papers to answer. The question topics span areas such as networks, algorithms, the Internet of things, artificial intelligence and human–computer interaction. Then, for each question, two other annotators searched the web to produce a rubric listing the key ingredients for a correct answer, categorized by importance (‘must have’ and ‘nice to have’), along with supporting quotes from sources for each ingredient. The annotators were instructed not to use any LLM services for this initial part of the task. After the initial web search, the annotators were shown corresponding responses from four LLM services (Claude 3.5 Sonnet, GPT-4o, Perplexity Pro and an unpublished RAG prototype based on Claude 3.5) in a randomized order in case they wanted to revise their rubrics. On average, each question is annotated with 4.4 key ingredients, each supported by 4.4 quotes. Furthermore, we collected 31 expert-written long-form answers, authored by a separate pool of PhD-level annotators, to serve as a measure of expert human performance.\n\nTo measure agreement, we had both annotators produce rubrics for a subset of ten randomly sampled questions. We then compute the scores for responses from the four LLM services to which the annotators were exposed using our automated approach, once for each set of annotator rubrics. Finally, we calculate Pearson’s correlation coefficient among the scores for each question and compute the average. Given the subjectivity of rubric annotation, we assess agreement both with and without the general criterion included in the scores, resulting in values of 79.3 and 59.5, respectively. Extended Data Fig. 1 shows an example.\n\nScholar-Bio and Scholar-Neuro\n\nWe further collected 2,759 expert-written literature review questions in biomedicine and neuroscience, recruiting six experts who have a PhD in relevant areas and are at present research scientists and engineers. The annotators were asked to choose papers from their area of expertise and generate complex scientific questions that biomedical scientists might reasonably ask about the scientific literature based on their parsing of those papers. We collected questions from different areas, such as bioimaging, genetics, microbiology and neuromodulation, for each. Owing to the cost of annotation, we focused only on curating the questions.\n\nScholar-Multi\n\nLast, we collected 108 literature review questions and expert-written answers with citations in three domains: computer science (artificial intelligence/machine learning, human–computer interaction), biomedicine (bioimaging, genetics) and physics (astrophysics, photonics, biophysics). All annotations are conducted by PhD students or postdoctoral scientists who have more than three years of research experience in the corresponding areas and have several first-author publications. We asked them to come up with questions that are related to the most recent literature and to compose answers to the questions using relevant papers that they found by means of a search. Our annotators were instructed not to use any LLM-based systems such as ChatGPT and told to only use general search (for example, Google Search) or paper search (for example, Semantic Scholar) systems. Statistics of collected questions are available in Table 3, The distribution of subjects is shown in Supplementary Information Fig. 1, along with the average annotation time per subject. We show several examples in Supplementary Information Figs. 12–15. On average, each annotator spent 56 minutes per instance.\n\nMetrics and evaluation protocols\n\nWe developed a multifaceted automatic evaluation pipeline to facilitate reproducible and efficient evaluations, complementing expert assessments. An overview of our evaluations is in Extended Data Fig. 2.\n\nCorrectness\n\nCorrectness evaluates the degree of overlap or agreement between model-generated answers and human-annotated reference answers. This metric is applied only to tasks for which reference answers are available. For single-paper tasks, we directly compare the model outputs to gold reference texts, following the evaluation methodologies proposed in previous work41,42,43. We refer to this metric as accuracy for simplicity. For SciFact and PubMedQA, which have fixed answer classes, we use exact match as the correctness metric. For QASA, we use ROUGE-L as an evaluation metric, following ref. 43.\n\nHowever, such approaches that rely on a single reference answer often fail to capture all valid outputs, especially in tasks requiring long-form answers synthesized from several papers, such as our multi-paper tasks. To address this, we introduce a new correctness evaluation framework based on Scholar-CS’s expert-annotated rubrics, which we refer to as rubric score (rubric-based evaluation). Specifically, we combine two components: annotation-driven criteria (60%), which assess the presence of key content elements (‘ingredients’) identified by annotators as necessary for a good answer, and general criteria (40% of the score), which evaluate aspects such as length, domain expertise, citation quality and use of supporting excerpts. GPT-4o Turbo scores each criterion and we compute a weighted sum to obtain the final correctness score. We conducted expert evaluations to measure the agreement between human and LLM judges on whether a rubric item was satisfied by a LLM-generated answer, using outputs from two LM systems and two expert annotators. The average agreement between the two human annotators was 0.80, whereas the average agreement between a human annotator and the LLM judge was 0.79. We conducted an analysis on the agreement between an evaluator, LM and a human, and the average correlation between humans was 0.62 and the average correlation between humans and the LLM judge was 0.81. More details are in Supplementary Information Section 2.3.1.\n\nCitation accuracy\n\nEvaluating long-form responses to literature review questions requires citation accuracy: LMs should correctly attribute relevant evidence for all citation-worthy statements. In ScholarQABench, all systems generate outputs with reference numbers (for example, [1], [2]) linked to passages provided during inference. Following previous work36,50, we check whether each citation-worthy statement has appropriate citations and whether the citations support the statement (citation recall). For each citation, we then verify its relevance and necessity—specifically, whether the citation supports the statement and whether its removal affects the integrity of remaining citations (citation precision). Finally, we compute citation F1 and use it as a primary metric for citation accuracy. Citation accuracy does not require gold reference answers or rubrics, so we apply this evaluation across all tasks. More details are in Supplementary Information Section 2.3.3.\n\nContent quality and organization on Scholar-Multi\n\nWe extend our evaluation beyond correctness and citation accuracy by defining further key aspects: relevance to the question, coverage in terms of topic breadth (for example, diversity of discussed papers) and depth (for example, sufficiency of details) and organization and writing flow. These aspects are difficult to capture using standard automatic metrics. We developed detailed instructions and five-point rubrics for each aspect and applied the same rubrics to both LLM and expert human evaluations. For the LLM judge, we use Prometheus v2 (ref. 44), a state-of-the-art open-source model for fine-grained evaluation, chosen to ensure reproducibility and avoid the instability and cost issues associated with proprietary models51. For human evaluations, when conducted by expert annotators on those three aspects, we also assess overall usefulness (usefulness). As previous studies show that LLM judges are less reliable when gold reference answers are not available52, this evaluation is only applied to a task with human-annotated reference answers, namely Scholar-Multi. We analysed the agreement between human and model assessments on fine-grained aspects. We found that, although the model and humans sometimes disagreed on adjacent categories—particularly between scores of 4 and 5—the evaluations of the model aligned well with human rankings and its accuracy on a collapsed three-point rating exceeded 80% across different aspects and subject LMs. More details are in Supplementary Information Section 2.3.2.\n\nRelated work\n\nScientific LMs\n\nScientific LMs have spanned various domains, including biomedical53,54,55, medical56,57,58,59, biomedical60,61,62, geoscience63 and astronomy64, with some models such as SciGLM65 and Uni-SMART66 that aim to cover diverse scientific domains in a single model. Recently, several works show that powerful general-purpose LLMs can also show strong capabilities in scientific tasks, such as medical question answering56,67, chemistry experimentation68 and applied mechanics69. However, the reliance of a LM on information memorized within its parameters leads to frequent hallucinations in its output70.\n\nLMs to assist scientists\n\nRecent studies have also examined the capabilities of LLMs to assist scientists in performing a range of scientific procedures, including generating new research ideas71,72 and automating experimental code generation73,74. Our work, however, focuses specifically on benchmarking and developing methods for automating literature reviews and addressing questions related to up-to-date research—tasks that are crucial to, and particularly challenging for, scientific inquiry. Several concurrent studies have attempted to build retrieval-augmented pipelines using proprietary LLMs and external APIs (for example, the Semantic Scholar API) for scientific literature review agents8,10,75. Although these studies and our research all explore the potential of retrieval-augmented LMs in automating literature synthesis, previous works often relied on proprietary, black-box systems and limited evaluations, which commonly entail small-scale human evaluation or simplified set-ups such as multiple-choice QA. By contrast, our work introduces a comprehensive benchmark with automated metrics, involves user studies with experts across three scientific disciplines and develops new methodologies to train specialized open models. OpenScholar greatly outperforms previously introduced systems and shows superiority over human experts in five domains.\n\nBenchmarks for scientific literature understanding\n\nSeveral works have developed benchmarks to evaluate the abilities of models to understand scientific literature. Previous datasets, such as SciFact42, QASPER76 and QASA43, largely focus on single-paper settings, in which the necessary information to answer queries is contained within a single preselected paper. However, in real-world scenarios, experts often need to synthesize information from several papers to answer questions. To address this gap, ScholarQABench introduces newly annotated tasks that require reasoning across several papers. There are also scientific summarization tasks, such as Multi-XScience77, in which models are provided with several papers and asked to generate summaries, typically based on the related work sections of those papers. However, in this work, we focus on scenarios in which the relevant papers are not specified in advance, making the task more challenging. Recently, Xu et al.13 introduced KIWI, a dataset containing 200 questions and human-verified or edited answers generated by state-of-the-art LLMs, with a focus on the natural language processing domain. KIWI also provides a set of relevant papers that models must consider. Although both KIWI and ScholarQABench feature multi-paper, information-seeking tasks, ScholarQABench includes both human-written answers and automatic evaluation pipelines. By contrast, KIWI focuses more on human evaluations and its reference answers are primarily model-generated.",
      "url": "https://www.nature.com/articles/s41586-025-10072-4",
      "source": "Nature",
      "published": "2026-02-05",
      "sentiment_score": 0.85,
      "reasoning": "The article reports the development of OpenScholar, an advanced open-source retrieval-augmented language model designed to synthesize scientific literature with high accuracy, citation awareness, and iterative self-feedback. This represents a significant technological breakthrough with broad impact on scientific research by improving literature review automation, enabling researchers across multiple disciplines to access and synthesize up-to-date scientific knowledge efficiently. The article provides detailed context on the model's architecture, data sources, evaluation benchmarks, and expert validation, demonstrating substantial substance and real-world benefit beyond niche applications.",
      "category": "Technology",
      "personality_title": "OpenScholar: A new AI tool that reads and summarizes scientific papers accurately",
      "personality_presentation": "**Context** – Scientists often need to read many research papers to answer complex questions, but this process can be slow and difficult. Existing AI tools sometimes give wrong or incomplete answers and don’t always show where their information comes from.\n\n**What happened** – Researchers introduced OpenScholar, an open-source AI model that searches through millions of scientific papers, picks the most relevant parts, and writes clear answers with proper citations. It uses a special method that lets it check and improve its own answers step-by-step. OpenScholar was tested across different fields like computer science, biology, and physics, and experts found its answers better than those written by humans or other AI tools.\n\n**Impact** – OpenScholar is unique because it combines a huge database of 45 million papers, smart search tools, and a way to double-check its answers with exact citations. This helps scientists trust its responses and saves time in literature reviews. The model’s ability to improve answers by itself also reduces mistakes and missing information, making it a powerful helper for researchers.\n\n**What's next step** – The team plans to keep improving OpenScholar by adding more up-to-date papers and expanding its training data. They also created ScholarQABench, a set of challenging tests with expert-written questions and answers, to track progress. This will help researchers develop even better AI systems for understanding scientific knowledge.\n\n**One-sentence takeaway** – OpenScholar is an advanced, open AI system that accurately reads, summarizes, and cites scientific papers, helping researchers find trusted answers faster.\n\n",
      "personality_title_fr": "OpenScholar : un nouvel outil d’IA qui lit et résume précisément les articles scientifiques",
      "personality_presentation_fr": "**Contexte** – Les scientifiques doivent souvent lire de nombreux articles de recherche pour répondre à des questions complexes, ce qui peut être long et difficile. Les outils d’IA existants donnent parfois des réponses erronées ou incomplètes et ne montrent pas toujours leurs sources.\n\n**Ce qui s’est passé** – Des chercheurs ont créé OpenScholar, un modèle d’IA open-source qui parcourt des millions d’articles scientifiques, sélectionne les passages les plus pertinents et rédige des réponses claires avec des citations précises. Il utilise une méthode spéciale qui lui permet de vérifier et d’améliorer ses réponses étape par étape. OpenScholar a été testé dans plusieurs domaines comme l’informatique, la biologie et la physique, et les experts ont trouvé ses réponses meilleures que celles des humains ou d’autres IA.\n\n**Impact** – OpenScholar est unique car il combine une énorme base de données de 45 millions d’articles, des outils de recherche intelligents et un système pour vérifier les citations. Cela aide les scientifiques à faire confiance aux réponses et à gagner du temps dans leurs revues de littérature. Sa capacité à s’auto-corriger réduit aussi les erreurs et les informations manquantes, faisant de lui un assistant précieux.\n\n**Prochaine étape** – L’équipe prévoit d’améliorer OpenScholar en ajoutant des articles plus récents et en enrichissant ses données d’entraînement. Ils ont aussi créé ScholarQABench, un ensemble de tests difficiles avec des questions et réponses écrites par des experts, pour suivre les progrès. Cela aidera à développer des systèmes d’IA encore meilleurs pour comprendre la science.\n\n**Une phrase résumée** – OpenScholar est un système d’IA avancé et open-source qui lit, résume et cite précisément les articles scientifiques, aidant les chercheurs à trouver rapidement des réponses fiables.\n\n",
      "personality_title_es": "OpenScholar: una nueva herramienta de IA que lee y resume artículos científicos con precisión",
      "personality_presentation_es": "**Contexto** – Los científicos suelen necesitar leer muchos artículos para responder preguntas complejas, pero este proceso es lento y difícil. Las herramientas de IA actuales a veces ofrecen respuestas incorrectas o incompletas y no siempre muestran sus fuentes.\n\n**Qué pasó** – Investigadores presentaron OpenScholar, un modelo de IA de código abierto que busca entre millones de artículos científicos, selecciona las partes más relevantes y escribe respuestas claras con citas exactas. Usa un método especial que le permite revisar y mejorar sus respuestas paso a paso. OpenScholar fue probado en campos como informática, biología y física, y los expertos encontraron que sus respuestas superan a las humanas y a otras IA.\n\n**Impacto** – OpenScholar es único porque combina una base de datos enorme de 45 millones de artículos, herramientas inteligentes de búsqueda y un sistema para verificar citas. Esto ayuda a los científicos a confiar en sus respuestas y ahorra tiempo en revisiones bibliográficas. Su capacidad de auto-mejora reduce errores e información faltante, siendo un asistente valioso para investigadores.\n\n**Próximos pasos** – El equipo planea seguir mejorando OpenScholar añadiendo artículos más recientes y ampliando sus datos de entrenamiento. También crearon ScholarQABench, un conjunto de pruebas difíciles con preguntas y respuestas escritas por expertos, para medir avances. Esto ayudará a desarrollar mejores sistemas de IA para comprender la ciencia.\n\n**Resumen en una frase** – OpenScholar es un sistema avanzado y abierto de IA que lee, resume y cita artículos científicos con precisión, ayudando a investigadores a encontrar respuestas confiables más rápido.\n\n",
      "image_url": "public/images/news_image_Synthesizing-scientific-literature-with-retrieval-.png",
      "image_prompt": "An intricate, warmly lit library scene with countless open scientific papers and glowing, interconnected threads weaving between them like a neural network, symbolizing the synthesis and retrieval of knowledge, with abstract silhouettes of thoughtful owls perched nearby representing wisdom and reflection."
    },
    {
      "title": "Discovery Learning predicts battery cycle life from minimal experiments",
      "summary": "Nature, Published online: 04 February 2026; doi:10.1038/s41586-025-09951-7Discovery Learning, a machine learning approach integrating active learning, physics-guided learning and zero-shot learning into a human-like reasoning loop, is described for predicting the battery cycle life of new designs from minimal experiments, reducing prototyping requirements.",
      "content": "Fast and reliable validation of new designs in complex physical systems such as batteries is critical to accelerating technological innovation. However, battery development remains bottlenecked by the high time and energy costs required to evaluate the lifetime of new designs1,2. Notably, existing lifetime forecasting approaches require datasets containing battery lifetime labels for target designs to improve accuracy and cannot make reliable predictions before prototyping, thus limiting rapid feedback3,4. Here we introduce Discovery Learning, a scientific machine learning approach that integrates active learning5, physics-guided learning6 and zero-shot learning7 into a human-like reasoning loop, drawing inspiration from educational psychology. Discovery Learning can learn from historical battery designs and reduce the need for prototyping, thereby predicting the lifetime of new designs from minimal experiments. To test Discovery Learning, we present industrial-grade battery data comprising 123 large-format lithium-ion pouch cells, including diverse material–design combinations and cycling protocols. Trained on public datasets of cell designs different from ours, Discovery Learning achieves 7.2% test error in predicting cycle life using physical features from the first 50 cycles of 51% of cell prototypes. Under conservative assumptions, this results in savings of 98% in time and 95% in energy compared with conventional practices. Discovery Learning represents a key advance in accurate and efficient battery lifetime prediction and, more broadly, helps realize the promise of machine learning to accelerate scientific discovery8.",
      "url": "https://www.nature.com/articles/s41586-025-09951-7",
      "source": "Nature",
      "published": "2026-02-05",
      "sentiment_score": 0.85,
      "reasoning": "The article describes a significant technological breakthrough in battery development by using a novel machine learning approach called Discovery Learning. This method drastically reduces the time and energy required to predict battery cycle life, which has broad implications for accelerating innovation in energy storage technologies. The impact is meaningful for the general public as improved battery technology supports advancements in electric vehicles, renewable energy storage, and portable electronics. The article provides sufficient detail about the approach, testing, and quantified benefits, making it a substantive and inspiring technological advancement.",
      "category": "Technology",
      "personality_title": "New AI method predicts battery life with minimal testing",
      "personality_presentation": "**Context** – Testing how long batteries last is usually slow and uses a lot of energy. This makes it hard to quickly improve battery designs, which are important for things like electric cars and phones.\n\n**What happened** – Scientists created a new machine learning method called Discovery Learning. It uses smart techniques that mimic human thinking to predict how long a battery will last by running very few tests. They tested it on 123 real lithium-ion batteries and used information from just the first 50 charging cycles of about half of them.\n\n**Impact** – Discovery Learning predicts battery life with only 7.2% error, which is very accurate. It can save up to 98% of the time and 95% of the energy usually needed for testing. This means battery makers can develop better batteries faster and with less cost.\n\n**What's next step** – This method could be used by battery companies to speed up designing new batteries. It might also be adapted to help improve other complex technologies that need long testing times.\n\n**One-sentence takeaway** – Discovery Learning uses smart AI to predict battery life quickly and accurately, cutting testing time and energy by almost 100%.",
      "personality_title_fr": "Nouvelle méthode d’IA prédit la durée de vie des batteries avec peu de tests",
      "personality_presentation_fr": "**Contexte** – Tester la durée de vie des batteries est généralement lent et consomme beaucoup d’énergie. Cela complique l’amélioration rapide des batteries, essentielles pour les voitures électriques et les téléphones.\n\n**Ce qui s’est passé** – Des scientifiques ont créé une nouvelle méthode d’apprentissage automatique appelée Discovery Learning. Elle utilise des techniques intelligentes imitant la pensée humaine pour prédire la durée de vie d’une batterie en réalisant très peu de tests. Ils l’ont testée sur 123 batteries lithium-ion réelles, en utilisant les données des 50 premiers cycles de charge d’environ la moitié d’entre elles.\n\n**Impact** – Discovery Learning prédit la durée de vie des batteries avec une erreur de seulement 7,2 %, ce qui est très précis. Elle permet d’économiser jusqu’à 98 % du temps et 95 % de l’énergie normalement nécessaires aux tests. Cela signifie que les fabricants peuvent développer des batteries meilleures plus rapidement et à moindre coût.\n\n**Prochaine étape** – Cette méthode pourrait être utilisée par les fabricants pour accélérer la conception de nouvelles batteries. Elle pourrait aussi être adaptée pour améliorer d’autres technologies complexes nécessitant de longs tests.\n\n**Résumé en une phrase** – Discovery Learning utilise une IA intelligente pour prédire rapidement et précisément la durée de vie des batteries, réduisant presque totalement le temps et l’énergie des tests.",
      "personality_title_es": "Nuevo método de IA predice la vida útil de baterías con pocas pruebas",
      "personality_presentation_es": "**Contexto** – Probar cuánto duran las baterías suele ser lento y consumir mucha energía. Esto dificulta mejorar rápidamente los diseños de baterías, importantes para autos eléctricos y teléfonos.\n\n**Qué pasó** – Científicos crearon un nuevo método de aprendizaje automático llamado Discovery Learning. Usa técnicas inteligentes que imitan el razonamiento humano para predecir la vida útil de una batería con muy pocas pruebas. Lo probaron en 123 baterías reales de iones de litio, usando datos de los primeros 50 ciclos de carga de cerca de la mitad de ellas.\n\n**Impacto** – Discovery Learning predice la vida útil con un error de solo 7.2 %, lo que es muy preciso. Puede ahorrar hasta un 98 % del tiempo y un 95 % de la energía que se usa normalmente para las pruebas. Esto permite a los fabricantes crear mejores baterías más rápido y a menor costo.\n\n**Próximo paso** – Este método podría usarse en empresas de baterías para acelerar el diseño de nuevas baterías. También podría adaptarse para mejorar otras tecnologías complejas que necesitan mucho tiempo de prueba.\n\n**Frase clave** – Discovery Learning usa IA inteligente para predecir rápida y precisamente la vida útil de baterías, reduciendo casi por completo el tiempo y la energía de las pruebas.",
      "image_url": "public/images/news_image_Discovery-Learning-predicts-battery-cycle-life-fro.png",
      "image_prompt": "A warm, detailed painting of a glowing lithium-ion battery cell resting on an open book with interconnected glowing lines and symbols representing learning and discovery gently emanating from it, surrounded by softly illuminated abstract gears and circuit patterns that symbolize machine learning and scientific innovation, all rendered in natural, muted earth tones and soft blues."
    },
    {
      "title": "Bacterial immune activation via supramolecular assembly with phage triggers",
      "summary": "Nature, Published online: 04 February 2026; doi:10.1038/s41586-025-10060-8An antiphage defence system has an activation mechanism that relies on the sensing of phage-encoded proteins that enforce geometry crucial to activation and are not typically present in non-infected cells.",
      "content": "Strains and growth conditions\n\nAll bacterial and phage strains used in this study are listed in Supplementary Table 2. E. coli strains were routinely grown at 37 °C in Luria–Bertani (LB) medium for cloning and maintenance. Phages were propagated by infecting a culture of E. coli MG1655 at an optical density at 600 nm (OD 600 ) of approximately 0.1–0.2 with a multiplicity of infection of 0.1. Cleared cultures were pelleted by centrifugation to remove residual bacteria and filtered through a 0.2-μm filter. Chloroform was then added to phage lysates to prevent bacterial growth. Antibiotics were used at the following concentrations (liquid; plates): carbenicillin (50 μg ml−1; 100 μg ml−1) and chloramphenicol (20 μg ml−1; 30 μg ml−1).\n\nPlasmid construction\n\nAll plasmids are listed in Supplementary Table 3. All primers and synthesized gene sequences are listed in Supplementary Table 4.\n\nFor the pEssential-gp77 construct, the coding sequence of gene 77 from phage SECΦ27 was codon optimized for expression in E. coli, and commercially synthesized by Integrated DNA Technology as a gBlock (TZ-1) and assembled into pEssential vector amplified with TZ-2 and TZ-3 by Gibson assembly.\n\nFor the pBAD33-gp77 constructs, gp77 was PCR amplified from phage SECΦ27 using primers TZ-4 and TZ-5 and inserted into pBAD33 linearized with primers TZ-6 and TZ-7 by Gibson assembly. To add a C-terminal HA tag, primers TZ-10 and TZ-11 were used to PCR amplify pBAD33-gp77 followed by Gibson assembly.\n\nFor the pBAD33-portal protein constructs, gp8T7, gp31T3, gp21SECΦ18 or gp52 SECΦ27 was PCR amplified from the corresponding T7, T3, SECΦ18 or SECΦ27 phage using primers TZ-12 to TZ-17, or TZ-23 and TZ-24, respectively. Amplified fragments were inserted into pBAD33 linearized with TZ-6 and TZ-7 using Gibson assembly. To add a C-terminal HA tag, primers TZ-20 to TZ-22 were each used in combination with TZ-10 to PCR amplify the corresponding pBAD33-portal protein constructs followed by Gibson assembly.\n\nFor the pBAD33-razr construct, razr was PCR amplified from pLAND-razr using primers TZ-18 and TZ-19 and inserted into pBAD33 linearized with TZ-6 and TZ-7 by Gibson assembly.\n\nFor the pET-razr–His 6 and pET-gp77–His 6 constructs: razr–His 6 or gp77–His 6 was PCR amplified from pLAND-razr or pBAD33-gp77 using primers TZ-27 to TZ-30, respectively. Amplified fragments were inserted into pET vector linearized with TZ-25 and TZ-26 by Gibson assembly.\n\nFor pACYC constructs, to construct pACYC-gp77–HA, gp77–HA was PCR amplified from pBAD33-gp77–HA using primers TZ-33 and TZ-34 and inserted into pACYC linearized with primers TZ-31 and TZ-32. To construct pACYC-gp8T7–Flag, first gp8T7–HA was PCR amplified from pBAD33-gp8T7–HA using primers TZ-34 and TZ-35 and inserted into pACYC linearized with primers TZ-31 and TZ-32. To replace the C-terminal HA tag with a Flag tag, primers TZ-36 and TZ-37 were used to amplify pACYC-gp8T7–HA followed by Gibson assembly.\n\nFor the pLAND-razr constructs, pLAND-razr–Flag or pLAND-razr(ΔAGN) was constructed by PCR amplifying pLAND-razr with primers TZ-8 and TZ-9, or TZ-38 and TZ39, followed by Gibson assembly. To construct pLAND-razrKv, DNA encoding the razrKv open reading frame was codon optimized for expression in E. coli and 200 bp of the upstream region from the source organism was added for native expression. DNA was commercially synthesized by Integrated DNA Technology as a gBlock (TZ-40) and assembled into a promoter-less backbone of pLAND amplified with TZ-41 and TZ-42 by Gibson assembly. To construct pLAND-razrEc + ZFD(Kv), sequences for ZFD from RAZRKv were amplified with primers TZ-43 and TZ-44 and inserted into pLAND-razrEc amplified with TZ-45 and TZ-46 by Gibson assembly. To construct pLAND-razrKv+ZFD(Ec), sequences for ZFD from RAZREc were amplified with primers TZ-47 and TZ-48 and inserted into pLAND-razrKv amplified with TZ-49 and TZ-50 by Gibson assembly.\n\nAll mutants were constructed by site-directed mutagenesis using primers designed by Takara Bio In-Fusion design tool.\n\nStrain construction\n\nPlasmids described above were introduced into E. coli MG1655 by Transformation and Storage Solution (TSS) transformation or electroporation.\n\nSECΦ27-mutant phages with gene 77 deleted were generated using a CRISPR–Cas system for targeted mutagenesis as previously described39. In brief, sequences for RNA guides to target Cas9-mediated cleavage were designed using the toolbox in Geneious Prime (v2022.0.2) and selected for targeting of gene 77 but nowhere else in the SECΦ27 genome. The guides were inserted into the pCas9 plasmid and tested for their ability to restrict SECΦ27. An efficient guide was selected, and the pCas9-guide plasmid was co-transformed into E. coli MG1655 with a high copy-number repair plasmid with sequences flanking gene 77 for homologous recombination, and a pEssential plasmid encoding a recoded version of Gp77 under an arabinose-inducible control. Single colonies of E. coli MG1655 containing all three plasmids were grown overnight in LB medium. Overnight cultures were back- diluted 1:10 in 3 ml of LB + 0.2% arabinose and grown at 37 °C for 2 h to induce Gp77 expression. The WT SECΦ27 phage was mixed with 150 μl of induced culture and 4 ml LB + 0.5% agar + 0.2% arabinose and spread on an LB + 1.2% agar + antibiotic + 0.2% arabinose plate. Plates were incubated at 25 °C overnight. Single plaques were screened by Sanger sequencing, and two clones with gene 77 deleted were propagated on strains containing pCas9-guide and pEssential-gp77(recoded) with arabinose induction.\n\nPhage spotting assays and EOP measurements\n\nPhage spotting assays were conducted similarly to that previously described40. For phage spotting assays, 80 μl of a bacterial strain of interest was mixed with 4 ml LB + 0.5% agar and spread on an LB + 1.2% agar + antibiotic plate. Phage stocks were then serially diluted in 1× FM buffer (20 mM Tris-HCl pH 7.4, 100 mM NaCl and 10 mM MgSO 4 ), and 2 μl of each dilution was spotted on the bacterial lawn. Plates were then incubated at 25 °C overnight before imaging. EOP was calculated by comparing the ability of the phage to form plaques on an experimental strain relative to the control strain. Experiments were replicated three times independently and representative images are shown.\n\nFor spotting phage SECΦ27 lacking gene 77, E. coli MG1655 containing pEssential-gp77(recoded) or an empty vector were grown overnight in LB medium. Overnight cultures were back diluted 1:10 in 3 ml of LB + 0.2% arabinose and grown at 37 °C for 2 h to induce Gp77 expression. Of each induced culture, 300 μl was mixed with 4 ml LB + 0.5% agar + 0.2% arabinose and spread on an LB + 1.2% agar + antibiotic + 0.2% arabinose plate. Phages were serially diluted as above and spotted on the bacterial lawn. Plates were incubated at 37 °C overnight. For spotting SECΦ27 lacking gene 77 onto strains producing Gp77 variants (L20R or V22R), E. coli MG1655 containing pBAD33-EV or pBAD33-gp77 (WT or each variant) were grown and processed as described above.\n\nIsolation of phage escape mutants to infect RAZR\n\nSECΦ27, T3, T7 or SECΦ18 escape mutants of RAZR were isolated by plating a population of phage onto RAZR-containing cells. 20 µl of 109–1010 plaque-forming units (PFU) per millilitre phage was mixed with 80 µl overnight culture of E. coli MG1655 pLAND-razr and the mixture was added to 4 ml of LB + 0.5% agar and spread onto LB + 1.2% agar. Plates were incubated at 25 °C overnight. Single plaques were isolated and propagated using the same strain in LB at 25 °C. Amplified phage lysates were pelleted to remove bacteria, and then plated to single plaques and propagated similarly for a second round of isolation to improve purity. Escape phages were then sequenced by Illumina sequencing as described below to identify mutations.\n\nPhage DNA extraction and Illumina sequencing\n\nPhage DNA extraction and sequencing were conducted as previously described40. To extract phage DNA, high titre phage lysates (more than 106 PFU µl−1) were treated with DNase I (0.001 U µl−1) and RNase A (0.05 mg ml−1) at 37 °C for 30 min. 10 mM EDTA was used to inactivate the nucleases. Lysates were then incubated with proteinase K at 50 °C for 30 min to disrupt capsids and release phage DNA. Phage DNA was isolated by ethanol precipitation. In brief, NaOAc pH 5.2 was added to 300 mM followed by 100% ethanol to a final volume fraction of 70%. Samples were incubated at −80 °C overnight, pelleted at 21,000g for 20 min and supernatant removed. Pellets were washed with 100 µl isopropanol and 200 µl 70% (v/v) ethanol, and then air dried at room temperature and resuspended in 25 µl 1× TE buffer (10 mM Tris-HCl and 0.1 mM EDTA pH 8).\n\nTo prepare Illumina sequencing libraries, 100–200 ng of genomic DNA was sheared in a Diagenode Bioruptor 300 sonicator water bath for 20 × 30 s cycles at maximum intensity. Sheared genomic DNA was purified using AmpureXP beads, followed by end repair, 3′ adenylation and adaptor ligation. Barcodes were added to both 5′ and 3′ ends by PCR with primers that anneal to the Illumina adaptors. The libraries were cleaned by Ampure XP beads using a double cut to elute fragment sizes matching the read lengths of the sequencing run. Libraries were sequenced on an Illumina MiSeq at the MIT BioMicro Center. Illumina reads were assembled to the corresponding reference genomes using Geneious Prime (v2022.0.2).\n\nToxicity assays on solid media\n\nBacterial toxicity assays were conducted similarly to that previously described40. For co-producing RAZR with Gp77 or portal proteins (Gp8T7, Gp31T3 and Gp21SECΦ18), single colonies of E. coli MG1655 harbouring pLAND-razr and pBAD33-gp77 or pBAD33-portal protein (WT or the corresponding variants) were grown for 6 h at 37 °C in LB–glucose to saturation. Of each saturated culture, 200 μl was then pelleted by centrifugation at 4,000g for 10 min, washed once in 1× PBS, and resuspended in 400 μl 1× PBS. Cultures were then serially diluted tenfold in 1× PBS and spotted on M9L plates supplemented with 0.4% glucose or 0.2% arabinose. M9L plates contain M9 medium (6.4 g l−1 Na 2 HPO 4 –7H 2 O, 1.5 g l−1 KH 2 PO 4 , 0.25 g l−1 NaCl and 0.5 g l−1 NH 4 Cl medium supplemented with 0.1% casamino acids, 0.4% glycerol, 2 mM MgSO 4 and 0.1 mM CaCl 2 ) supplemented with 5% LB (v/v). Plates were then incubated at 37 °C overnight before imaging.\n\nTo quantify colony sizes, cultures were processed as described above and plated on M9L plates supplemented with 0.4% glucose or 0.2% arabinose to single colonies. Colony sizes of 30–100 colonies on each plate were quantified by Fiji, and the ratio of colony sizes on arabinose and glucose plates were calculated. Data reported are three independent biological replicates.\n\nFor producing full-length RAZR, E. coli MG1655 containing pBAD33-razr were grown to saturation and processed as above. Cultures were plated onto 0.4% glucose and 0.2% arabinose and incubated at 37 °C overnight.\n\nCo-immunoprecipitation analysis\n\nCo-immunoprecipitation experiments were conducted similar to those previously described40. For co-producing RAZR with Gp77 or portal proteins (Gp8T7, Gp31T3 and Gp21SECΦ18), E. coli MG1655 containing pLAND-razr–Flag (WT or mutant variants) and pBAD33-gp77–HA (WT or mutant variants) or pBAD33-portal protein–HA were grown overnight in M9–glucose. Overnight cultures were back diluted to OD 600 = 0.05 in 50 ml of M9 (no glucose) and grown to OD 600 of approximately 0.3 at 37 °C. Cells were induced with 0.2% arabinose for 30 min at 37 °C, then OD 600 was measured and cells were pelleted at 4,000g for 10 min at 4 °C. Supernatant was removed and cells were resuspended in 800 μl lysis buffer (25 mM Tris-HCl pH 8.0, 150 mM NaCl, 1 mM EDTA, 1% Triton X-100 and 5% glycerol) supplemented with protease inhibitor (Roche), 1 μl ml−1 Ready-Lyse Lysozyme Solution (Lucigen) and 1 μl ml−1 benzonase nuclease (Sigma). Samples were lysed by two freeze–thaw cycles, and lysates were normalized by OD 600 . Lysates were pelleted at 21,000g for 10 min at 4 °C, and 750 μl of supernatant were incubated with pre-washed anti-Flag magnetic agarose beads (Pierce) for 1 h at 4 °C with end-over-end rotation. Beads were then washed three times with 500 μl lysis buffer. Laemmli sample buffer (1×; Bio-Rad) supplemented with 2-mercaptoethanol was added to beads directly to elute proteins. Samples were boiled at 95 °C and analysed by 4–20% SDS–PAGE and transferred to a 0.2-μm PVDF membrane. Anti-Flag and anti-HA antibodies (#14793 and #3724, Cell Signaling Technology) were used at a final concentration of 1:1,000, and SuperSignal West Femto Maximum Sensitivity Substrate (Thermo Fisher) was used to develop the blots. Blots were imaged by a ChemiDoc Imaging system (Bio-Rad). Images shown are representatives of two or three independent biological replicates.\n\nProtein expression and purifications\n\nTo produce His 6 -tagged RAZR (WT or mutant variants) and His 6 -tagged Gp77 (WT or mutant variants), E. coli BL21(DE3) cells were transformed with pET-razr–His 6 or pET-gp77–His 6 and grown in LB medium to OD 600 of 0.5. Protein expression was induced by addition of 0.2 mM IPTG, and cells were grown overnight at 16 °C. The culture was centrifuged at 4,000g for 10 min at 4 °C, and cell pellet was resuspended in lysis buffer (50 mM Tris-HCl pH 8.0, 150 mM NaCl, 2 mM MgCl 2 , 10 μM ZnCl 2 and 1 mM dithiothreitol) supplemented with 0.4 mM PMSF and 10 μg ml−1 lysozyme. Cells were disrupted using sonication with amplitude 50 and 3 min total process time (10 s on and 20 s off; Qsonica) and glycerol was added to the lysate at final 10% concentration after sonication. The supernatant was separated from the pellet by centrifugation (15,000 rpm for 30 min, JA-25.50 rotor (Beckman Coulter)). The clarified supernatant was loaded onto a gravity-flow chromatography column (Bio-Rad) packed with 2 ml Ni-NTA agarose resin (Qiagen) pre-equilibrated with 15 ml lysis buffer. The resin was washed with 10 column volumes of wash buffer 1 (50 mM Tris-HCl pH 8.0, 500 mM NaCl, 2 mM MgCl 2 , 10 μM ZnCl 2 , 10 mM imidazole, 10% glycerol and 1 mM dithiothreitol), and then with 10 column volumes of wash buffer 2 (50 mM Tris-HCl pH 8.0, 150 mM NaCl, 2 mM MgCl 2 , 10 μM ZnCl 2 , 50 mM imidazole, 10% glycerol and 1 mM dithiothreitol). The proteins were eluted in 4 ml elution buffer (50 mM Tris-HCl pH 8.0, 150 mM NaCl, 10 μM ZnCl 2 , 250 mM imidazole, 10% glycerol and 1 mM dithiothreitol). To remove remaining contaminants, the eluted protein sample was loaded onto a size-exclusion chromatography Superose 6 Increase 10/300 GL column (Cytiva) pre-equilibrated in the size-exclusion chromatography buffer (50 mM Tris-HCl pH 8.0, 150 mM NaCl, 10 μM ZnCl 2 , 5% glycerol and 1 mM dithiothreitol). The purity of the protein samples were assessed spectrophotometrically and by SDS–PAGE. To reconstitute the WT complex in vitro, purified RAZR–His 6 and Gp77–His 6 were mixed at a 1:1 molar ratio using concentrations calculated by Bradford assay and loaded onto a Superose 6 Increase 10/300 GL column (Cytiva). The fractions containing the complex of interest were pooled, concentrated and used for structural determination.\n\nFor the RAZR(H154A)–Gp77 complex, E. coli BL21(DE3) cells were transformed with pET-razr(H154A)–His 6 and pACYC-gp77–HA. Protein expression and purification were conducted as described above.\n\nFor the RAZR(H154A)–Gp8T7 complex, E. coli BL21(DE3) cells were transformed with pET-razr(H154A)–His 6 and pACYC-gp8T7–Flag. Protein expression and purification with Ni-NTA agarose resin were conducted as described above. Eluted protein sample was mixed with 2 ml anti-Flag M2 affinity gel (Millipore Sigma) pre-equilibrated with the size-exclusion chromatography buffer, and incubated overnight at 4 °C. The mixture was loaded onto a gravity-flow chromatography column (Bio-Rad) and washed three times with 3 ml size-exclusion chromatography buffer. One bed volume (2 ml) of 1 mg ml−1 Flag peptide (APExBIO) was added to the resin, incubated for 30 min at 4 °C, and proteins were eluted by gravity. Elution was repeated three times, pooled and concentrated, and then loaded onto a Superose 6 Increase 10/300 GL column (Cytiva) for further purification.\n\nFor analytical size-exclusion chromatography, purified RAZR–His 6 , Gp77–His 6 or the co-purified RAZR(H154A)–His 6 –Gp77–HA complex was loaded onto a Superose 6 Increase 3.2/300 analytical size-exclusion chromatography column (Cytiva) pre-equilibrated in the size-exclusion chromatography buffer. Each fraction corresponding to the peak of RAZR(H154A)–His 6 –Gp77–HA was analysed by SDS–PAGE, transferred to a PVDF membrane and blotted with anti-HA (Cell Signaling Technology) and anti-His (Invitrogen) antibody as described below.\n\nCryo-EM sample preparation\n\nFor the co-purified RAZR(H154A)–His 6 –Gp77–HA complex, before vitrification, 2.5 µl of the complex (2.6 mg ml−1) was placed on 200-mesh Quantifoil 2/1 copper grids (0.5-s incubation time), which had been glow-discharged for 60 s in an easiGlow glow discharger (Pelco) at 25 mA and were blotted using a FEI Vitrobot Mark IV instrument for 4 s with a blot force of +4 (6 °C; 95% relative humidity).\n\nFor the in vitro-reconstituted complex of RAZR–His 6 and Gp77–His 6 , 2.5 µl of the sample (0.5 mg ml−1) was placed on 200-mesh Quantifoil 2/1 copper grids using the same parameters as mentioned above.\n\nFor the RAZR–His 6 , 2.5 µl of the sample (0.75 mg ml−1) was placed on 200-mesh Quantifoil 2/1 copper grids using the same parameters as mentioned above.\n\nFor the Gp77–His 6 , 2.5 µl of the sample (1 mg ml−1) was placed on 200-mesh Quantifoil 2/1 copper grids using the same parameters as mentioned above, except with a 5-s incubation time of the sample on the grid.\n\nFor the co-purified complex of Gp8T7–Flag and RAZR(H154A)–His 6 , 2.5 µl of the complex (0.6 mg ml−1) was applied to 200-mesh Quantifoil 2/1 copper grids with a 2-nm carbon support. The grids were glow discharged for 20 s in an easiGlow glow discharger (Pelco) at 25 mA. The sample was incubated on the grid for 10 s, then blotted using a FEI Vitrobot Mark IV instrument for 3.5 s with a blot force of +4 (6 °C; 95% relative humidity).\n\nThe protein–antibody complex was formed by incubating the co-purified RAZR(H154A)–His 6 –Gp77–HA complex (2.6 mg ml−1) with either 0.5 mg ml−1 His-tag antibody (MA1-2135, Invitrogen) or 0.25 mg ml−1 Strep-tag antibody (MA5-37747, Invitrogen) for 1 h at 4 °C before grid freezing, or with buffer only as a control. All concentrations are reported as final. Samples were prepared by applying 2.5 μl of the mixture onto 200-mesh Quantifoil 2/1 copper grids. The grids were glow-discharged using a GloQube Plus (MiTeGen) at 25 mA for 60 s. Sample-loaded grids were then blotted for 4 s with a blot force of +4 at 6 °C and 100% relative humidity using a FEI Vitrobot Mark IV instrument (Thermo Scientific).\n\nCryo-EM data collection\n\nFor the co-purified complex of Gp77 and RAZR(H154A), a total of 12,965 movies without stage tilt (0° tilt) and 8,911 movies with a 30° stage tilt were collected using EPU (v2.12.1; Thermo Fisher Scientific) on a Titan Krios G3i microscope (Thermo Fisher Scientific). The microscope was operated at an acceleration voltage of 300 kV with a magnification of ×130,000, and data were recorded in super-resolution mode on a K3 detector (Gatan) at a pixel size of 0.65 Å (pre-binned by 2). Each movie consisted of 40 frames and was collected within a defocus range of −0.25 to −1.75 µm for the 0° tilt data and −0.75 to −2.5 µm for the 30° tilt data. The total electron dose per specimen was 47.62 e− Å−2 for the 0° tilt data and 47.96 e− Å−2 for the 30° tilt data.\n\nFor in vitro-reconstituted complex of RAZR–His 6 and Gp77–His 6 , a total of 11,152 movies were collected using EPU (v2.12.1; Thermo Fisher Scientific) on a Titan Krios G3i microscope (Thermo Fisher Scientific). The microscope was operated at an acceleration voltage of 300 kV, with a magnification of ×130,000. Data were recorded in super-resolution mode on a K3 detector (Gatan) at a pixel size of 0.65 Å (pre-binned by 2). Each movie consisted of 40 frames and was collected within a defocus range of −0.5 to −2.0 µm. The total electron dose per specimen was 45.7 e− Å−2.\n\nFor Gp77–His 6 , 16,605 movies were collected using EPU (v2.12.1; Thermo Fisher Scientific) on a Titan Krios G3i microscope (Thermo Fisher Scientific). The microscope was operated at an acceleration voltage of 300 kV, with a magnification of ×130,000. Data were recorded in super-resolution mode on a K3 detector (Gatan) at a pixel size of 0.65 Å (pre-binned by 2). Each movie consisted of 40 frames and was collected within a defocus range of −0.75 to −2.25 µm. The total electron dose per specimen was 46.2 e− Å−2.\n\nFor RAZR–His 6 , 19 micrographs were collected using EPU (v2.12.1; Thermo Fisher Scientific) on a Talos Arctica G2 microscope (Thermo Fisher Scientific). The microscope was operated at an acceleration voltage of 200 kV, with a magnification of ×73,000. Data were recorded on a Falcon 3EC detector at a pixel size of 1.95 Å and a defocus of −5 µm during sample screening.\n\nFor the co-purified complex of Gp8T7–Flag and RAZR(H154A)–His 6 , a total of 4,139 movies were collected using EPU (v2.12.1; Thermo Fisher Scientific) on a Titan Krios G3i microscope (Thermo Fisher Scientific). The microscope was operated at an acceleration voltage of 300 kV, with a magnification of ×130,000. Data were recorded in super-resolution mode on a K3 detector (Gatan) at a pixel size of 0.65 Å (pre-binned by 2). Each movie consisted of 40 frames and was collected within a defocus range of −0.75 to −2.25 µm. The total electron dose per specimen was 48.53 e− Å−2.\n\nFor RAZR(H154A)–His 6 –Gp77 samples incubated with either anti-His antibody or buffer-only control, 2,000 movies were collected for each condition using EPU (v.3.11.0) on a Titan Krios G3 operated at 300 kV. For the RAZR(H154A)–His 6 –Gp77 samples incubated with anti-Strep tag II antibody, 1,000 movies were collected under otherwise identical conditions. Data were acquired at a nominal magnification of ×130,000 with multiple images per hole and recorded on a Falcon 4 detector, with an effective pixel size of 0.776 Å. Each movie consisted of 50 frames, with a defocus range of −0.4 to −2 μm and a total exposure of 52.48 e− Å−2.\n\nCryo-EM pre-processing and particle picking\n\nData processing was performed in cryoSPARC41 (v4.5.3 and v4.7.0) using default parameters unless otherwise noted. For data #1 (0° tilt data) of the co-purified complex of Gp77–RAZR(H154A) (Supplementary Fig. 2), 12,965 raw movies were pre-processed using ‘Patch Motion Correction’ and ‘Patch CTF Estimation’. Visual inspection revealed that the particles predominantly adopted ‘top’ views. In the cryoSPARC live session, 14 2D classes were selected and used for ‘Template Picker’ (particle diameter of 170 Å). Particles were extracted (box size of 520 × 520 pixels, Fourier cropped to 256). After two rounds of 2D classification, 1,015,530 particles were selected and extracted (box size of 720 × 720 pixels, Fourier cropped to 360), followed by three additional rounds of 2D classification. A final stack of 793,658 particles was selected.\n\nFor data #2 (30° tilt data) of the co-purified complex of Gp77 and RAZR(H154A) (Supplementary Fig. 2), 8,911 raw movies were pre-processed using Patch Motion Correction and Patch CTF Estimation. Two 2D classes from the 0° tilt data (representing particles with one and three rings) were selected for use in the ‘Template Picker’ (particle diameter of 250 Å). Particles were extracted (box size of 800 × 800 pixels, Fourier cropped to 360). After two rounds of 2D classification, 33,400 particles were selected as a preliminary stack.\n\nFor the in vitro-reconstituted Gp77–RAZR complex (Supplementary Fig. 3), 11,152 raw movies were pre-processed with Patch Motion Correction and Patch CTF Estimation. A total of 1,796 particles were manually picked using the Manual Picker utility. After 2D classification, two classes corresponding to top and side views (1,470 particles) were selected and used for training with the ‘Topaz Train’ 42 in cryoSPARC, followed by ‘Topaz Extract’. Particles (n = 195,741) were extracted (box size of 720 × 720 pixels, Fourier cropped to 360), followed by three rounds of 2D classification. A final stack of 42,191 particles was selected as a preliminary stack.\n\nAb initio reconstruction, global refinement and model building\n\nFor the co-purified complex of Gp77 and RAZR(H154A), ab initio reconstruction was performed using two classes and C 1 symmetry. On the basis of 2D class averages, the presence of higher-order symmetry (C 12 or C 24 ) was evident in both the inner and the outer rings. Homogeneous refinement was carried out using C 24 symmetry, followed by heterogeneous refinement (C 24 ) using two classes. Particles (n = 238,489) were selected and further refined using homogeneous refinement (C 24 ).\n\nWe suspected that there might be a discrepancy in symmetry within the inner and outer ring, therefore homogenous refinement was tested with C 12 symmetry. Duplicate particles were removed using the ‘Remove Duplicates’ utility, and the data were subjected to non-uniform refinement (C 6 symmetry).\n\nParticles (n = 33,400) from the 30° tilt dataset were used for ab initio reconstruction (C 1 ), followed by homogeneous refinement (C 1 ), and another round of homogeneous refinement using C 12 symmetry. These particles were then extracted (box size of 720 × 720 pixels, Fourier cropped to 360) and combined with the 0° tilt data for further homogeneous refinement (C 1 ). After heterogeneous refinement (C 1 ), 234,639 particles were selected for non-uniform refinement with C 12 symmetry.\n\nA mask containing both the inner and the outer rings was generated using ChimeraX43 (threshold of 0.2, soft pad 15) for local refinement. The particles were then subjected to global and local CTF refinement, followed by another round of local refinement and the application of the ‘remove duplicates’ utility. Subsequently, another round of local refinement and remove duplicates was performed, followed by both global and local CTF refinement. Finally, the particles underwent a final round of local refinement (C 12 symmetry) using the previous mask, resulting in a GSFSC (gold-standard Fourier shell correlation) of approximately 3.4 Å map with a sphericity score of 0.79 (out of 1; Extended Data Fig. 4a–c).\n\nFor the in vitro-reconstituted Gp77–RAZR complex, ab initio reconstruction was first performed using a single class with C 12 symmetry, followed by non-uniform refinement (C 12 ). Particles were then re-extracted (box size of 1,000 × 1,000 pixels, Fourier cropped to 440) and subjected to an additional round of 2D classification. A second ab initio reconstruction was carried out using two classes with the ‘ab initio reconstruction, high symmetry’ utility (C 12 symmetry). Particles corresponding to the volume representing the intact complex were selected for non-uniform refinement (C 12 ). After global and local CTF refinement, another round of non-uniform refinement (C 12 ) was performed. To further improve the model, symmetry expansion (C 12 ) was applied, followed by 3D classification with a mask containing one copy of Gp77 and two copies of RAZR, without pose refinement (10 classes). A class exhibiting well-defined density for the full complex was selected and used as the initial model for a subsequent non-uniform refinement of the original particle stack (before symmetry expansion), following local CTF refinement. This procedure yielded a final map at approximately 3.4 Å resolution (GSFSC) (Extended Data Fig. 4d–f and Supplementary Fig. 3a) with a sphericity score of 0.94 (out of 1). To improve the resolution of Gp77, signal subtraction was performed using two masks: one containing only the inner ring and another containing only the outer rings. To prepare particles for signal subtraction, local refinement was carried out with a mask applied to the outer ring to improve alignment of particles on the outer ring features. Signal subtraction was then performed using the outer ring mask. The resulting signal-subtracted particle images were used for homogeneous reconstruction with a mask applied to the central ring, followed by local refinement with C 24 symmetry. This yielded a Gp77-only map at a 3.1 Å GSFSC resolution (Extended Data Fig. 5a–c and Supplementary Fig. 3b) with a sphericity score of 0.94 (out of 1).\n\nTo further improve the resolution of the RAZR ZFD (central ring), focused refinement was performed on the final particle stack (28,190) using a mask that included only the central ring, yielding a focused-refined map at a GSFSC resolution of 3.5 Å (Extended Data Fig. 5d–f and Supplementary Fig. 3c) with a sphericity score of 0.75 (out of 1). Applying the same procedure to the outermost ring did not result in improved resolution.\n\nFor atomic model generation, only the in vitro-reconstituted complex maps were used, as these showed reduced anisotropy compared with the co-purified map. The Gp77-focused map enabled docking of residues 1–125 into the inner ring using ‘phenix.local_em_fitting’ in ChimeraX, followed by manual adjustments in Coot and subsequent refinement in Phenix. The final modelled residue indicated that the remaining C-terminal domain—predicted to form three α-helices followed by a long unstructured region—is flexibly positioned at the top of the assembly, corresponding to a low-resolution density forming an additional ring-like feature (Extended Data Fig. 6b). The accuracy of the Gp77 model (1–125) was supported by well-resolved side-chain densities for bulky residues (Extended Data Fig. 6a; map–model correlation coefficient (mask) of 0.71; Q score44 (global/expected) of 0.48/0.56).\n\nThe central ring was modelled using an AlphaFold prediction of the RAZR ZFD, fitted with phenix.local_em_fitting45 and refined through manual rebuilding in Coot and Phenix. A continuous density bridging the ZFD and HEPN domains clearly delineated the interdomain loop, suggesting that the HEPN domain occupies the outermost ring, which was resolved at substantially lower resolution relative to the Gp77 core and the RAZR ZFD domain (Extended Data Fig. 4). Docking of the AlphaFold-predicted HEPN domain into this region enabled modelling of RAZR residues 3–179. The remaining portion of the HEPN domain, which could not be confidently placed, is predicted to adopt a helix–loop–helix motif that may flexibly position between subunits. After placement of the AlphaFold-predicted HEPN domain, the entire model was further refined in Phenix to reduce steric clashes and improve overall geometry (map–model CC (mask) of 0.75; overall quality control score (global/expected) of 0.41/0.48; RAZR(ZFD) Q score of 0.42; and RAZR(HEPN) Q score of 0.13).\n\nThe 270 Å diameter of the ring reported for the in vitro-reconstituted RAZR–Gp77 complex was a distance measured between the Cα atoms of Q140 in chains OA and OM, and the diameter of approximately 170 Å in Gp8T7 was measured between Cα Q201 of chains A and G (PDB ID: 7EY8) that is almost identical to that of the Gp77 ring in our cryo-EM structure (measured between Cα S31 of chains IA and IM).\n\nModel building was performed using ChimeraX43 (v1.6), Coot46 (v0.9.4) and Phenix47 (v1.21.2-5419). Final maps were sharpened using cryoSPARC. Local resolution was estimated using MonoRes48 within cryoSPARC; angular Fourier shell correlations were calculated using the 3DFSC server; and Q scores were calculated using a ChimeraX Q score plugin44.\n\nMass photometry\n\nPurified RAZR(H154A)–His 6 –Gp77–HA or Gp77–His 6 samples were diluted to 50 nM (as a complex) in a buffer containing 50 mM Tris-HCl pH 8.0, 150 mM NaCl, 10 μM ZnCl 2 and 1 mM dithiothreitol. Of each protein, 2 μl was added to 18 μl of buffer in the well, and measured using a Refeyn TwoMP mass photometer (Refeyn) with a data acquisition time of 60 s. Data were acquired by AcquireMP and analysed by DiscoverMP software. The recorded events were fitted to Gaussian distributions, and masses were calculated by applying calibrations performed with BSA (66 kDa) and thyroglobulin (660 kDa) in the same buffer. Each sample was measured independently two times as replicates.\n\nWestern blot of RAZR expression levels\n\nSingle colonies of E. coli MG1655 pLAND-razr–Flag (WT or mutant variants) were grown overnight in LB. Overnight cultures were back diluted to OD 600 = 0.05 in 5 ml fresh LB and grown to OD 600 = 0.3 at 37 °C. OD 600 was measured, and 3 ml of cells was pelleted at 6,000g for 10 min with OD 600 normalized. Supernatant was removed and pellets were resuspended in 1× Laemmli sample buffer (Bio-Rad) supplemented with 2-mercaptoethanol. Samples were then boiled at 95 °C for 15 min and analysed by 4–20% SDS–PAGE and transferred to a 0.2-μm PVDF membrane. Anti-Flag antibody (#14793, Cell Signaling Technology) was used at a final concentration of 1:1,000, and SuperSignal West Femto Maximum Sensitivity Substrate (Thermo Fisher) was used to develop the blots. Blots were imaged by a ChemiDoc Imaging system (Bio-Rad). Blots were stained with Coomassie stain and imaged as loading control. The image shown is a representative of two independent biological replicates.\n\nIncorporation assays\n\nIncorporation assays were performed similarly to those previously described40. For co-producing RAZR and Gp77, single colonies of E. coli MG1655 containing pLAND-razr and pBAD33-gp77 or corresponding empty vectors were grown overnight in M9–glucose. Overnight cultures were back diluted to OD 600 = 0.05 in 25 ml M9–glucose and grown to OD 600 of approximately 0.3 at 37 °C. Cells were pelleted at 4,000g for 5 min at 4 °C and washed once with M9 (no glucose), and then back diluted to OD 600 = 0.1 in 15 ml M9 (no glucose) and recovered for 45 min at 37 °C. At the beginning of the experiment, cells were induced with 0.2% arabinose. At the indicated time points (0, 10, 20, 30 and 40 min), OD 600 was measured and an aliquot of 250 μl of cells was transferred to microcentrifuge tube containing [methyl-3H]-thymidine (Revvity; 40 μCi ml−1) for replication measurements, [5,6-3H]-uridine (Revvity; 4 μCi ml−1) for transcription measurements or EasyTag EXPRESS-35S Protein Labeling Mix, [35S] (Revvity; 22 μCi ml−1) for translation measurements. Tubes were incubated at 37 °C for 2 min, then quenched by addition of non-radioactive thymidine (1.5 mM), uridine (1.5 mM) or cysteine and methionine (15 mM each) and incubated for an additional 2 min. Samples were then added to ice-cold trichloroacetic acid (TCA) (10% w/v) and incubated at least 30 min on ice to allow for precipitation. Resulting samples were vacuum filtered onto a glass microfibre filter (1820-024, Whatman) that had been pre-wetted with 5% w/v TCA. Filters were washed with 35× volume of 5% w/v TCA, then with 5× volume of 100% ethanol. Air-dried filters were placed in tubes with scintillation fluid and measured in a scintillation counter (PerkinElmer). Counts per million was normalized to OD 600 and percent incorporation at each time point was calculated by normalizing to T = 0. Data reported are the individual data points from two (transcription or translation) or four (replication) independent biological replicates.\n\nCell-free translation\n\nExperiments with PURExpress in vitro protein synthesis kit (E6800, NEB) were performed as per the manufacturer’s instructions. All reactions were supplemented with 0.8 U µl−1 RNase Inhibitor Murine (M0314S, NEB). Purified His 6 -tagged RAZR protein and purified His 6 -tagged Gp77 protein were added to the 15 µl reaction at a final concentration of 1 µM each (as monomers). A template plasmid encoding the control protein DHFR was used at 5 ng µl−1. The reactions were incubated at 37 °C for 2 h, and 2 µl of each reaction was mixed with 10 µl of 1× Laemmli sample buffer (Bio-Rad) supplemented with 2-mercaptoethanol. The mixtures were boiled for 5 min at 95 °C and analysed by 12% SDS–PAGE. The gels were stained with Coomassie stain and imaged by a ChemiDoc Imaging system (Bio-Rad). Images shown are representatives of three independent biological replicates.\n\nRNA extraction from cells expressing RAZR and Gp77\n\nSingle colonies of E. coli MG1655 containing pLAND-razr and pBAD33-gp77 or the corresponding empty vector were grown overnight in M9–glucose. Overnight cultures were back diluted to OD 600 = 0.05 in 20 ml M9–glucose and grown to OD 600 of approximately 0.3 at 37 °C. Cells were pelleted at 4,000g for 5 min at 4 °C and washed once with M9 (no glucose), and then back diluted to OD 600 = 0.1 in 15 ml M9 (no glucose) and recovered for 45 min at 37 °C. At the beginning of the experiment, cells were induced with 0.2% arabinose. At the indicated time points (0, 30 and 60 min), cells were harvested by adding 900 µl of culture to 100 µl of stop solution (95% ethanol and 5% acid-buffered phenol) on ice and spinning at 13,000g for 30 s. Supernatants were removed, and pellets were flash frozen in liquid nitrogen. To extract RNAs, 400 µl of TRIzol (Invitrogen) preheated to 65 °C was added to each pellet. Resuspended pellets were incubated at 65 °C for 10 min at 2,000 rpm in a thermomixer, flash frozen in liquid nitrogen for 10 min and thawed to room temperature. Samples were spun at 21,000g for 5 min at 4 °C, and supernatants were mixed with 400 µl of ethanol. RNAs were purified using the Direct-zol RNA Miniprep kit (Zymo Research) per the manufacturer’s instructions with on-column DNase treatment. RNA yield was measured by a Nanodrop spectrophotometer. Of each purified RNA, 80 ng was analysed by a Novex 15% TBE–urea gel (Invitrogen) in 1× TBE buffer and stained with SYBR Gold (Invitrogen). For visualizing rRNAs, 1 µg of purified RNAs were analysed by 1% agarose gel in 1× TAE buffer supplemented with 1% bleach and ethidium bromide. The gels were imaged by a ChemiDoc Imaging system (Bio-Rad). Images shown are representatives of two independent biological replicates.\n\nRNA extraction following phage infection\n\nSingle colonies of E. coli MG1655 containing pLAND-razr or an empty vector were grown overnight in LB medium. Overnight cultures were back diluted to OD 600 = 0.05 in 25 ml LB and grown to OD 600 of approximately 0.2 at 25 °C. At the beginning of the experiment, cells were infected with phage T7 at multiplicity of infection = 10. At the indicated time points (0, 10, 20, 30 and 40 min), 500 µl of cells were mixed with 500 µl of preheated lysis buffer containing 2% SDS and 4 mM ETDA pH 8.0. Samples were incubated at 100 °C for 5 min, flash frozen in liquid nitrogen and thawed to room temperature. Of acid-buffered phenol (pH 4.5, Sigma) preheated to 67 °C, 1 ml was added to each sample and incubated at 67 °C for 2 min. Samples were pelleted at 21,000g for 10 min, and the aqueous layer was collected and repeated with another hot phenol extraction. A third round of extraction was done with 1 ml acid-buffered phenol chloroform (pH 4.5, Ambion). RNA was precipitated with 1× volume isopropanol, 0.1× volume 3 M NaOAc pH 5.5 and 0.01× volume GlycoBlue on ice for 1 h. RNAs were pelleted at 21,000g for 30 min at 4 °C. Pellets were washed twice with 500 µl of ice-cold 70% ethanol, air dried and resuspended in 90 µl nuclease-free water. To remove DNA, samples were treated with 10 µl of 10× Turbo DNase buffer (Invitrogen) and 2 µl Turbo DNase I (Invitrogen) for 20 min at 37 °C. An additional 2 µl of Turbo DNase I was added, and samples were incubated for another 20 min at 37 °C. Samples were then mixed with 96 µl of water, extracted with 200 µl of acid-buffered phenol chloroform (pH 4.5, Ambion) and precipitated for 1 h at −20 °C with 3× volume ice-cold ethanol, 0.1× volume 3 M NaOAc pH 5.5 and 0.01× volume GlycoBlue. RNAs were pelleted, washed and resuspended as described above. RNAs were analysed by a Novex 15% TBE–urea gel and a 1% agarose gel with 1% bleach as described above.\n\nIn vitro cleavage assays\n\nFor tRNA cleavage assays, purified RAZR–His 6 and Gp77–His 6 were added to a 5 µl reaction at a final concentration of 1.2 µM each (as monomers) and mixed with 180 ng of extracted bulk E. coli tRNAs in cleavage buffer (50 mM Tris-HCl pH 8.0, 150 mM NaCl, 30 mM KCl, 7 mM MgCl 2 , 10 μM ZnCl 2 and 1 mM dithiothreitol). The co-purified RAZR(H154A)–His 6 –Gp77–HA complex or the in vitro-reconstituted complex RAZR–His 6 –Gp77–His 6 were each added at a final concentration of 2.5 µM (as monomers). After incubation at 37 °C for 1 h, 2.5 µl of each reaction was mixed with 2.5 µl of Novex TBE–urea sample buffer (Invitrogen) and analysed by a Novex 15% TBE–urea gel (Invitrogen) in 1× TBE buffer and stained with SYBR Gold stain.\n\nFor rRNA cleavage assays, purified RAZR–His 6 and Gp77–His 6 were added to a 15 µl reaction at a final concentration of 1.2 µM each and mixed with E. coli 70S ribosomes (NEB) at a final concentration of 0.44 µM in cleavage buffer. The co-purified RAZR(H154A)–His 6 –Gp77–HA complex or the in vitro-reconstituted complex RAZR–His 6 –Gp77–His 6 were each added at a final concentration of 2.5 µM (as monomers). After incubation at 37 °C for 1 h, RNAs were extracted by acid-buffered phenol chloroform and precipitated as described above. Of purified RNAs, 1 µg were analysed by 1% agarose gel in 1× TAE buffer supplemented with 1% bleach and ethidium bromide.\n\nFor cleavage assays of RNA or DNA oligos (single stranded or double stranded), purified RAZR–His 6 and Gp77–His 6 were added to a 15 µl reaction at a final concentration of 1.2 µM each (as monomers) and mixed with 500 ng each corresponding oligo in cleavage buffer. After incubation at 37 °C for 1 h, 3 µl of each reaction was analysed by a Novex 15% TBE–urea gel (Invitrogen) as described above. Oligo sequences are listed in Supplementary Table 4 (TZ-51 to TZ-54).\n\nFor mRNA cleavage assays, mRNA substrates were in vitro transcribed using MEGAscript T7 transcription kit (Invitrogen) per the manufacturer’s instructions. In brief, each coding sequence was PCR amplified from genomic DNA of E. coli MG1655 with a T7 promoter introduced directly upstream of it using primers TZ-55 to TZ-70. Amplified DNA was purified with DNA clean & concentrator kit (Zymo Research) and used as template. In vitro transcription reactions were incubated at 37 °C for 4 h, and treated with 1 µl Turbo DNase at 37 °C for 15 min. Transcribed mRNAs were purified by phenol-chloroform extraction and ethanol precipitation as described above. For cleavage assays, purified RAZR–His 6 and Gp77–His 6 were added to a 15 µl reaction at a final concentration of 1.2 µM each (as monomers) and mixed with 2 µg of each mRNA substrate in cleavage buffer. After incubation at 37 °C for 1 h, 0.75 µl of each reaction was analysed by a Novex 15% TBE–urea gel (Invitrogen) as described above.\n\nRNA-seq sample preparation and analysis\n\nFor co-producing Gp77 and RAZR, RNAs were extracted as described above from E. coli MG1655 cells containing pLAND-razr and pBAD33-gp77 or the corresponding empty vector, after inducing with arabinose for 0, 10 and 30 min. RNAs were purified using the Direct-zol RNA Miniprep kit (Zymo Research) per the manufacturer’s instructions and eluted in 90 µl nuclease-free water. To remove DNA, samples were treated with 10 µl of 10× Turbo DNase buffer (Invitrogen) and 2 µl Turbo DNase I (Invitrogen) for 20 min at 37 °C. An additional 2 µl of Turbo DNase I was added, and samples were incubated for another 20 min at 37 °C. Samples were then mixed with 96 µl of water, extracted with 200 µl of acid-buffered phenol chloroform (pH 4.5, Ambion), and precipitated for 1 h at −20 °C with 3× volume ice-cold ethanol, 0.1× volume 3 M NaOAc pH 5.5 and 0.01× volume GlycoBlue. RNAs were pelleted, washed and resuspended as described above. RNA quality was assessed by a Novex 6% TBE–urea gel, and yield was measured by a Nanodrop spectrophotometer. For RNA-seq during T7 infection, RNAs were extracted and processed as described above.\n\nTo prepare libraries for RNA-seq, rRNA was removed using a developed E. coli rRNA depletion kit49, with 1.7 µg of total RNAs as input. rRNA-depleted samples were further purified using RNA Clean and Concentrator kit (Zymo Research). Libraries were generated using NEBNext Ultra II RNA Library Prep Kit for Illumina (NEB) following the manufacturer’s instructions for purified mRNA or rRNA-depleted RNA. Libraries were sequenced on an Illumina NextSeq at the MIT BioMicro Center. Two biological replicates were harvested and sequenced independently.\n\nRNA-seq data were analysed similarly to that previously described30. FASTQ files for each sample were trimmed using cutadapt (v1.15)50 and then mapped to the MG1655 genome (NC_00913.2) and the T7 genome (V01146), or the consensus map of rRNA loci as previously described30 using bowtie2 (v2.3.4.1)51 with the following arguments: –D 20, –I 40, –X 300, –R 3, –N 0, –L 20, –i S,1,0.50. Sam files generated from bowtie2 mapping were converted to bam files using samtools (v1.7)52, and then converted to numpy arrays using the genomearray3 Python library30. Gene names and coding regions were extracted from NCBI annotations. For the T7 transcriptome, mRNA positions were extracted based on the T7 promoter and terminator positions from NCBI annotations, similar to that previously described53. For analysis of fragment density across the transcriptome, one count was added for all positions between and including the 5′ and 3′ ends of the reads. To correct for variability in sequencing depth, counts at each position were divided by a sample size factor as previously described for normalization30. In brief, counts in each coding region were summed for each sample, and the geometric mean of these sums was calculated to yield a reference sample. The total counts in each coding region were then normalized by the reference sample, and the median of these ratios was taken as the size factor for that sample. The cleavage ratio at each nucleotide position was calculated as the log 2 transformed + Gp77:empty vector ratio (for co-producing Gp77 and RAZR) or the log 2 transformed + RAZR:empty vector ratio (for phage infection), and the average of two replicates were taken. Any regions in the empty vector sample that had fewer counts than the expression cut-off of 64 counts (for co-producing Gp77 and RAZR) or 32 counts (for phage infection) were discarded, and minimum cleavage ratio was taken for each coding region.\n\nHomology search and conservation analysis\n\nHomologues of RAZR were identified by ConSurf54 with default settings to search the UniRef90 database, using AlphaFold-predicted RAZR structure as input. Homologues (n = 150) were used to generate the multiple sequence alignment by MAFFT. Conservation scores were calculated using the Bayesian method and default settings.\n\nReporting summary\n\nFurther information on research design is available in the Nature Portfolio Reporting Summary linked to this article.",
      "url": "https://www.nature.com/articles/s41586-025-10060-8",
      "source": "Nature",
      "published": "2026-02-05",
      "sentiment_score": 0.85,
      "reasoning": "The article reports a significant scientific breakthrough in understanding bacterial immune defense mechanisms against phages via supramolecular assembly. This discovery has broad implications for microbiology, biotechnology, and potentially medicine, as it enhances knowledge of bacterial immunity and could inspire new antimicrobial strategies. The detailed experimental evidence and structural insights provide substantial context and demonstrate a meaningful advancement in technology and biological science.",
      "category": "Technology",
      "personality_title": "Scientists uncover how bacteria use special protein rings to defend against viruses",
      "personality_presentation": "**Context** – Bacteria are tiny living things that can get sick from viruses called phages. To protect themselves, bacteria have immune systems. Scientists have been trying to understand exactly how these defenses work at a molecular level.\n\n**What happened** – Researchers discovered a new way bacteria activate their immune defense. They found that bacteria detect certain proteins made by invading phages. When these phage proteins appear, they help form special ring-shaped structures made of bacterial and phage proteins. This assembly triggers the bacterial immune system to attack the virus. The scientists studied this process using a bacteria called E. coli and phages that infect it. They used advanced methods like cryo-electron microscopy to see the shape of these protein rings and genetic techniques to understand how they work.\n\n**Impact** – This finding is important because it reveals a clear mechanism of bacterial defense that depends on recognizing virus proteins and building a complex molecular structure. Understanding this can help scientists design new ways to fight harmful bacteria or viruses. It also adds to our knowledge of how simple organisms defend themselves, which could inspire future medical or biotechnological tools.\n\n**What's next step** – Researchers will likely explore how widespread this defense system is among different bacteria and phages. They may try to harness or modify these protein rings to develop new antibiotics or treatments that protect beneficial bacteria or kill harmful ones. Further studies might also reveal more details about the molecular steps involved in activating this immune response.\n\n**One-sentence takeaway** – Scientists have revealed that bacteria defend against viruses by forming special protein rings triggered by virus proteins, opening new paths for biological and medical advances.",
      "personality_title_fr": "Les scientifiques découvrent comment les bactéries utilisent des anneaux protéiques spéciaux pour se défendre contre les virus",
      "personality_presentation_fr": "**Contexte** – Les bactéries sont de minuscules organismes qui peuvent être infectés par des virus appelés phages. Pour se protéger, elles possèdent des systèmes immunitaires. Les scientifiques cherchent à comprendre précisément comment ces défenses fonctionnent au niveau moléculaire.\n\n**Ce qui s’est passé** – Des chercheurs ont découvert une nouvelle façon dont les bactéries activent leur défense immunitaire. Ils ont trouvé que les bactéries détectent certaines protéines produites par les phages envahisseurs. Lorsque ces protéines de phage apparaissent, elles aident à former des structures en anneaux spéciales composées de protéines bactériennes et de phage. Cette assemblée déclenche le système immunitaire bactérien pour attaquer le virus. Les scientifiques ont étudié ce processus en utilisant la bactérie E. coli et les phages qui l’infectent. Ils ont utilisé des méthodes avancées comme la cryo-microscopie électronique pour observer la forme de ces anneaux protéiques et des techniques génétiques pour comprendre leur fonctionnement.\n\n**Impact** – Cette découverte est importante car elle révèle un mécanisme clair de défense bactérienne qui dépend de la reconnaissance des protéines virales et de la formation d’une structure moléculaire complexe. Cela peut aider les scientifiques à concevoir de nouvelles façons de combattre les bactéries ou virus nuisibles. Cela enrichit aussi notre compréhension de la façon dont des organismes simples se défendent, ce qui pourrait inspirer des outils médicaux ou biotechnologiques.\n\n**Prochaine étape** – Les chercheurs vont probablement étudier la présence de ce système de défense chez différentes bactéries et phages. Ils pourraient essayer d’utiliser ou modifier ces anneaux protéiques pour développer de nouveaux antibiotiques ou traitements qui protègent les bonnes bactéries ou tuent les mauvaises. D’autres études pourraient aussi révéler plus de détails sur les étapes moléculaires de l’activation de cette réponse immunitaire.\n\n**Phrase clé** – Les scientifiques ont montré que les bactéries se défendent contre les virus en formant des anneaux protéiques spéciaux déclenchés par des protéines virales, ouvrant de nouvelles voies pour la biologie et la médecine.",
      "personality_title_es": "Científicos descubren cómo las bacterias usan anillos proteicos especiales para defenderse de los virus",
      "personality_presentation_es": "**Contexto** – Las bacterias son organismos muy pequeños que pueden enfermarse por virus llamados fagos. Para protegerse, las bacterias tienen sistemas inmunes. Los científicos han estado tratando de entender exactamente cómo funcionan estas defensas a nivel molecular.\n\n**Qué sucedió** – Investigadores descubrieron una nueva forma en que las bacterias activan su defensa inmune. Encontraron que las bacterias detectan ciertas proteínas hechas por los fagos invasores. Cuando estas proteínas de fago aparecen, ayudan a formar estructuras en forma de anillos especiales hechas de proteínas bacterianas y de fago. Esta unión activa el sistema inmune bacteriano para atacar al virus. Los científicos estudiaron este proceso usando la bacteria E. coli y los fagos que la infectan. Usaron métodos avanzados como la criomicroscopía electrónica para ver la forma de estos anillos proteicos y técnicas genéticas para entender cómo funcionan.\n\n**Impacto** – Este hallazgo es importante porque revela un mecanismo claro de defensa bacteriana que depende de reconocer proteínas virales y construir una estructura molecular compleja. Entender esto puede ayudar a los científicos a diseñar nuevas formas de combatir bacterias o virus dañinos. También aumenta nuestro conocimiento sobre cómo organismos simples se defienden, lo que podría inspirar futuras herramientas médicas o biotecnológicas.\n\n**Próximo paso** – Los investigadores probablemente explorarán qué tan común es este sistema de defensa entre diferentes bacterias y fagos. Podrían intentar usar o modificar estos anillos proteicos para desarrollar nuevos antibióticos o tratamientos que protejan bacterias beneficiosas o eliminen las dañinas. Estudios adicionales podrían revelar más detalles sobre los pasos moleculares que activan esta respuesta inmune.\n\n**Frase clave** – Los científicos han revelado que las bacterias se defienden contra virus formando anillos proteicos especiales activados por proteínas virales, abriendo nuevos caminos para avances biológicos y médicos.",
      "image_url": "public/images/news_image_Bacterial-immune-activation-via-supramolecular-ass.png",
      "image_prompt": "A detailed, warm-toned painting of intricately interlocking molecular rings symbolizing bacterial immune proteins assembling with phage components, depicted as glowing, puzzle-like structures fitting together harmoniously against a soft, natural background of gentle earth and muted blues, evoking a sense of cooperative defense and precise biological interaction without any human figures or text."
    }
  ]
}