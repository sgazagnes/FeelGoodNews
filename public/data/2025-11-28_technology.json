{
  "personality": null,
  "timestamp": "2025-11-28T04:40:08.384593",
  "category": "Technology",
  "news_summary": "Today's technology news highlights advancements in addressing biases in large language models, enhancing scientific innovation through structured systems, and breakthroughs in neural task composition and universal physical laws.",
  "news_summary_fr": "L'actualité technologique d'aujourd'hui met en lumière les progrès réalisés dans le traitement des biais dans les grands modèles linguistiques, l'amélioration de l'innovation scientifique grâce à des systèmes structurés, ainsi que les avancées dans la composition des tâches neuronales et les lois physiques universelles.",
  "news_summary_es": "Las noticias tecnológicas de hoy destacan los avances en la resolución de los sesgos de los grandes modelos lingüísticos, la mejora de la innovación científica mediante sistemas estructurados y los grandes avances en la composición de tareas neuronales y las leyes físicas universales.",
  "articles": [
    {
      "title": "Large language models are biased — local initiatives are fighting for change",
      "summary": "Nature, Published online: 27 November 2025; doi:10.1038/d41586-025-03891-yDespite advances, AI models continue to be geared towards the needs of English-speaking people in high-income countries.",
      "content": "Omar Florez (right) is part of a team training the large language model Latam-GPT.Credit: CENIA\n\nIn early 2023, Álvaro Soto was looking for Las doce figuras del mundo, a short story co-written by one of his favourite authors, Jorge Luis Borges. To track down the book in which the story was originally published in 1942, Soto enlisted the help of artificial-intelligence chatbot ChatGPT. But the response did not sit right with Soto. “This is wrong,” he thought.\n\nNature Outlook: Robotics and artificial intelligence\n\nThe book ChatGPT proposed was one that Soto knew, and he was sure that the story — which translates as The Twelve Figures of the World — was not in it. For him, this is just one example of how AI technology often fails to grasp nuance, especially that which depends on cultural context. “These models were not trained with quality data from our region,” says Soto, a computer scientist and director of Chile’s National Center for Artificial Intelligence (CENIA) in Santiago. “When they don’t find specific information, they make it up.”\n\nLike many computer and data scientists, Soto thinks that AI will drive a technological revolution, just as the Internet did three decades ago. As AI-based products flood the global market, it’s important to him that no one in Latin America — including those who speak minority languages, such as Rapa Nui spoken by those on Chile’s Easter Island — are left behind.\n\nRoughly 7,000 languages exist worldwide but fewer than 5% are meaningfully represented online. Languages spoken by communities in Asia, Africa and the Americas account for about 5,500 of the total. Many of these are spoken by small populations and are at risk of disappearing by the end of the century. English, Spanish and French dominate in these regions as a result of their colonial history. AI systems are built mostly on data using these widely spoken languages, especially English, for which there are plenty of tools and data needed for natural-language processing.\n\nThis imbalance can hinder AI’s global reach. “Many of us live in multicultural societies, and many of our grandparents don’t speak English or type on a computer the way we do,” says Leslie Teo, a data scientist and a director of AI products at AI Singapore, a programme launched to boost the country’s AI capabilities. “If people don’t feel the AI understands them, or if they can’t access it, they won’t benefit from it,” he says.\n\nDevelopers often say that their chatbot services can converse in a wide variety of languages. But the responses often have telltale signs that their core training was in English. “To deploy AI-driven technologies in our communities, they need to speak a language and context people are comfortable with,” says Mpho Primus, a computational linguist and co-director of the Institute for Artificial Intelligent Systems at the University of Johannesburg in South Africa. For her, that means going further than simply line-by-line translation; AI should also reflect culturally specific knowledge and norms. “How I speak to my mother-in-law is a very different way and use of words than how I speak to my mother,” she says.\n\nAcross Africa, Latin America and southeast Asia, researchers are collating data sets tailored to local cultures and languages that can be used to train AI systems. And some are already using these to build models locally. But because the architectures underlying these models are often developed in the United States, it remains to be seen what degree of cultural bias might still be present. “The global push to develop AI is no longer just about computing power or algorithmic breakthroughs,” wrote Primus in Nature Africa in June1. “It’s about who gets to speak and who gets left out in the digital future.”\n\nFluent, but not local\n\nTransformer architecture — the artificial neural network that underpins so much of the current AI boom, and that is represented by the ‘T’ in OpenAi’s ChatGPT — started out in 2017 as a way to improve machine translation. It feeds on massive data sets of online content and pulls out patterns in how people speak and write, forming the basis for large language models (LLMs) such as GPT, Gemini and Claude.\n\nThe Latam-GPT team spent two years collecting data from public Latin American sources.Credit: CENIA\n\nIn October 2018, Google released its first LLM based on transformer architecture: BERT, which was trained in US English using text from Wikipedia (about 2.5 billion words) and the Toronto BookCorpus (800 million words from books published online by independent authors). BERT has since been surpassed in performance and flexibility by other transformer models, such as Llama and Gemma. It was ChatGPT, however, released in November 2022, that made the world really take notice of LLMs. In July 2025, the chatbot received queries from around 700 million users weekly in around 80 languages.\n\nOne of the main sources of training data for the generative AI systems that power modern chatbots is Common Crawl: a non-profit organization that has compiled an open archive of around 300 billion web pages since 2008. But it is strongly skewed towards English content. This is partly because of safety filtering. “Common Crawl applies filters to exclude unsafe content,” explains Vukosi Marivate, a data scientist at the University of Pretoria in South Africa. “But these filters don’t always extend well to languages other than English, so a lot of non-English content gets excluded.” The result is a data imbalance that limits the representation of non-English languages in LLMs.\n\nEven speakers of English dialects, such as African American or Indian, Irish, Jamaican, Kenyan or Singaporean English, report feeling unrepresented. In 2024, researchers at the Berkeley Artificial Intelligence Research (BAIR) Lab at the University of California, looked at how two versions of GPT responded to texts written in eight widely spoken English dialects, alongside Standard American and British English. “We wanted to understand if discrimination was slipping under the radar,” says computer scientist Eve Fleisig at BAIR. She and her colleagues found that the versions routinely defaulted to American spelling, even when British spelling is the default in most non-US countries. When speakers of non-standard versions of English evaluated the chatbot’s responses, they flagged stereotyping and condescending or demeaning responses2. “Language carries power,” says Genevieve Smith, a social scientist at BAIR. “Reinforcing a particular way of speaking reflects what linguists call ‘standard language ideology’ — the belief that one variety is correct. It’s ultimately a power balance issue.”\n\nThe bias of LLMs can be seen in other ways, too. “It’s not just about language, but how the AI interacts with us,” Teo says. A 2024 study by researchers at Cornell University in Ithaca, New York, explored whether generative AI models reflect specific cultural value systems3. The team asked five GPT models to answer ten questions drawn from a global survey that is commonly used when studying psychology across cultures. The responses consistently aligned with the values of English-speaking and Protestant European countries, according to the authors. However, the scientists found that ‘cultural prompting’ — asking the models to respond from the perspective of someone from a specific country — helped to reduce the bias.\n\nAt CENIA, researchers have built a ‘cultural benchmark’ test to compare how well LLMs represent the knowledge of Latin American people. “We designed questions based on verified information, structured into ‘knowledge graphs’ that serve as a ground truth,” explains Bianca Sofía del Solar, who studies engineering sciences at CENIA. The benchmark evaluates coverage (how much of the expected knowledge the model retrieves), accuracy (whether the answers match the graph) and hallucinations (whether the model invents information). The results revealed a stark gap. “All the models tested could identify the country where Buenos Aires is,” says computer scientist Andrés Carballo at CENIA, “but mostly fail in knowing what is ‘porotos con rienda’, or who Carlos Caszely is”.\n\nIn southeast Asia, researchers developed the evaluation suite Southeast Asian Holistic Evaluation of Language Models (SEA-HELM), a benchmark to assess LLMs for southeast Asian languages and culture. Researchers used SEA-HELM to compare regional models with some level of proficiency in these languages against larger ones, such as GPT-4 or the Chinese-built DeepSeek R1. They found that models fine-tuned using local data outperformed the largest global models for southeast Asian languages and that performance can improve through curated data collection3. AI Singapore built some of the models tested. “Two years ago, southeast Asian content made up about 0.5% of the data in models like Llama,” says Teo. In its latest model, which is built on Gemma, “it is closer to 40%, with the rest comprising English (40%), code and Chinese”, he says.\n\nTechnology giants such as Meta are also increasing the number of languages that their LLMs can translate and are incorporating more regional data into their training sets. But quantity does not guarantee quality. A 2023 analysis found that many of the data are scraped from websites that aren’t hosted in the countries of speakers of the target languages or include machine-translated content. This can undermine authenticity and limit the performance of models in informal or culturally specific contexts, state the paper’s authors, contradicting statements made by companies regarding high-quality multilingual translation. Researchers are also concerned that by creating the impression that machine translation has been solved, crucial resources could be taken away from the local research ecosystem.\n\nFixing the data gap\n\nIn December 2023, researchers at AI Singapore launched the first version of Southeast Asian Languages in One Network (SEA-LION). The model, which was trained from scratch, used data from Common Crawl that was filtered to preserve southeast Asian content and data sets contributed by local communities. Content from southeast Asian languages made up 13% of the training data. “We need data filters that can recognize languages like Thai, Indonesian or Tagalog and preserve them,” says Teo, who is the team lead for SEA-LION. “That means redoing much of the crawling and filtering pipeline.”\n\nVukosi Marivate co-founded the Masakhane initiative. Credit:\n\nAI specialist Omar Florez at CENIA is leading the pretraining of a local LLM known as Latam-GPT. Rather than relying on web scraping, the team trains the LLM with high-quality sources, such as repositories of undergraduate and graduate theses, local books digitized by university libraries and even transcripts from legislative sessions of the Colombian Congress — a rich data set for understanding how arguments, negotiations and policies are constructed in the region. “Our hypothesis is that culture lives in these documents,” says Florez.\n\nThe Masakhane initiative — masakhane means “we build together” in isiZulu — is an African-wide community of around 1,000 participants from 30 countries, collaborating on projects focused on collecting speech, text and annotation data sets to support African-language models. “Grassroots organizations like Deep Learning Indaba and Masakhane were created to strengthen machine learning and natural-language processing in Africa, and to make sure Africans voices are heard,” says Marivate, who co-founded both initiatives.\n\nMaskahane’s projects include MakerereNLP, which collects text and speech from East African university archives, newspapers, plays and radio transcripts, and MasakhaNER, a data set for named-entity recognition for ten African languages. This enables AI systems to identify key information such as names of people, places and dates in text — a task that can be especially complex in African languages, for which spelling and name patterns often differ from English norms. Other projects involving members of Masakhane are focused on collecting hundreds of hours of audio recordings of under-represented African languages to create high-quality speech data sets. These initiatives demonstrate how research involving speakers of local languages and local institutions, can scale the creation of meaningful data sets.\n\nIn 2025, the African Next Voices project announced it had recorded and transcribed 9,000 hours of everyday conversations across Kenya, Nigeria and South Africa funded by a US$2.2 million grant from the Gates Foundation in Seattle, Washington. The data set captures dialogue around farming, health and education in 18 African languages, including Kikuyu, Hausa, Yoruba, isiZulu and Tshivenda. It is also open access, enabling developers to create and train AI tools that transcribe, translate and respond in African languages.\n\nCollecting data in under-represented languages improves model coverage and boosts representation, Marivate says. Still, “users must understand that current models won’t fully eliminate biases because dominant languages continue to shape the data”, he cautions.\n\nLocal models for global voices\n\nHigh-quality regional training data sets are important, but they are only the first step. In the global south, researchers are developing models built on locally sourced data, using tools and techniques tailored to cultural and computational realities. “Above all, we need a belief in building for ourselves,” says Marivate.\n\nLeslie Teo presents SEA-LION, which uses data filtered for southeast Asian content.Credit:\n\nOne popular approach is to build on top of existing models for which the trained parameters are publicly available to allow developers to use and adapt them, such as Llama and Gemma. The SEA-LION model, which was initially trained from scratch, is now built on Gemma 3. “With the first version, we proved that more southeast Asian data improved performance on regionally relevant tasks,” says Teo. “But it was computationally and data-wise expensive to do everything from scratch.” Supplementing existing models with region specific capabilities — the original SEA-LION data set was supplemented with data from Wikipedia and high-quality sources in Indonesian, Tamil, Thai and Vietnamese — resulted in a 10–100 times reduction in cost, says Teo. This approach can, however, cause ‘forgetting’, which is when a model loses skills such as coding or reasoning after exposure to new data by overwriting old information. The model aims to capture local nuances, such as recognizing that if someone who is Muslim asks for food recommendations it should avoid suggesting pork dishes. SEA-LION now has 30,000–50,000 monthly users inputting requests such as “translate this into Tamil”, says Teo,\n\nThe Latam-GPT project is using Llama 3 as its basis. The team spent two years collecting data from public Latin American sources and through collaborations with institutions across the region. “We categorized and filtered data to create a mix that represents the traditions, facts and cultural expressions of Latin America,” explains Florez. “Collecting data from smaller countries or niche topics is expensive and time-consuming,” he says, “but this approach fills gaps and leverages the model itself to preserve and amplify local knowledge often missing from global datasets.”\n\nThe team is still training Latam-GPT, but is aiming for it to be open-source and publicly available by January 2026. “We’re not competing with the global models,” says Soto. “Our goal is to build a tool from and for Latin America.” The training involves the new Latin American data and the original LLM documents to prevent forgetting, and incorporates translations of Indigenous languages, such as Mapudungun, Náhuatl, Quechua and Aymara, to preserve and integrate knowledge. “In many ways,” says Florez, “large language models could become the most durable repositories of cultural memory, lasting longer than books or archives.”\n\nWhat distinguishes these efforts from those of the dominant technology firms is the close collaboration with local communities. We’re “working to build a network of people who believe in the project and want to contribute”, says Florez. Technical progress and a sense of ownership is enabling regional groups to shape digital tools that will increasingly govern their information ecosystems. AI models are not simply being localized — they are being co-created, with language at the forefront.",
      "url": "https://www.nature.com/articles/d41586-025-03891-y",
      "source": "Nature",
      "published": "2025-11-28",
      "sentiment_score": 0.85,
      "reasoning": "The article highlights significant local initiatives addressing bias and underrepresentation in large language AI models by developing culturally relevant and linguistically diverse datasets and models. These efforts have broad societal impact by promoting inclusion of minority and indigenous languages, preserving cultural knowledge, and enabling equitable AI access globally. The story is focused, detailed, and describes tangible progress toward reducing AI bias and empowering diverse communities.",
      "category": "Technology",
      "personality_title": "Local teams develop AI models that better reflect diverse languages and cultures",
      "personality_presentation": "**Context** – Large language models (LLMs) like ChatGPT often work best in English and other widely spoken languages. This happens because most training data comes from English sources, leaving many cultures and minority languages underrepresented.\n\n**What happened** – Researchers in places like Latin America, Africa, and Southeast Asia are creating new AI models using data from their own languages and cultures. For example, a team in Chile is building Latam-GPT with books, university theses, and government records from Latin America. In Southeast Asia, the SEA-LION model uses local language data to improve AI understanding. In Africa, groups like Masakhane collect speech and text in African languages to train better AI tools.\n\n**Impact** – These efforts help AI understand and speak languages that are usually left out. This means more people can use AI in their own language and cultural context. It also helps preserve languages that might disappear. For example, Latam-GPT includes Indigenous languages like Quechua and Aymara. This work shows that AI can be fairer and more useful when built with local knowledge.\n\n**What's next step** – Many of the new local AI models plan to become open and available for everyone. Latam-GPT aims to launch publicly by January 2026. Researchers also want to keep collecting high-quality data and improve AI’s ability to respect cultural details. The goal is to build AI tools with local communities, not just for them.\n\n**One-sentence takeaway** – Local researchers worldwide are creating AI language models that better represent diverse languages and cultures, making AI more fair and useful for everyone.\n",
      "personality_title_fr": "Des équipes locales développent des modèles d’IA qui reflètent mieux la diversité linguistique et culturelle",
      "personality_presentation_fr": "**Contexte** – Les grands modèles de langage (LLM) comme ChatGPT fonctionnent souvent mieux en anglais et dans d’autres langues largement parlées, car la plupart des données d’entraînement proviennent de sources anglophones, laissant de nombreuses cultures et langues minoritaires sous-représentées.\n\n**Ce qui s’est passé** – Des chercheurs en Amérique latine, en Afrique et en Asie du Sud-Est créent de nouveaux modèles d’IA à partir de données issues de leurs propres langues et cultures. Par exemple, une équipe au Chili construit Latam-GPT avec des livres, des thèses universitaires et des documents gouvernementaux d’Amérique latine. En Asie du Sud-Est, le modèle SEA-LION utilise des données locales pour améliorer la compréhension de l’IA. En Afrique, des groupes comme Masakhane collectent des textes et des enregistrements dans des langues africaines pour entraîner de meilleurs outils d’IA.\n\n**Impact** – Ces initiatives permettent à l’IA de comprendre et de parler des langues souvent ignorées, offrant ainsi à plus de personnes la possibilité d’utiliser l’IA dans leur langue et leur contexte culturel. Cela aide aussi à préserver des langues menacées. Par exemple, Latam-GPT inclut des langues indigènes comme le quechua et l’aymara. Ces travaux montrent que l’IA peut être plus juste et plus utile quand elle est construite avec la connaissance locale.\n\n**Étapes suivantes** – Beaucoup de ces nouveaux modèles locaux prévoient de devenir ouverts et accessibles à tous. Latam-GPT vise une sortie publique en janvier 2026. Les chercheurs veulent continuer à collecter des données de qualité et améliorer la capacité de l’IA à respecter les détails culturels. L’objectif est de construire des outils d’IA avec les communautés locales, et non seulement pour elles.\n\n**Résumé en une phrase** – Des chercheurs locaux dans le monde entier créent des modèles d’IA linguistiques qui représentent mieux la diversité des langues et des cultures, rendant l’IA plus équitable et utile pour tous.\n",
      "personality_title_es": "Equipos locales desarrollan modelos de IA que reflejan mejor la diversidad lingüística y cultural",
      "personality_presentation_es": "**Contexto** – Los grandes modelos de lenguaje (LLM) como ChatGPT suelen funcionar mejor en inglés y otros idiomas muy hablados, porque la mayoría de los datos usados para entrenarlos provienen de fuentes en inglés, dejando muchas culturas y lenguas minoritarias poco representadas.\n\n**Qué pasó** – Investigadores en América Latina, África y el sudeste asiático están creando nuevos modelos de IA usando datos de sus propios idiomas y culturas. Por ejemplo, un equipo en Chile está construyendo Latam-GPT con libros, tesis universitarias y registros gubernamentales de América Latina. En el sudeste asiático, el modelo SEA-LION usa datos locales para mejorar la comprensión de la IA. En África, grupos como Masakhane recogen textos y audios en lenguas africanas para entrenar mejores herramientas de IA.\n\n**Impacto** – Estos esfuerzos ayudan a que la IA entienda y hable idiomas que normalmente se dejan afuera. Esto permite que más personas usen la IA en su propio idioma y contexto cultural. También ayuda a preservar lenguas que podrían desaparecer. Por ejemplo, Latam-GPT incluye lenguas indígenas como el quechua y el aimara. Este trabajo muestra que la IA puede ser más justa y útil cuando se construye con conocimiento local.\n\n**Próximos pasos** – Muchos de los nuevos modelos locales planean ser abiertos y accesibles para todos. Latam-GPT espera lanzarse públicamente en enero de 2026. Los investigadores también quieren seguir recolectando datos de alta calidad y mejorar la capacidad de la IA para respetar detalles culturales. El objetivo es construir herramientas de IA con las comunidades locales, no solo para ellas.\n\n**Mensaje en una frase** – Investigadores locales en todo el mundo están creando modelos de IA que representan mejor la diversidad de idiomas y culturas, haciendo la IA más justa y útil para todos.\n",
      "image_url": "public/images/news_image_Large-language-models-are-biased-local-initiatives.png",
      "image_prompt": "A warm, detailed painting of a vibrant, open book whose pages transform into colorful, flowing ribbons inscribed with diverse scripts representing Latin American indigenous languages and Southeast Asian and African alphabets, weaving together into an intricate tapestry that connects to glowing, stylized neural network patterns symbolizing large language models, all set against a softly lit, earthy background emphasizing cultural richness and collaborative knowledge sharing."
    },
    {
      "title": "A structured system: the secrets of Germany’s scientific reputation",
      "summary": "Nature, Published online: 27 November 2025; doi:10.1038/d41586-025-03778-yThe European country has long been recognized as a model of efficiency and innovation — here’s how its research ecosystem is organized.",
      "content": "The Max Planck Institutes in Germany house cutting-edge equipment, such as this ultra-high vacuum facility at The Max Planck Institute of Microstructure Physics in Halle. Credit: Hendrik Schmidt/dpa/ALAMY\n\nIn 2019, shortly after finishing her master’s at Nanjing University in China, Xinyi Zhao opened an e-mail to learn that she had been offered a PhD position at the Max Planck Institute for Demographic Research in Rostock, Germany. “When I told my parents, they asked me to double-check whether the offer was real, as they weren’t familiar with the institute.” But Zhao knew of its glowing scientific reputation. “I felt very excited but also quite surprised,” says Zhao. “I heard they do amazing research.” Six years on, she’s still happily employed in Germany, now as a postdoc at the Berlin-based Max Planck Institute for Human Development.\n\nNature Career Guide: Germany\n\nBetween 2012 and 2022, the number of international scientists at the country’s four largest non-university research organizations doubled, from 8,115 to 16,6251. These institutes — the Max Planck Society, the Leibniz Association, the Helmholtz Association of German Research Centres and the Fraunhofer Society — all ranked among the world's top 20 non-profit organizations for research output in 2025, according to the Nature Index Research Leaders. The country’s universities also maintain a strong international profile, with 8 included in the world’s top 100, as reported in the Times Higher Education’s 2026 World University Rankings.\n\nXinyi Zhao, a postdoc at the Max Planck Institute for Human Development in Berlin, likes the research environment in Germany. Credit: Sarah Otterstetter / Max Planck Institute for Human Development\n\nGermany’s success in academic-excellence rankings lies partly in the structure of its scientific ecosystem — in which each research organization specializes in a particular kind of science — alongside the remarkably stable flow of public research funding. “Germany is a country without natural resources,” says Otmar Wiestler, former president of the Helmholtz Association. “We don’t have cheap labour. All we have is the brains of our people.” As a result, he adds, “the German government is really committed to promoting research and innovation”.\n\nStreamlined systems\n\nIn 2023, around 3.1% of Germany’s gross domestic product (GDP) — roughly €132 billion (US$152 billion) — was spent on research and development, a lower percentage than those of Israel (6%) and the United States (3.4%), but higher than the relative research spending of the United Kingdom (2.6%) and China (2.7%). Around one-third of this spending was from the public sector and the rest came from private industry. German governmental support for science has remained stable throughout various political leaderships. In the past 20 years, “we have never had a year where federal spending has decreased”, says Max Voegler, the vice-president of Global Strategic Networks for Germany, Austria and Switzerland at the Amsterdam-based academic publisher Elsevier, who co-wrote a report on Germany’s research system earlier this year2.\n\nAlthough the bulk of the funding for research and teaching in countries such as France and the United Kingdom comes from the central governments, in Germany, the task of supporting academic activities falls to both federal and state governments. These jointly provide the majority of the institutional funding for most of the country’s non-university research organizations. And in contrast to the United States and the United Kingdom, where universities draw much of their income from tuition fees, German states are responsible for providing the base funding for universities.\n\nOtmar Wiestler, former president of the Helmholtz Association, says that the German government is committed to promoting research and innovation.Phil Dera/Helmholtz\n\nFunding flows into a highly structured research ecosystem consisting of more than 420 universities, most of which are public, and four main non-university research organizations. Each of the four fulfils a defined, specialized task, says Rainer Frietsch, a specialist in science-and-innovation research at the Fraunhofer Institute for Systems and Innovation Research. The Max Planck Society, for instance, focuses on fundamental research, with each of its 84 institutes around the country specializing in a range of fields, such as animal behaviour and neurogenetics. The Leibniz Society’s institutes strive to integrate scientific knowledge into society by giving economic reports and providing museums that conduct research and communicate findings. The Helmholtz Association’s centres house some of the country’s largest pieces of research equipment, including the world’s largest artificial sun, used in the study of solar processes.\n\nThe 75 institutes of the Fraunhofer Society, meanwhile, fulfil the distinct task of conducting applied research; for instance, researchers at the Fraunhofer Institute for Solar Energy Systems in Freiburg are developing more-efficient solar cells, and scientists at the Fraunhofer Institute for Manufacturing Technology and Advanced Materials, which has several centres across Germany, are identifying innovative non-stick coatings for food-processing tools. Some of the society’s projects are for and funded by industry partners. Many countries, including France, the United Kingdom and China, have tried to emulate the Fraunhofer model in some form, but it works best in Germany’s research landscape, according to Frietsch. “Fraunhofer is as successful as it is because the whole system has exactly the layout that it has,” he says: the defined task division of the research organizations efficiently avoids overlap.\n\nAlthough distinct in their operations, Germany’s universities and non-university research organizations aren’t siloed from one another, says Voegler; universities often work with the non-university institutions nearby. In 2005, partly as a way of encouraging collaboration, the federal and the state governments launched the Excellence Initiative, which includes funding for interdisciplinary research projects at clusters of institutions.\n\nFrom 2026 to 2032, the initiative — now called the Excellence Strategy — has pledged €539 million per year for 70 ‘clusters of excellence’ projects across the country. One of these is the Cardio–Pulmonary System Cluster of Excellence in the state of Hessen, which brings together researchers from Goethe University Frankfurt, Justus Liebig University Giessen and the Max Planck Institute for Heart and Lung Research in Bad Nauheim to study heart and lung disease. By promoting the exchange of expertise across research entities, “suddenly the whole becomes much more than the sum of its parts”, Voegler says.\n\nChampioning discovery\n\nDuring discussions on European Union science-funding programmes, some member states have expressed a desire to invest mainly in applied research, Wiestler says. But German representatives have defended the need to focus on fundamental, ‘blue sky’ research — work for which practical applications aren’t immediately obvious. Enrico Schleiff, the president of Goethe University Frankfurt, attributes the emphasis on basic research to the fact that freedom of science is a constitutional right in Germany, enabling scientists to determine the direction of their research without government or political influence — something he says has led to many fruitful discoveries.\n\nSome of those impactful discoveries include the development of blinatumomab, the first T-cell engager drug, which coaxes patients’ immune cells into attacking leukaemia cells. More recently, scientists at Friedrich-Alexander University of Erlangen–Nuremberg were part of a team that discovered that these and other kinds of cancer immunotherapies could also be used to target the dysfunctional, body-attacking immune cells responsible for causing autoimmune diseases such as lupus. “Now the whole world is developing T-cell engagers” for this purpose, says Patrick Baeuerle, a biotechnology entrepreneur and the co-founder and chief scientific advisor of Cullinan Therapeutics, a company in Cambridge, Massachusetts, that licensed a T-cell engager drug from a China-based biotech firm in June.\n\nPatrick Baeuerle is a biotechnology entrepreneur and the co-founder and chief scientific adviser of Cullinan Therapeutics in Cambridge, Massachusetts.Credit: Cullinan Therapeutics\n\nOne of Germany’s long-standing weaknesses, Wiestler says, is that its research products are seldom commercialized in the country, because it lacks the kind of start-up culture and abundant sources of venture-capital funding that supports the biotech scene in countries such as the United States and Israel. That’s partly why the Helmholtz Association, for instance, has been increasingly establishing partnerships with companies to translate findings into marketable products. One example is the creation in 2023 of the Solar TAP Innovation platform — a joint venture between several Helmholtz Centres and industry partners to develop innovative solar technologies — and the Joint Innovation Lab, a collaboration between the German Cancer Research Center and the Hamburg-based skincare company Beiersdorf to develop strategies to prevent skin cancer.",
      "url": "https://www.nature.com/articles/d41586-025-03778-y",
      "source": "Nature",
      "published": "2025-11-28",
      "sentiment_score": 0.85,
      "reasoning": "The article highlights Germany's highly structured and well-funded scientific research ecosystem, which supports fundamental and applied research with broad societal benefits. It details significant investments in research infrastructure, international collaboration, and successful innovations such as cancer immunotherapies and solar technologies. The story emphasizes stable public funding, interdisciplinary cooperation, and a focus on discovery that collectively contribute to meaningful scientific progress with global impact.",
      "category": "Technology",
      "personality_title": "How Germany’s organized research system fuels scientific progress",
      "personality_presentation": "**Context** – Germany is known for its strong and efficient scientific research. The country has many specialized research institutes and universities that work together, supported by steady government funding.\n\n**What happened** – A report shows that Germany’s four biggest research organizations have doubled their international scientists in ten years and rank among the world’s best. These organizations focus on different areas: basic science, applied research, and connecting science with society. Germany spends a large amount of money on research, with both the federal and state governments sharing the cost. This system helps researchers like Xinyi Zhao, a scientist from China now working in Berlin, to thrive.\n\n**Impact** – Germany’s clear division of research tasks and stable funding create a strong environment for discovery. For example, German scientists helped develop new cancer treatments that use the body’s immune cells to fight disease. The cooperation between universities and research institutes also leads to projects like improving solar energy technology and studying heart and lung diseases. This organized approach makes research more effective and avoids duplication of efforts.\n\n**What's next step** – The government will continue funding collaborative research projects through the Excellence Strategy, supporting 70 groups across Germany until 2032. Research centers are also building partnerships with companies to turn discoveries into real products, such as new solar technologies and skin cancer prevention methods.\n\n**One-sentence takeaway** – Germany’s well-structured research system, backed by steady funding and teamwork, supports important scientific discoveries and practical innovations.\n",
      "personality_title_fr": "Comment le système organisé de recherche en Allemagne stimule le progrès scientifique",
      "personality_presentation_fr": "**Contexte** – L’Allemagne est reconnue pour sa recherche scientifique forte et efficace. Le pays compte de nombreux instituts et universités spécialisés qui collaborent, soutenus par un financement gouvernemental stable.\n\n**Ce qui s’est passé** – Un rapport montre que les quatre plus grandes organisations de recherche allemandes ont doublé leur nombre de scientifiques internationaux en dix ans et figurent parmi les meilleures au monde. Ces organisations se concentrent sur différents domaines : la science fondamentale, la recherche appliquée et le lien entre science et société. L’Allemagne consacre beaucoup d’argent à la recherche, avec un partage des coûts entre le gouvernement fédéral et les États. Ce système aide des chercheurs comme Xinyi Zhao, une scientifique chinoise travaillant maintenant à Berlin, à s’épanouir.\n\n**Impact** – La répartition claire des tâches de recherche et le financement stable créent un environnement propice à la découverte. Par exemple, des scientifiques allemands ont aidé à développer de nouveaux traitements contre le cancer qui utilisent les cellules immunitaires du corps pour combattre la maladie. La coopération entre universités et instituts de recherche mène aussi à des projets comme l’amélioration des technologies solaires et l’étude des maladies cardiaques et pulmonaires. Cette approche organisée rend la recherche plus efficace et évite les doublons.\n\n**Prochaines étapes** – Le gouvernement continuera à financer des projets de recherche collaboratifs via la Stratégie d’Excellence, soutenant 70 groupes à travers l’Allemagne jusqu’en 2032. Les centres de recherche développent aussi des partenariats avec des entreprises pour transformer les découvertes en produits réels, comme de nouvelles technologies solaires et des méthodes de prévention du cancer de la peau.\n\n**Message clé en une phrase** – Le système de recherche bien structuré de l’Allemagne, soutenu par un financement stable et la collaboration, favorise des découvertes scientifiques importantes et des innovations pratiques.\n",
      "personality_title_es": "Cómo el sistema organizado de investigación de Alemania impulsa el progreso científico",
      "personality_presentation_es": "**Contexto** – Alemania es conocida por su investigación científica fuerte y eficiente. El país tiene muchos institutos y universidades especializadas que trabajan juntas, apoyadas por un financiamiento gubernamental estable.\n\n**Qué pasó** – Un informe muestra que las cuatro mayores organizaciones de investigación de Alemania han duplicado a sus científicos internacionales en diez años y están entre las mejores del mundo. Estas organizaciones se enfocan en áreas diferentes: ciencia básica, investigación aplicada y conectar la ciencia con la sociedad. Alemania gasta mucho dinero en investigación, con costos compartidos entre el gobierno federal y los estados. Este sistema ayuda a investigadores como Xinyi Zhao, una científica china que ahora trabaja en Berlín, a prosperar.\n\n**Impacto** – La clara división de tareas de investigación y el financiamiento estable crean un ambiente fuerte para el descubrimiento. Por ejemplo, científicos alemanes ayudaron a desarrollar nuevos tratamientos contra el cáncer que usan las células inmunitarias del cuerpo para combatir la enfermedad. La cooperación entre universidades e institutos de investigación también lleva a proyectos como mejorar la tecnología solar y estudiar enfermedades del corazón y pulmones. Este enfoque organizado hace que la investigación sea más efectiva y evita esfuerzos duplicados.\n\n**Próximo paso** – El gobierno seguirá financiando proyectos de investigación colaborativos a través de la Estrategia de Excelencia, apoyando a 70 grupos en toda Alemania hasta 2032. Los centros de investigación también están formando asociaciones con empresas para convertir descubrimientos en productos reales, como nuevas tecnologías solares y métodos para prevenir el cáncer de piel.\n\n**Conclusión en una frase** – El sistema de investigación bien estructurado de Alemania, respaldado por financiamiento estable y trabajo en equipo, apoya importantes descubrimientos científicos e innovaciones prácticas.\n",
      "image_url": "public/images/news_image_A-structured-system-the-secrets-of-Germanys-scient.png",
      "image_prompt": "A detailed, warm-toned painting of an intricate, interconnected network of glowing scientific instruments and laboratory buildings shaped like puzzle pieces fitting perfectly together, symbolizing Germany’s structured research ecosystem; the scene includes stylized silhouettes of diverse researchers represented by abstract light figures collaborating around a central radiant core labeled with subtle motifs of microscopes, solar panels, and DNA strands, all rendered in natural, simple colors."
    },
    {
      "title": "Building compositional tasks with shared neural subspaces",
      "summary": "Nature, Published online: 26 November 2025; doi:10.1038/s41586-025-09805-2The brain can flexibly perform multiple tasks by compositionally combining task-relevant neural representations.",
      "content": "Monkeys\n\nTwo adult male rhesus macaques (Macaca mulatta) participated in the experiment. The number of monkeys (2) follows previous work using similar approaches21. Monkeys Si and Ch were between 8 and 11 years old and weighed approximately 12.7 and 10.7 kg, respectively. All of the experimental procedures were approved by the Princeton University Institutional Animal Care and Use Committee (protocol, 3055) and were in accordance with the policies and procedures of the National Institutes of Health.\n\nBehavioural task\n\nEach trial began with the monkeys fixating on a dot at the centre of the screen. During a fixation period lasting 500 ms–800 ms, the monkeys were required to keep their gaze within a circle with a radius of 3.25 degrees of visual angle around the fixation dot. After the fixation period, the stimulus and all four response locations were simultaneously displayed.\n\nStimuli were morphs consisting of both a colour and shape (Fig. 1a). The stimuli were rendered as three-dimensional models using POV-Ray and MATLAB (MathWorks) and displayed using Psychtoolbox on a Dell U2413 LCD monitor positioned 58 cm from the animal. Stimuli were morphed along circular continua in both colour and shape (that is, drawn from a four-dimensional ‘Clifford’ torus; Fig. 1b). Colours were drawn from a photometrically isoluminant circle in the CIELAB colour space, connecting the red and green prototype colours. Shapes were created by circularly interpolating the parameters defining the lobes of the ‘bunny’ prototype to the parameters defining the corresponding lobes of the ‘tee’ prototype. The mathematical representation of the morph levels adhered to the equation \\({X}_{1}^{2}+{X}_{2}^{2}+{X}_{3}^{2}={P}^{2}\\) where X represents the parameter value in a feature dimension (for example, L, a, b values in CIELAB colour space). We chose the radius (P) to ensure sufficient visual discriminability between morph levels. The deviation of each morph level from prototypes (0% and 100%) was quantified using percentage, corresponding to positions on the circular space from −π to 0 and 0 to π. Morph levels were generated at eight levels: 0%, 30%, 50%, 70%, 100%, 70%, 50%, 30%, corresponding to 0, π/6, π/2, 5π/6, π, −5π/6, −π/2 and −π/6, respectively. 50% morph levels for one feature were only generated for prototypes of the other feature (that is, 50% colours were only used with 0% or 100% shape stimuli and vice versa). Stimuli were presented at fixation and had a diameter of 2.5 degrees of visual angle.\n\nThe monkeys indicated the colour or shape category of the stimulus by saccading to one of the four response locations, positioned 6 degrees of visual angle from the fixation point at 45, 135, 225 and 315 degrees, relative to the vertical line. The reaction time was taken as the moment of leaving the fixation window relative to the time of stimulus onset. Trials with a reaction time below 150 ms were terminated, followed by a brief timeout (200 ms). Correct responses were rewarded with juice, while incorrect responses resulted in short timeouts lasting 1 s for monkey Ch and 5 s for monkey Si. After the trial finished, there was an intertrial interval lasting 2–2.5 s before the next trial began.\n\nThe animals performed three category-response tasks (Fig. 1c). The S1 task required the monkeys to categorize the stimulus by its shape. For a stimulus with a shape that was closer to the ‘bunny’ prototype, the animals had to make a saccade to the UL location to get rewarded. For a stimulus with a shape closer to the ‘tee’ prototype, the animals had to make a saccade to the LR location to be rewarded. For ease of notation, we refer to the combination of the UL and LR target locations as axis 1. The C1 task required the monkeys to categorize the stimulus by its colour. When a stimulus’ colour was closer to ‘red’, the animals made an eye movement to the LR location and when it was closer to ‘green’ the animals made an eye movement to the UL location. Finally, in the C2 task, the monkeys again categorized stimuli based on their colour but responded to the UR location for red stimuli and the LL location for green stimuli. Together, the UR and LL targets formed axis 2. This set of three tasks was designed to be related to one another: the C1 and C2 tasks both required categorizing the colour of the stimulus while the C1 and S1 tasks both required responding on axis 1.\n\nThe monkeys were not explicitly cued as to which task was in effect. However, they did perform the same task for a block of trials allowing the animal to infer the task based on the combination of stimulus, response and reward feedback. Tasks switched when the monkeys’ performance reached or exceeded 70% on the last 102 trials of task S1 and task C1 or the last 51 trials of task C2. For monkey Si, block switches occurred when their performance reached or exceeded the 70% threshold for all morphed and prototype stimuli in the relevant task dimension independently. For monkey Ch, block switches occurred when their average performance at each morph level in the relevant task dimension exceeded the 70% threshold (that is, average of all 30% morphs, average of all 70% morphs, and 0% and 100% prototypes in colour dimension were all equal or greater than 70% accuracy in the C1 and C2 tasks). Moreover, on a subset of recording days, to prevent monkey Ch from perseverating on one task for an extended period of time, the threshold was reduced to 65% over the last 75 trials for S1 and C1 tasks after the monkey had already done 200 or 300 trials on that block.\n\nWhen the animal hit the performance threshold, the task switched. This was indicated by a flashing yellow screen, a few drops of reward and a delay of 50 s.\n\nTo ensure even sampling of tasks despite the limited number of trials each day, the axis of response always changed following a block switch. During axis 1 blocks, either S1 task or C1 task was pseudorandomly selected, interleaved with C2 task blocks. Pseudorandom selection within axis 1 blocks avoided three consecutive blocks of the same task, ensuring the animal performed at least one block of each task during each session. On average, animals performed of 560, 558 and 301 trials and 2.68, 2.77 and 5.4 blocks per day for the S1, C1 and C2 tasks, respectively. Task orders and trial conditions were randomized across trials within each session.\n\nMonkeys Si and Ch underwent training for 36 and 60 months, respectively. Both animals were trained in the same order of tasks: S1, C2 and then C1. Each animal underwent exposure to every task manipulation. As all of the animals were allocated to a single experimental group, blinding was neither necessary nor feasible during behavioural training. Electrophysiological recordings began when the monkeys consistently executed five or more blocks daily. Further details on the behavioural methods and results have been previously reported19.\n\nCongruent and incongruent stimuli\n\nDuring the S1 and C1 tasks red-tee stimuli and green-bunny stimuli were ‘congruent’ as they required a saccade to the LR and UL locations, respectively, in both tasks. By contrast, stimuli in the red-bunny and green-tee portion of stimulus space were ‘incongruent’, as they required different responses during the two tasks (UL and LR in S1 task; LR and UL in C1 task, respectively). To ensure the animals performed the task well, 80% of the trials included incongruent stimuli. Note that, as the C2 task was the only one to use axis 2, there were no congruent or incongruent stimuli on those blocks.\n\nAnalysis of behavioural data\n\nPsychometric curves plot the fraction of trials in which the animals classified a stimulus with a specific morph level as being a member of the ‘green’ category for the C1 and C2 tasks or the ‘tee’ category for the S1 task. The fraction of responses for a given morph level of the task-relevant stimulus dimension was averaged across all morph levels of the task-irrelevant dimension during the last 102/102/51 trials of the S1/C1/C2 task (for example, for the C1 task, the fraction of responses for 70% green stimuli were averaged across all shape morph levels). As the stimulus space was circular, we averaged the behavioural response for the two stimuli at each morph level on each side of the circle (Fig. 1b). Psychometric curves were quantified by fitting the mean datapoints with a modified Gauss error function (erf):\n\n$$F=\\theta +\\lambda \\times \\left(\\mathrm{erf}\\,(\\frac{x-\\mu }{\\sigma })+1\\right)/2,$$\n\nwhere erf is the error function, θ is a vertical bias parameter, λ is a squeeze factor, μ is a threshold, and σ is a slope parameter. Fitting was done in MATLAB with the maximum likelihood method (fminsearch.m function).\n\nTo calculate performance during task discovery (Fig. 1f), we used a sliding window of 15 trials, stepped 1 trial, to calculate a running average of performance immediately after a switch or right before a switch. Performance was estimated using trials from all blocks of the same task, regardless of the identity of the stimulus. To test whether behavioural performance differed significantly between the C1–C2–C1 and S1–C2–C1 sequences during task discovery (Fig. 4a; S1–C2–S1 or C1–C2–S1 in Extended Data Fig. 8a and C1–C2 or S1–C2 in Extended Data Fig. 8b), we applied χ2 test using the chi2test.m function as implemented previously59.\n\nTo compare distribution of task performance after switch and before switch for C1 and S1 tasks (Extended Data Fig. 2e,f), we computed behavioural performance in first 15 trials after switch and last 15 trials before switch for each block, respectively.\n\nTo compare colour categorization performance based on the stimulus shape morph level in C1 and C2 tasks (Extended Data Fig. 2c), for each colour morph level, we computed colour categorization psychometric curves using combination of each colour morph level and three sets of shape morph levels: ambiguous, 50% and 150% (that is, −π/2); intermediate, 30%, 70%, 130% (that is, −5π/6) and 170% (that is, −π/6); and prototype, 0% and 100%.\n\nSurgical procedures and electrophysiological recordings\n\nMonkeys were implanted with a titanium headpost to stabilize their head during recordings and two titanium chambers (19 mm diameter) placed over frontal and parietal cortices that provided access to the brain. Chamber positions were determined using 3D model reconstruction of the skull and brain using structural MRI scans. We recorded neurons from the LPFC (Brodmann area 46 d and 46 v, 480 neurons), aIT (area TEa, 239 neurons), PAR (Brodmann area 7, 64 neurons), the FEF (Brodmann area 8, 149 neurons) and the striatum (STR, caudate nucleus, 149 neurons). The number of monkeys and the number of neurons recorded per region were chosen to follow previous work using similar approaches38.\n\nTwo types of electrodes were used during recordings. To record from the LPFC and FEF we used epoxy-coated tungsten single electrodes (FHC). Pairs of single electrodes were placed in a custom-built grid with manual micromanipulators that lowered electrode pairs using a screw. This enabled us to record 20–30 neurons simultaneously from cortical areas near the surface. To record from deeper regions (PAR, aIT and STR) we used 16ch or 32ch Plexon V-Probes (Plexon). These probes were lowered using the same custom-built grid through guide tubes. During lowering, we used both structural MRI scans and the characteristics of the electrophysiological signal to track the position of the electrode. A custom-made MATLAB GUI tracked electrode depth during the recording session and marked important landmarks until we found the brain region of interest.\n\nRecordings were acute; up to 50 single electrodes and three V-Probes were inserted into the brain each day (in some recording sessions one additional V-Probe was also inserted in the LPFC). Single electrodes and V-probes were lowered though the intact dura at the beginning of recording session and allowed to settle for 2–3 h before recording, improving the stability of recordings. Neurons were recorded without bias. Electrodes were positioned to optimize the signal-to-noise ratio of the electrophysiological signal without consideration of neural type or selectivity. Experimenters were blinded to experimental conditions while recording neurons. We did not simultaneously record from all five regions in all recording sessions. We began recording from the LPFC and FEF for the first 5–10 days and then added PAR, STR and aIT on successive days.\n\nBroadband neural activity was recorded at 40 kHz using a 128-channel OmniPlex recording system (Plexon). We performed 15 recording sessions with monkey Ch and 19 recording sessions with monkey Si. After all recordings were complete, we used electrical microstimulation in monkey Si, and structural MRI and microstimulation in monkey Ch, to identify the FEF. Electrical stimulation was delivered as a train of anodal-leading biphasic pulses with a width of 400 µs and an interpulse frequency of 330 Hz. A site was identified as the FEF when electrical microstimulation of around 50 µA evoked a stereotyped eye-movement on at least 50% of the stimulation attempts. In monkey Ch, untested electrode locations were classified as FEF if they fell between two FEF sites (as confirmed with electrical stimulation) in a region that was confirmed as being FEF using MRI.\n\nEye position was recorded at 1 kHz using an Eyelink 1000 Plus eye-tracking system (SR Research). Sample onset was recorded using a photodiode attached to the screen. Eye position, photodiode signal and behavioural events generated during the task were all recorded alongside neural activity in the OmniPlex system.\n\nSignal preprocessing\n\nElectrophysiological signals were filtered using a 4-pole high pass 300 Hz Butterworth filter. To reduce common noise, the median of the signals recorded from all single electrodes in each chamber was subtracted from the activity of all single electrodes. For V-Probe recordings, we subtracted the median activity for all channels along the probe from each channel. To detect spike waveforms, a 4σ n threshold was used where σ n is an estimate of s.d. of noise distribution of signal x defined as:\n\n$${\\sigma }_{n}=\\mathrm{median}\\,\\left(\\frac{|x|}{0.6745}\\right)$$\n\nTimepoints at which the electrophysiological signal (x) crossed this threshold with a negative slope were identified as putative spiking events. Spike waveforms were saved (total length was 44 samples, 1.1 ms, of which 16 samples, 0.4 ms, were pre-threshold). Repeated threshold crossing within 48 samples (1.2 ms) was excluded. All waveforms recorded from a single channel were manually sorted into single units, multiunit activity or noise using Plexon Offline Sorter (Plexon). Units that were partially detected during a recording session were also excluded. Experimenters were blinded to the experimental conditions while sorting waveforms into individual neurons. All analyses reported in this Article were performed on single units.\n\nFor all reported electrophysiology analysis, saccade time was calculated as the moment at which the instantaneous eye speed exceeded a threshold of 720 degrees of visual angle per second. Instantaneous eye speed was calculated as \\(\\sqrt{{\\left(\\frac{{\\rm{d}}x}{{\\rm{d}}t}\\right)}^{2}+{\\left(\\frac{{\\rm{d}}y}{{\\rm{d}}t}\\right)}^{2}}\\), where x and y are the position of the eye on the monitor at time t.\n\nStatistics and reproducibility\n\nIndependent experiments were performed on two monkeys and the data were combined for subsequent analyses. As described below, statistical tests were corrected for multiple comparisons using two-tailed cluster correction unless stated otherwise. Unless otherwise noted, nonparametric tests were performed using 1,000 iterations; therefore, exact P values are specified when P > 0.001. Unless stated otherwise, all data were smoothed with a 150 ms boxcar. To compare the onset timing differences between within and shared-colour representations across brain regions (Fig. 2g and Extended Data Fig. 6d), classifier accuracy for each region was smoothed using a 50 ms boxcar filter.\n\nAll analyses were performed in MATLAB 2021b (MathWorks).\n\nUsing cluster mass to correct for multiple comparisons\n\nTo assess the significance of observed clusters in the time series data, we used a two-tailed cluster mass correction method60. This approach is particularly useful when dealing with multiple comparisons and helps identify clusters of contiguous timepoints that exhibit statistically significant deviations from the null distribution.\n\nWe first generated a null distribution (NullDist) by shuffling the observed data, breaking the relationship between the neural signal and the task parameter of interest (details of how data were shuffled are included with each test below). The observed data were also included as a ‘shuffle’ in the null distribution and the z-score of the null distribution for each timepoint was calculated. To define significant moments in time, we computed the upper and lower thresholds based on the null distribution. The thresholds were determined non-parametrically using percentiles. The resulting thresholds, denoted as P threshold upper and P threshold lower , serve as critical values for identifying significant deviations in both tails.\n\nTimepoints of significant signal were identified by finding the moments when the value in each shuffle within the null distribution or the observed data exceeded the computed thresholds. These timepoints were then clustered in time, such that contiguous values above the threshold were summed together. The sum was calculated on the z-transformed values of each time series. This resulted in a mass of each contiguous cluster in the data. To correct for multiple comparisons, we took the maximum absolute value of the cluster mass across time for each shuffle. This resulted in a distribution of maximum cluster masses. Finally, the two-tailed P value of each cluster in the observed data was determined by comparing its mass to the distribution of maximum cluster masses in the null distribution.\n\nFR calculation\n\nIn all analyses, we estimated the FR of each neuron by averaging the number of action potentials within a 100 ms window, stepping the window every 10 ms. Changing the width of the smoothing window did not qualitatively change our results. The time labels in the figures denote the trailing edge of this moving window (that is, 0 ms would be a window from 0 ms to 100 ms).\n\nGLM\n\nTo estimate how individual neurons represented task variables, we used a generalized linear model (GLM) to relate the activity of each neuron (y) to the task variables at each moment of time. The full model was:\n\n$$y={\\beta }_{0}+{\\beta }_{1}\\times \\mathrm{stimulus}\\,\\mathrm{colour}\\,\\mathrm{category}+{\\beta }_{2}\\times \\mathrm{stimulus}\\,\\mathrm{shape}\\,\\mathrm{category}+{\\beta }_{3}\\times \\mathrm{time}+{\\beta }_{4}\\times \\mathrm{task}\\,\\mathrm{identity}+{\\beta }_{5}\\times \\mathrm{motor}\\,\\mathrm{response}\\,\\mathrm{direction}+{\\beta }_{6}\\times \\mathrm{reward}$$\n\nwhere y is the FR of each neuron, normalized to the maximum FR across all trials, and β 0 , β 1 , …, β 6 are the regression coefficients corresponding to each predictor. Predictors were: stimulus colour category, indicating the colour category of the stimulus (categorical variable); stimulus shape category, indicating the shape category of the stimulus (categorical variable); time, indicating the temporal progression within a recording session, normalized between 0 and 1 (continuous variable); task identity, indicating the identity of the task (categorical variable); motor response direction, indicating the direction of the motor response (categorical variable; UL, UR, LL or LR); reward, indicating whether a reward was received following the response (binary variable; 1 for reward, 0 for no reward).\n\nThe GLM coefficients (β) were estimated using maximum-likelihood estimation as implemented by MATLAB’s lassoglm.m function. Independent models were fit for each timepoint. To address potential overfitting, we applied Lasso (L1) regularization to the GLM weights with regularization coefficients (lambda) values of [0, 0.0003, 0.0006, 0.0009, 0.0015, 0.0025, 0.0041, 0.0067, 0.0111, 0.0183]. In a separate set of runs, we fit the GLM models to 80% of data and tested on the remaining 20%. The lambda value with maximum R2 on the withheld data was used when estimating the CPD.\n\nCPD calculation\n\nTo quantify the unique contribution of each predictor to a neuron’s activity, we used the CPD. This metric quantifies the percent of explained variance that is lost when a specific factor is removed from the full model. The CPD was computed by initially fitting the GLM with all the factors (full model) as described above. Each factor was then sequentially removed to create a set of reduced models, each of which were fit to the data. The CPD for each predictor X(t) was calculated as:\n\n$$\\mathrm{CPD}(X(t))=\\frac{{\\mathrm{SSE}}_{\\mathrm{reduced}}-{\\mathrm{SSE}}_{\\text{full model}}}{{\\mathrm{SSE}}_{\\mathrm{reduced}}}\\times 100$$\n\nwhere SSE reduced is the sum of square error due to predictor X, SSE full model is the sum of square error of the full model with all predictors61. Note that, because the CPD statistic estimates the additional explained variance captured by each term, it controls for potential covariation between terms. Nevertheless, it is important to note that, as with all neurophysiological studies, it is difficult to distinguish between the encoding of a specific cognitive variable (for example, task) and its effect on the representation of other cognitive variables (for example, the observed suppression of sensory and/or motor representations).\n\nWe used a permutation test to assess whether CPDs for each predictor, at each timepoint, and for each neuron, was significantly larger than expected by chance. To compute a null distribution for CPD for each predictor, we generated 1,000 permuted datasets by randomly permuting the predictor values relative to the neural activity and refitting the full and reduced models (as above). The likelihood of the observed CPD was then estimated by computing the proportion of permuted datasets that yielded a CPD greater than or equal to that of the observed dataset. To account for multiple comparisons across timepoints, we used cluster correction (detailed above) to estimate corrected P values for each predictor. Neurons that had at least one significant cluster (P < 0.05) were considered to significantly carry information about a task variable. To assess whether an area had a significant number of neurons for a given task variable, we used a binomial test against the alpha-level 0.05.\n\nFor Fig. 1h–l, we compensated for a baseline CPD by subtracting the average permuted CPD from the observed CPD. For Fig. 1m, we averaged the CPD for each factor across all recorded neurons, subtracted the average baseline in −200–0 ms as the stimulus onset period and then normalized the resulting CPD curve to its maximum value. For Extended Data Fig. 3a, we averaged the CPD for task identity factor across all recorded neurons then normalized the resulting CPD curve to its maximum value. This highlighted the timing of different factors during the trial.\n\nQuantifying population overlap for encoding of task variables\n\nTo determine whether task variables were encoded by overlapping neural populations (Extended Data Fig. 3b–k), we examined neurons for each task variable pair (for example, task identity and reward) to identify those that significantly encoded either or both variables using the GLM model explained above. Using a permutation test, we calculated the P value for neurons encoding both variables. For a neural population of size N Total , with N TaskVar1 neurons encoding the first task variable, N TaskVar2 neurons encoding the second, and N TaskVar1&TaskVar2 neurons encoding both, we iterated 10,000 random samples of N TaskVar1 and N TaskVar2 neurons from N Total . In each iteration, we counted neurons encoding both variables. The likelihood of the observed N TaskVar1&TaskVar2 was estimated by calculating the proportion of permutations yielding an equal or greater count of neurons encoding both task variables.\n\nClassifiers\n\nTo understand how task variables were represented in the neural population, we trained a set of binary classifiers to discriminate the vector of FRs across the neural population for two different categories of task variables (depending on the variable of interest). Classifiers were trained with the logistic regression algorithm (as implemented in MATLAB, fitclinear.m function). In brief, the linear classifier relates x (vector of neural responses) to \\(y\\) (task labels, either +1 or −1) through a linear equation, with weights (w) and intercept (b):\n\n$$f(x)={w}^{T}{\\bf{x}}+b$$\n\nwhere w and b are optimized to minimize the logistic loss function: \\({\\mathcal{L}}[y,f(x)]=\\frac{-1}{N}{\\sum }_{i=1}^{N}[{y}_{i}\\log (f({x}_{i}))+(1-{y}_{i})\\log (1-f({x}_{i}))]\\,\\), where N is the number of samples. Ridge (L2) regularization with a regularization coefficient λ = 1/60 was used to minimize over-fitting. The classifier was trained with the Broyden–Fletcher–Goldfarb–Shanno algorithm.\n\nConstruction of pseudopopulations\n\nAll classifiers were trained on pseudopopulation of responses constructed from neurons across all recording sessions. A separate pseudopopulation was constructed for each classifier analysis based on the trial types of interest (described below). To be included in the pseudopopulation, each neuron had to be recorded for a minimum number of trials (N train + N test ) of each type of trial. The FR of neurons were concatenated to form a vector of neuron FRs (the pseudopopulation). Neural activity was aligned in time relative to either the sample onset or saccade onset, depending on the analysis.\n\nTo combine neurons across recording sessions into a single ‘trial’ of the pseudopopulation, we drew trials for each neuron with matching experimental conditions (that is, matched in terms of reward, colour and shape morph level, task identity and response direction). For example, when constructing a pseudopopulation for classifying the colour category, the first trial was constructed by concatenating the neural responses of neurons on trials that were rewarded, had a colour morph level of 100 and a shape morph level of 30. If a neuron did not have a trial with an exactly matching stimulus, then a trial that matched the colour category and reward would be randomly chosen.\n\nCross-validation of classifiers\n\nA separate logistic regression classifier was trained and tested on withheld trials for each timepoint. To estimate variability, the entire analysis was repeated 250 times, with each iteration involving a different partition of trials into training and testing sets. This new set of trials was randomly sampled with replacement (always ensuring test trials were separate from the train trials).\n\nWhen testing the performance of a classifier trained on the same set of conditions (for example, training and testing colour categorization on the C2 task), the performance of the classifier was taken as the average performance across tenfold cross-validation. However, as detailed in the main text, many of the classifiers were trained and tested across conditions (for example, across different tasks or across trials during learning of the task). In this case, the test trials were randomly resampled ten times for each training set, and the classifier’s performance was averaged across folds.\n\nBalancing classifier conditions\n\nTo avoid bias in the classifier, we balanced trial conditions to ensure the observed results were not due to other experimental factors. Conditions were balanced in three ways:\n\n(1) Balanced congruency: classifiers were trained on an equal number of congruent and incongruent trials, resulting in equal number of trials for four stimulus types: green-bunny, green-tee, red-bunny and red-tee. (2) Balanced reward: classifiers were trained on an equal number of rewarded and unrewarded trials, balancing the stimulus identity of the relevant dimension on each side of the classifier. (3) Balanced response direction: classifiers were trained on an equal number of trials from each response direction on the task’s response axis (for example, trials with response on axis 1 for the C1 task). This necessarily included error trials but balanced the number of each response direction on each side of the classifier.\n\nClassifying the colour category and shape category of stimuli\n\nColour and shape classifiers were trained to decode the stimulus colour category or shape category from the vector of activity across the pseudopopulation of neurons (Fig. 2a,b). A balanced number of congruent and incongruent stimuli were included in the training data to ensure colour and shape information could be decoded independently. Classifiers were trained for each task independently.\n\nMost classifiers were trained on correct trials alone. This maximized the number of trials included in the analysis and ensured the animals were engaged in the task. As many of our analyses were tested across tasks, this mitigates concerns that motor response information might confound stimulus category information. However, to ensure response direction did not affect our analyses, we controlled for motor response by balancing response direction in a separate set of analysis (Extended Data Figs. 4a and 5c,d). Although this significantly reduced the number of trials, and required us to include error trials, qualitatively similar results were often observed. The total number of LPFC, STR, aIT, PAR and FEF neurons used for colour category classification was 403, 110, 195, 54 and 116, respectively. The total numbers of LPFC, STR, aIT, PAR and FEF neurons used for shape category classification were 480, 149, 239, 64 and 149, respectively.\n\nClassifying response direction\n\nA separate set of response classifiers were trained to decode the motor response from the vector of activity across the pseudopopulation of neurons. Response direction was decoded within each axis (for example, UL versus LR for axis 1). When training and testing on the same task (Fig. 2c), we included only trials that the animal responded on the correct axis (for example, axis 1 for the S1 and C1 tasks).\n\nFor testing whether response direction could be decoded across axes (Extended Data Fig. 5d), we trained the classifier using trials from task C1, where the animal responded on axis 1, and then tested this classifier using trials from task C2, where the animal responded on axis 2, and vice versa. To control for stimulus-related information, we balanced rewarded and unrewarded trials for each condition and only included incongruent trials. This ensured that trials from the same stimulus category were present in both response locations. Similar results were seen when balancing for congruency and reward simultaneously, although the low number of incorrect trials for congruent trials resulted in a small set of neurons with the minimum number of train and test trials. The total number of LPFC, STR, aIT, PAR and FEF neurons used for response direction classification was 403, 110, 195, 54 and 116, respectively. Owing to limited number of trials, the total number of LPFC neurons for Extended Data Figs. 4b and 5d was 95 (only from animal Si).\n\nUsing permutation tests to estimate the likelihood of classifier accuracy\n\nWe used permutation tests to estimate the likelihood the observed classifier accuracy occurred by chance. To create a null distribution of classifier performance, we randomly permuted the labels of training data 1,000 times. Importantly, only the task-variable of interest was permuted—permutations were performed within the set of trials with the same identity of other (balanced) task variables. For example, if trials were balanced for reward, we shuffled labels within correct trials and within incorrect trials, separately. This ensured that the shuffling broke only the relationship between the task-variable of interest and the activity of neurons. Shuffling of the labels was performed independently for each neuron before building the pseudopopulation and training classifiers. Neurons that were recorded in the same session had identical shuffled labels. To stabilize the estimate of the classifier performance, the performance of the classifier on the observed and each instance of the permuted data were averaged over 10-folds and 20 to 50 novel iterations of each classifier. The null distribution was the combination of the permuted and observed values (total n = 1,001). The likelihood of the observed classifier performance was then estimated as its percentile within the null distribution. As described above, we used cluster correction to control for multiple comparisons across time.\n\nTesting classification across tasks\n\nTo quantify whether the representation of colour, shape, or response direction generalized across tasks (Fig. 2e,f,h,i), we trained classifiers on trials from one task and then tested the classifier on trials from another task. For example, to test cross-generalization of colour information across the C1 and C2 tasks, a colour classifier was trained on trials from the C2 task was tested on trials from the C1 task (Fig. 2f). To remove any bias due to differences in baseline FR across tasks, we subtracted the mean FR during each task from all trials of that task (at each timepoint). Similar results were observed when we did not subtract the mean FR.\n\nCross-temporal classification\n\nTo measure how classifiers generalized across time, we trained classifiers to discriminate colour category (Fig. 2e and Extended Data Fig. 5b) or response direction (Fig. 2h) using 100 ms time bins of FR data, sliding by 10 ms. Classifiers trained on each time bin were then tested on all time bins of the test trials.\n\nProjection onto the encoding axis of classifier\n\nTo visualize the high-dimensional representation of task variables, we used the MATLAB predict.m function to project the FR response on test trials onto the one-dimensional encoding space defined by the vector orthogonal to the classification hyperplane. In other words, the projection measures the distance of the neural response vector to the classifier hyperplane. For example, to measure the encoding of each colour category in the C1 task, we projected the trial-by-trial FR onto the axis orthogonal to the hyperplane of the colour classifier trained during the C1 task.\n\nQuantifying the impact of task sequence on task discovery\n\nThe task-discovery period was defined as the first 110–120 trials after a switch in the task. The monkey’s performance increased during this period, as they learned which task was in effect (Fig. 1f). As described in the main text, the animal’s behavioural performance depended on the sequence of tasks (Fig. 4a) and we therefore analysed neural representations separately for two different sequences of tasks:\n\n(1) C1–C2–C1 and S1–C2–S1 block sequence (same task transition): the task on axis 1 (C1 or S1) repeated across blocks. As shown in the main text, monkeys tended to perform better during these task sequences, as if they were remembering the previous axis 1 task. (2) S1–C2–C1 and C1–C2–S1 block sequence (different task transition): the task on axis 1 changed across blocks. As shown in the main text (Fig. 4a), monkeys tended to perform worse on these task sequences.\n\nAs we were interested in understanding how changes in neural representations corresponded to the animals’ behavioural performance, we divided the ‘different task transition’ sequences into two further categories: in ‘Low Initial Performance’ blocks, the animal’s behavioural performance during the first 25 trials of the block was less than 50%, while on the ‘high initial performance’ blocks, the performance was greater than 50%.\n\nOwing to constrains on number of neurons, different but overlapping population of neurons were used to quantify neural representations in C1–C2–C1 and S1–C2–C1 tasks sequences.\n\nTask belief representation during task discovery\n\nTo measure the animal’s internal representation of the task (that is, their ‘belief’ about the task, as represented by the neural population), we trained a task classifier to decode whether the current task was C1 or S1. The classifier was trained on neural data from the last 75 trials of each task block, when the animal’s performance was high (reflecting the fact that the animals were accurately estimating the task at the end of the block). The number of congruent and incongruent trials were balanced in the training dataset (32 trials: 4 trials for each of the four stimulus types, in each task). We included all C1 blocks regardless of their task sequence in training set. Only correct trials were used to train and test the classifier. To minimize the effect of neural response to visual stimulus, the classifier was trained on neural activity from the fixation period (that is, before stimulus onset). As we were interested in measuring differences between tasks, we did not subtract the mean firing rate before training the classifier.\n\nThe task classifier was tested on trials from the beginning of blocks of the C1 task. Test trials were drawn from windows of 40 trials, slid every 5 trials, during learning (starting from trial 1–40 to trial 71–110). Overlapping test and train trials from the same task were removed. In contrast to training set, testing was done separately for S1–C2–C1 and C1–C2–C1 task sequences (Fig. 4d). As we were interested in focusing on the learning of the C1 task, we only used S1–C2–C1 task sequences with low initial performance (see above). Classifiers were tested on pseudopopulations built from trials within each trial window, with a minimum of four test trials (one trial for each of the four stimulus congruency types from task C1). Neurons that did not include the required number of test trials for all trial windows were dropped. Note that, as we were using a small moving window of trials during task discovery, we had to trade-off the number of included neurons and the number of train/test trials for this analysis. Moreover, although the number of correct trials increased as the animal discovered the task in effect, we kept a constant number of train and test trials in each of the sliding trial windows.\n\nTo quantify the animal’s belief above the current task, we measured the distance to the hyperplane of the C1/S1 task classifier (task belief encoding). For Fig. 4d, we averaged the performance of the classifier in pre-stimulus processing window (−400ms to 0 ms).\n\nCorrelation between task belief encoding and behavioural performance\n\nTo calculate the correlation between the animal’s task belief during task discovery and their behavioural performance (Fig. 4e), we used Kendall’s τ statistic with a permutation test to correct for autocorrelation in the signal (detailed below). This measurement was performed using data from each window of trials and the belief encoding, as estimated from the task classifier, on that same window of trials. As we are working with pseudopopulations of neurons, we estimated the behavioural performance for each window of trials during learning as the average of behavioural performance across all the trials in the window. The behavioural performance for each trial was taken as the average of the animal’s behavioural performance during the previous 10 trials. This yielded one average performance for each of the 16 trial windows. The task belief was measured for the same trials, using the average distance to the task classifier hyperplane, averaged over the time period from 400 ms to 0 ms before the onset of the stimulus (note that, as described above, all of these trials are withheld from the training data). Task belief was then taken as the average distance across all trials in a window of trials.\n\nEvolution of colour category representation during task discovery\n\nWe were interested in quantifying how the shared colour representation was engaged as the animal discovered the C1 task (Fig. 4f,g). To this end, we trained a classifier to categorize colour using the last 75 trials of the C2 task, when the animal’s behavioural performance was high, and then tested it during discovery of the C1 task. The classifier was trained on only correct trials and the training data were balanced for congruent and incongruent stimuli (16 trials: 4 trials for each of the four stimulus congruency types). This ensured an equal number of correct trials for each congruency type across all trial windows, controlling for motor response activity during the task discovery period. As cross-axis response decoding between the C1 and C2 tasks was weak (Extended Data Fig. 5d), cross-task classifiers are capturing the representation of the colour category that is shared between tasks.\n\nSimilar to the task classifier described above, the shared colour classifier was tested using trials from the C1 task in a sliding window of 40 trials stepped 5 trials, in both the S1–C2–C1 and C1–C2–C1 sequences of tasks (Fig. 4f; as above, only low initial performance blocks were used for the S1–C2–C1 task sequences). We used four trials to test the classifier (one trial for each of the four stimulus congruency types from task C1). For Fig. 4g, we averaged the performance of the classifier in stimulus processing window (100 ms to 300 ms).\n\nEvolution of shape category representation during task discovery\n\nWe were interested in measuring the change in the representation of the stimulus’ shape category as the animal learned the C1 task (Fig. 4i). Our approach followed that of the colour category representation described above, and so we only note differences here. We trained a classifier to categorize stimulus shape based on neural responses during the S1 task (limited to the last 75 trials of each block). To ensure the classifier was only responding to shape (and not motor response), we trained the classifier on a balanced set of correct (rewarded) and incorrect (unrewarded) trials (16 trials: 4 trials for each reward condition and for each shape category). The classifier was tested using trials from the C1 task (Fig. 4i). Note that we used the same C1 trials to quantify the representation of shared colour category, shape category and task belief for S1–C2–C1 task sequences. For Fig. 4j, we averaged the performance of the classifier in the window of time when stimuli were processed (100 ms to 300 ms after stimulus onset). The total numbers of LPFC neurons included for S1–C2–C1 and C1–C2–C1 task sequences to quantify the representation of shared colour category, shape category and task belief were 136 and 154, respectively.\n\nEvolution of response direction representation during task discovery\n\nTo measure the change in response direction representation as the monkey’s learned the C1 task, we trained a classifier to categorize the response direction based on the neural response during the S1 task (Fig. 4k; last 75 trials of the block). As above, we used a balanced number of correct and incorrect trials to control for information about the stimulus (12 trials: 3 trials for each reward condition and each response location). The classifier was then tested on correct trials from the C1 task during task learning (Fig. 4k; as above: sliding windows of 40 trials, stepped 5 trials, in S1–C2–C1 and C1–C2–C1; tested on 4 trials: 1 for each reward condition and each response location). For Fig. 4l, we averaged the performance of the classifier in the window of time when response location was processed (200 ms to 400 ms after stimulus onset). The total numbers of LPFC neurons included for S1–C2–C1 and C1–C2–C1 task sequences to quantify the representation of shared response direction were 120 and 155, respectively.\n\nClassifier statistical test to detect trends during task discovery period\n\nTo quantify the statistical significance of trends in representations during task discovery (Figs. 4 and 5), we used trend-free prewhitening (TFPW)62,63, as implemented in MATLAB64. This method helps to reduce serial correlation in time-series data to obtain robust statistical inference in the presence of trends. TFPW first detrends the time series by removing Sen’s slope. It then prewhitens the time series by modelling autocorrelation with an autoregressive (AR) model (typically AR(1)) to produce residuals free of temporal dependencies. Finally, it adds back the original trend to generate processed time series. Both the Mann–Kendall statistics and Sen’s slope were used to estimate the significance of trends in the data.\n\nTo ensure that our reported statistics are fully unbiased, we used the estimated Sen’s slope on prewhitened data using TFPW to estimate the P value of the observed data. To do so, we used permutation tests to estimate the likelihood the observed trend slope occurred by chance. To create a null distribution of classifier performance, we randomly permuted the labels of test data 250 times across all trial windows (trials 1–110 for Fig. 4 and 1–120 for Fig. 5). To stabilize the estimate of the classifier performance, we tested classifiers using the same set trained classifiers (for example, use same set of classifiers trained to decode colour category in C2 task to test colour category decoding in C1 task during task discovery). Furthermore, the performance of the classifier on the observed and each instance of the permuted data were averaged over 10-folds and 50 novel iterations of each classifier. The slope was calculated for each timepoint across all time trial windows to estimate the trend for observed and permuted classifier performances. The null distribution was the combination of the permuted and observed slope values (total n = 251). The likelihood of the observed slope was then estimated as its percentile within the null distribution. As described above, we used cluster correction to control for multiple comparisons across time.\n\nTo measure rank correlation between two random variables (for example, correlation between task belief and behavioural performance, Fig. 4e), we used Kendall’s τ. As TFPW requires a monotonic time variable and cannot be applied here, we computed Kendall’s τ for both observed and permuted datasets. z-score values of observed data against permuted values were reported to account for autocorrelation inflation.\n\nTransfer of information between subspaces\n\nAs described in the main text, we were interested in testing the hypothesis that the representation of the stimulus colour in the shared colour subspace predicted the response in the shared response subspace on a trial-by-trial basis (Fig. 3c–e and Extended Data Fig. 7a–c). To this end, we correlated trial-by-trial variability in the strength of encoding of colour and response along four different classifiers (using Pearson’s correlation as implemented in MATLAB’s corr.m function).\n\nFirst, as described above, a shared colour classifier was trained to decode colour category from the C2 task and tested on trials from the C1 task. Training data were balanced for correct and incorrect trials (rewarded and unrewarded trials, 16 trials: 4 trials for each reward condition and each colour category). Test trials were all correct trials (2 trials: 1 trial for each colour category). Both train and test trials were drawn from the last 50 trials from the block (to ensure the animals were performing the task well). The total number of LPFC neurons included for this classifier was 63 (only from monkey Si, owing to constraints on the number of trials).\n\nSimilarly, a second shared response classifier was trained on trials from the S1 task and then tested on the same set of test trials as the shared colour classifier.\n\nA third classifier was trained to decode the response direction on axis 2, using a balance of correct and incorrect trials from the C2 task. This classifier was tested on the same set of test trials as the shared colour classifier.\n\nFinally, a fourth classifier was trained to decode the shared colour representation but was now trained on correct trials from the C1 task (12 trials: 3 trials for each of the four stimulus congruency types) and tested on correct trials from C2 task (2 trials: 1 trial for each colour category). The total number of LPFC neurons included for this classifier was 101 (only from animal Si, owing to constraints on the number of trials).\n\nTo account for the arbitrary nature of positive and negative labels, we calculated the magnitude of encoding by flipping the encoding for negative labels. All four classifiers were trained on 2,000 iterations of training set trials. Note, that the first three classifiers were tested on the same test trials, allowing for trial-by-trial correlations to be measured. Furthermore, all four classifiers were trained over time, enabling us to measure the cross-temporal correlation between any pairs of classifiers.\n\nTogether, these four classifiers allowed us to test three hypothesized correlations that reflect the transfer of task-relevant stimulus information into representations of behavioural responses. First, one might expect that, on any given trial before the start of saccade, the strength and direction of the shared colour representation, as estimated by the distance to the hyperplane of the first classifier, should be correlated with the shared response, as estimated by the distance to the hyperplane of the second classifier. This correlation is seen in Fig. 3c.\n\nSecond, during the performance of the C1 task, one would expect that before the start of saccade the shared colour representation should not be correlated with the response on the Axes 2 predicted by the C2 task. This is quantified by correlating the distance to the hyperplane of the first classifier and the distance to the hyperplane of the third classifier, as seen in Fig. 3d.\n\nHowever, one would expect these representations to be correlated when the animal is performing the C2 task. This is quantified by correlating the distance to the hyperplane of the fourth classifier and the distance to the hyperplane of the third classifier, as seen in Extended Data Fig. 7a.\n\nDistance along colour and shape encoding axes during the discovery of the task\n\nTo understand how the geometry of the neural representation of the colour and shape of the stimulus evolved during task discovery (Fig. 5), we measured the distance in neural space between the two prototype colours (red and green) and the two prototype shapes (tee and bunny). Distance was measured along the encoding axis for each stimulus dimension (that is, the axis that is orthogonal to the colour and shape classifiers, described above). So, for each test trial, the distance along the colour encoding axis was:\n\n$$\\begin{array}{l}\\mathrm{colour}\\,\\mathrm{encoding}\\,\\mathrm{distance}\\\\ \\,=\\,\\mathrm{Avg}\\,\\left(\\begin{array}{l}\\mathrm{abs}(\\mathrm{encoding}(\\mathrm{red}\\,\\mathrm{bunny})-\\mathrm{encoding}(\\mathrm{green}\\,\\mathrm{bunny})),\\\\ \\mathrm{abs}(\\mathrm{encoding}(\\mathrm{red}\\,\\mathrm{tee})-\\mathrm{encoding}(\\mathrm{green}\\,\\mathrm{tee}))\\end{array}\\right)\\,;\\end{array}$$\n\nThis approach enabled us to calculate the distance along red/green colours while controlling for differences across shapes. The colour encoding distance was estimated for 250 iterations of the classifiers, enabling us to estimate the mean and standard error of the distance. A similar process was followed for estimating the shape encoding distance.\n\nTo calculate the colour and shape encoding distance during task discovery (Fig. 5a,b), we calculated the distance above in the sliding window of 45 trials, stepped 5 trials, after the switch into the C1 task during the S1–C2–C1 sequences of tasks with low initial performance.\n\nCPI\n\nCPI was defined as the log of the ratio of average colour encoding distance and average shape encoding distance described above:\n\n$$\\mathrm{CPI}=\\log \\,\\left(\\frac{\\mathrm{avg}(\\mathrm{Colour}\\,\\mathrm{encoding}\\,\\mathrm{distance})}{\\mathrm{avg}(\\mathrm{Shape}\\,\\mathrm{encoding}\\,\\mathrm{distance})}\\right)$$\n\nTo calculate the CPI for each task (Fig. 5c), we computed the shape and colour encoding distance using all trials of a given task. To measure the shape distance for all three tasks, we trained a classifier to categorize stimulus shape based on neural responses during the S1 task (limited to the last 75 trials of each block). To ensure that the classifier was responding only to shape (and not motor response), we trained the classifier on a balanced set of correct (rewarded) and incorrect (unrewarded) trials (20 trials: 5 trials for each reward condition and for each shape category). Shape encoding for each task was calculated using four trials to test the classifier (one trial for each of the four stimulus congruency types).\n\nTo measure colour distance in the S1–C2–C1 task, we trained colour category classifier on correct trials of the C1–C1–C2 task, balancing for congruent and incongruent stimuli (20 trials: 5 trials for each of the four stimulus congruency types). Colour encoding for each task was calculated using four trials of that task to test the classifier (one trial for each of the four stimulus congruency types).\n\nTo track changes in the relative strength of colour and shape information during task discovery, we calculated CPI in a sliding window of 45 trials (slid every 5 trials starting from trial 1–45 to trial 76–120) after the monkey’s switched into the C1 task (during the S1–C2–C1 sequences of tasks, with low initial performance). Note, as we controlled for motor response when calculating shape and colour distance, the CPI is not affected by motor response information65.\n\nCorrelation between task belief encoding and CPI\n\nTo correlate the CPI with task belief encoding, we used the same set of C1 test trials to calculate both CPI and estimate the task identity using the task classifier described above. CPI values in 100 ms–300 ms after stimulus onset and belief encoding values in 400 ms to 0 ms before stimulus onset were averaged for all trials in each trial window and the Mann–Kendall correlation between the resulting two vectors was calculated (Fig. 5e).\n\nQuantifying suppression of axis representation\n\nWe trained a response axis classifier to decode whether the current task axis was axis 1 or axis 2. The classifier was trained on neural data from all trials of C1 and C2 tasks with an equal number of trials from each response direction on task specific axis (for example, equal trials for UL and LR response on axis 1 for C1 task, 36 trials: 9 trials for each response direction, in each task). All correct and incorrect trials were used to train and test the classifier. To capture the response period, the classifiers were trained on the number of spikes between 200 ms and 450 ms after stimulus onset. As we were interested in measuring the difference in neural activity between axis of responses, we did not subtract the mean FR before training the classifier. For Extended Data Fig. 10d,f–i, the classifier was trained to decode response axis using S1 and C2 trials.\n\nTo create a null distribution for classifier weights, we randomly permuted the response axis labels for a given response direction (1,000 iterations).\n\nSelectivity for axis of response\n\nWe used the classifier β weights to group neurons according to their axis selectivity. Neurons with significantly negative β weights were categorized as selectively responding to axis 1. Neurons with significantly positive β weights were categorized as selectively responding to axis 2. Neurons without significant β weights were categorized as non-selective (Extended Data Fig. 10a). To determine the significance of a neuron’s classifier weight, we compared the observed β weight to a null distribution (two-tailed permutation test).\n\nTo quantify the suppression of neural activity for each category of neurons, we averaged FRs of neurons in each category during trials of the C1 task when the animal responded on axis 1 or axis 2 (Fig. 5g; Extended Data Fig. 10e shows the same analysis for the S1 task). This meant including all trials when the animal responded on axis 1 (both correct and error) and all trials when the animal responded on axis 2 (all errors). As the monkeys rarely responded on incorrect axis (Extended Data Fig. 2g), the number of trials was limited and so neurons without any error trials on axis 2 were excluded.\n\nNote that, although neurons were sorted by their activity during the animal’s response (200–450 ms after stimulus onset), we observed suppression across the entire trial (Fig. 5g). Furthermore, similar results were seen when a neuron’s axis selectivity was quantified on withheld trials (for example, in Extended Data Fig. 10d–i, axis selectivity was defined on S1 and C2 tasks and applied to C1 trials).\n\nDecoding response axis during the discovery of the task\n\nWe trained a response axis classifier to decode whether the current task axis was axis 1 or axis 2. The classifier was trained on neural data from the last 75 trials of the C1 and C2 tasks, when the monkey’s performance was high (reflecting the fact that the animals were accurately responding to the correct axis at the end of the block). The classifier was trained on an equal number of trials from each direction on the response axis (for example, equal trials for UL and LR response on axis 1 for C1 task; 36 trials: 9 trials for each response direction, in each task). Correct and incorrect trials were used to train and test the classifier. To remove the effect of stimulus processing, the classifier was trained on neural activity from the fixation period (that is, −400 to 0 ms before stimulus onset). As we were interested in measuring differences between axis of responses, we did not subtract the mean FR before training the classifier.\n\nThe task classifier was tested on trials from the beginning of blocks of the C1 task. Test trials were drawn from windows of 10 trials, slid every 3 trials, during task discovery (starting from trial 1–10 to trial 66–75). Overlapping test and train trials from the same task were removed. Classifiers were tested on pseudopopulations built from trials within each trial window (two trials: one trial for each response direction on axis one from task C1).\n\nQuantifying the angle between classifier hyperplanes\n\nTo quantify the similarity between the decision boundaries of classifiers trained on different tasks, we calculated the angle between the hyperplanes defined by their weight vectors. Each classifier produced a weight vector w i corresponding to the hyperplane that separates the data points according to the respective task. We averaged the resulting hyperplane across resampling repetitions of trials (250 iterations).\n\nThe angle \\(\\theta \\) between two average hyperplanes, defined by weight vectors w i and w j , was calculated using the cosine similarity, which is given by:\n\n$$\\cos (\\theta )=\\frac{{{\\bf{w}}}_{i}\\cdot {{\\bf{w}}}_{j}}{\\Vert {{\\bf{w}}}_{i}\\Vert \\Vert {{\\bf{w}}}_{j}\\Vert }$$\n\nThe angle θ was then obtained by taking the inverse cosine of the cosine similarity. Angles close to 0° indicate that the classifiers’ hyperplanes are nearly parallel, implying similar decision boundaries across the tasks. Conversely, angles close to 90° suggest orthogonal hyperplanes, indicating distinct decision boundaries.\n\nTo measure the angle within the C1 task, we calculated the angle between classifiers trained to classify colour category on the last 50 trials of C1 task in both the C1–C2–C1 task sequence and the S1–C2–C1 task sequence. This ensured that there were no overlapping trials between training samples. Only correct trials were used in this analysis and training and test trials were balanced for congruent and incongruent stimuli (20 trials: 5 trials for each of the four stimulus congruency types).\n\nTo measure the angle between the colour category classifier in the C1 task and the C2 task, we trained the classifier on last 50 trials of task C1 and task C2.\n\nTDR analysis\n\nThe TDR analysis requires multiple steps21. Here, we describe each step in turn.\n\nLinear regression\n\nWe used a GLM to relate the activity of each neuron (y) to the task variables at each moment of time. The full model was:\n\n$$\\begin{array}{l}y={\\beta }_{0}+{\\beta }_{1}\\times \\mathrm{stimulus}\\,\\mathrm{colour}\\,\\mathrm{morph}\\,\\mathrm{level}+{\\beta }_{2}\\\\ \\,\\times \\,\\mathrm{stimulus}\\,\\mathrm{shape}\\,\\mathrm{morph}\\,\\mathrm{level}+{\\beta }_{3}\\times \\mathrm{time}+{\\beta }_{4}\\times \\mathrm{task}\\,{\\rm{S}}1+{\\beta }_{5}\\\\ \\,\\times \\,\\mathrm{task}\\,{\\rm{C}}1+{\\beta }_{6}\\times \\mathrm{task}\\,{\\rm{C}}2+{\\beta }_{7}\\\\ \\,\\times \\,\\mathrm{motor}\\,\\mathrm{response}\\,\\mathrm{direction}\\,\\mathrm{on}\\,\\mathrm{axis}\\,1+{\\beta }_{7}\\\\ \\,\\times \\,\\mathrm{motor}\\,\\mathrm{response}\\,\\mathrm{direction}\\,\\mathrm{on}\\,\\mathrm{axis}\\,2+{\\beta }_{9}\\times \\mathrm{reward}\\end{array}$$\n\nwhere y is the FR of each neuron, z scored by subtracting the mean response from the FR at each timepoint and dividing it by the s.d. The mean and s.d. were computed across all trials and timepoints for each neuron.\n\nβ 0 , β 1 , …, β 9 are the regression coefficients corresponding to each predictor. Predictors were: stimulus colour morph level, coded as −1 for 0% morph level, −0.5 for 30% and 170% morph levels, +0.5 for 70% and 130% morph levels, and +1 for 100% morph level; stimulus shape morph level, coded as −1 for 0% morph level, −0.5 for 30% and 170% morph levels, +0.5 for 70% and 130% morph levels and +1 for 100% morph level; time, indicating the temporal progression within a recording session, normalized between 0 and 1 (continuous variable); task identity, indicating the identity of the task (categorical variable: S1, C1, C2); motor response direction on axis 1, indicating the direction of the motor response (categorical variable; −1 for UL, +1 for LR); motor response direction on axis 2, indicating the direction of the motor response (categorical variable; −1 for LL, +1 for UR); reward, indicating whether a reward was received following the response (binary variable; 1 for reward, 0 for no reward).\n\nThe GLM coefficients (β) were estimated using maximum-likelihood estimation as implemented by MATLAB’s lassoglm.m function. Independent models were fit for each timepoint. To address potential overfitting, we applied Lasso (L 1 ) regularization to the GLM weights with regularization coefficients (lambda) values of 0.0015. Correct and incorrect trials were used in fitting the model.\n\nPopulation average responses\n\nWe estimated the average response of the neural population to each task variable. To calculate the average neural response to each colour and shape morph level, we separately averaged trials for within each morph level (0, 30, 70, 100, 130, 170) for each task. To calculate the average neural response for each response direction, we separately averaged trials for within each response direction (UL, UR, LL or LR) for each task. For all task variables, we smoothed the resulting response in time with a 150 ms Boxcar. Finally, we z-scored the smoothed average for a given unit by subtracting the mean across times and conditions, and by dividing the result by the corresponding s.d.\n\nPCA\n\nWe used principal component analysis (PCA) (as implemented in MATLAB, pca.m function) to find the dimensions in the state space that captured most of the variance of the population average response and to mitigate the effect of noise. We concatenated the average response across conditions to build matrix X of size (N condition × T) × N unit , where N condition is the total number of conditions and T is the number of time samples. We used the first N PCA that captured 90% of the explained variance to define a de-noising matrix D of size N unit × N unit .\n\nRegression subspace\n\nWe used the regression coefficients described above to identify dimensions in the state space containing task related variance. We used four task variables to define this space: colour morph level, shape morph level, response direction on axis 1 and response direction on axis 2. For each of these task variables, we built coefficient vectors \\({\\beta }_{v,t}(i)\\) corresponding to regression coefficient for task variable v, time t and unit i. We then denoised each variable vector by projecting it to the subspace spanned by N PCA principal components from the population average defined above.\n\n$${\\beta }_{v,t}^{{\\rm{PCA}}}=D{\\beta }_{v,t}$$\n\nWe computed \\({t}_{v}^{\\max }\\) for each task variable where norm of \\({\\beta }_{v,t}^{\\mathrm{PCA}}\\) matrix had its maximum value. This defined the time-independent, denoised regression vectors.\n\n$${\\beta }_{v}^{\\max }={\\beta }_{v,{t}_{v}^{\\max }}^{\\mathrm{PCA}}$$\n\nWith \\({t}_{v}^{\\max }={\\mathrm{argmax}}_{t}\\parallel {\\beta }_{v,t}^{\\mathrm{PCA}}\\parallel \\). To compute orthogonal axes of colour, shape and response direction on axis 1 and response direction on axis 2, we orthogonalized the regression vectors \\({{\\boldsymbol{\\beta }}}_{v}^{\\max }\\) using QR decomposition.\n\n$${B}^{\\max }=QR$$\n\nWhere the first four columns of Q are orthogonalized regression vectors \\({{\\boldsymbol{\\beta }}}_{v}^{\\perp }\\) of four task variables that comprise the regression subspace.\n\nTo then estimate the representation of task-related variables in the neural population, we projected the average population response (described above) for each colour morph level, shape morph level, and response direction onto these orthogonal axes.\n\n$${p}_{v,c}={{{\\boldsymbol{\\beta }}}_{v}^{\\perp }}^{T}{X}_{c}$$\n\nWhere p v,c is the time series with length T for each morph level in a specific task.\n\nNote that because TDR orthogonalizes task features, it can control for motor response when estimating the neural response to colour and shape.\n\nCPI using TDR\n\nTo measure the distance between the two prototype colours (red and green) and the two prototype shapes (tee and bunny). We first projected the average PSTH for each protype object onto the orthogonal encoding axes of colour and shape.\n\n$${p}_{\\mathrm{colour},\\mathrm{protoype}}={{{\\boldsymbol{\\beta }}}_{\\mathrm{colour}}^{\\perp }}^{T}{X}_{\\mathrm{prototype}}$$\n\n$${p}_{\\mathrm{shape},\\mathrm{protoype}}={{{\\boldsymbol{\\beta }}}_{\\mathrm{shape}}^{\\perp }}^{T}{X}_{\\mathrm{prototype}}$$\n\nDistance was measured along the orthogonal encoding axis of shape and colour using projected responses:\n\n$$\\mathrm{colour}\\,\\mathrm{encoding}\\,\\mathrm{distance}=\\mathrm{Avg}\\,\\left(\\begin{array}{c}\\mathrm{abs}\\left({p}_{\\mathrm{colour},\\text{red bunny}}-{p}_{\\mathrm{colour},\\text{green bunny}}\\right),\\\\ \\mathrm{abs}\\left({p}_{\\mathrm{colour},\\text{red tee}}-{p}_{\\mathrm{colour},\\text{green tee}}\\right)\\end{array}\\right)$$\n\n$$\\mathrm{shape}\\,\\mathrm{encoding}\\,\\mathrm{distance}=\\mathrm{Avg}\\left(\\begin{array}{c}\\mathrm{abs}\\left({p}_{\\mathrm{shape},\\text{red bunny}}-{p}_{\\mathrm{shape},\\text{red tee}}\\right),\\\\ \\mathrm{abs}\\left({p}_{\\mathrm{shape},\\text{green bunny}}-{p}_{\\mathrm{shape},\\text{green tee}}\\right)\\end{array}\\right)$$\n\nCPI was defined as the log of the ratio of the average colour encoding distance and average shape encoding distance as described above:\n\n$$\\mathrm{CPI}=\\log \\,\\left(\\frac{\\mathrm{avg}(\\text{colour encoding distance})}{\\mathrm{avg}(\\mathrm{shape\\; encoding\\; distance})}\\right)$$\n\nExtended Data Fig. 9f reports the average CPI for 100 ms to 300 ms since stimulus onset.\n\nQuantifying the angle between colour category and response direction subspace across tasks using TDR\n\nTo quantify the angle between colour category subspace we used orthogonal task variable vectors defined by TDR. Similar to fitting a regression model for TDR, we first fit a regression model that included separate colour, shape and response axis weights for each task.\n\n$$y={\\beta }_{0}+{\\beta }_{1}\\times {\\rm{C}}1\\,\\mathrm{task}\\,\\mathrm{stimulus}\\,\\mathrm{colour}\\,\\mathrm{morph}\\,\\mathrm{level}+{\\beta }_{2}\\times {\\rm{C}}2\\,\\mathrm{task}\\,\\mathrm{stimulus}\\,\\mathrm{colour}\\,\\mathrm{morph}\\,\\mathrm{level}+{\\beta }_{3}\\times {\\rm{S}}1\\,\\mathrm{task}\\,\\mathrm{stimulus}\\,\\mathrm{colour}\\,\\mathrm{morph}\\,\\mathrm{level}+{\\beta }_{4}\\times {\\rm{C}}1\\,\\mathrm{task}\\,\\mathrm{stimulus}\\,\\mathrm{shape}\\,\\mathrm{morph}\\,\\mathrm{level}+{\\beta }_{5}\\times {\\rm{C}}2\\,\\mathrm{task}\\,\\mathrm{stimulus}\\,\\mathrm{shape}\\,\\mathrm{morph}\\,\\mathrm{level}+{\\beta }_{6}\\times {\\rm{S}}1\\,\\mathrm{task}\\,\\mathrm{stimulus}\\,\\mathrm{shape}\\,\\mathrm{morph}\\,\\mathrm{level}+{\\beta }_{8}\\times {\\rm{S}}1\\,\\mathrm{task}\\,\\mathrm{motor}\\,\\mathrm{response}\\,\\text{direction on}\\,\\mathrm{axis}\\,1+{\\beta }_{9}\\times {\\rm{C}}1\\,\\mathrm{task}\\,\\mathrm{motor}\\,\\mathrm{response}\\,\\text{direction on}\\,\\mathrm{axis}\\,1+{\\beta }_{10}\\times {\\rm{C}}2\\,\\mathrm{task}\\,\\mathrm{motor}\\,\\mathrm{response}\\,\\text{direction on}\\,\\mathrm{axis}\\,2+{\\beta }_{11}\\times \\mathrm{time}+{\\beta }_{12}\\times \\mathrm{task}\\,{\\rm{S}}1+{\\beta }_{13}\\times \\mathrm{task}\\,{\\rm{C}}1+{\\beta }_{14}\\times \\mathrm{task}\\,{\\rm{C}}2+{\\beta }_{15}\\times \\mathrm{reward}.$$\n\nModels were fit on 200 resamples, each randomly drawing 75% of all trials for each individual task. For ‘from switch’ conditions, the first 50 trials after switch to each task were used and, for ‘to switch’ conditions, the last 50 trials in the blocks were used. Similar to TDR method described above, we used QR decomposition to find the orthogonal axes encoding each task variable, but now specific to each task (colour morph level, shape morph level, task identity, reward and task specific axis of response):\n\n$${B}^{\\max ,{\\rm{C}}1\\mathrm{task}}={Q}_{{\\rm{C}}1}{R}_{{\\rm{C}}1},{B}^{\\max ,{\\rm{C}}2\\mathrm{task}}={Q}_{{\\rm{C}}2}{R}_{{\\rm{C}}2},{B}^{\\max ,{\\rm{S}}1\\mathrm{task}}={Q}_{{\\rm{S}}1}{R}_{{\\rm{S}}1}$$\n\nFor each resampling, the angle between orthogonal axis of colour of pairs of tasks (C1–C2, C1–S1 and C2–S1) was measured as\n\n$$\\cos (\\theta )=\\frac{{{\\boldsymbol{\\beta }}}_{\\mathrm{colour},\\mathrm{task}\\,i}^{\\perp }\\cdot {{\\boldsymbol{\\beta }}}_{\\mathrm{colour},\\mathrm{task}\\,j}^{\\perp }}{\\parallel {{\\boldsymbol{\\beta }}}_{\\mathrm{colour},\\mathrm{task}\\,i}^{\\perp }\\parallel \\parallel {{\\boldsymbol{\\beta }}}_{\\mathrm{colour},\\mathrm{task}\\,j}^{\\perp }\\parallel }$$\n\nWhere i and j are task identity of the tasks. Within-task angles (Extended Data Fig. 9c,d) were computed by randomly taking 200 pairs of resampling regression coefficient repetitions, and finding the orthogonal axes explained above. All angles were wrapped to 90 deg. This was because the process of orthogonalization with QR decomposition might result in vectors that are mathematically equivalent but with a flipped sign compared to the input vectors owing to numerical choices or conventions.\n\nWe used a permutation test to assess whether the angle between pairs of tasks was significantly larger than expected by chance. The observed angle was estimated by fitting the regression model explained above to all trials. To compute a null distribution for angles, we generated 1,000 permuted datasets by randomly permuting the predictor values relative to the neural activity and refitting the model. For each permuted dataset, we computed the angle between task pairs as described above. The likelihood of the observed angle was then estimated by computing the proportion of permuted datasets that yielded an angle smaller than or equal to that of the observed angle.\n\nQuantifying whether task representations transfer across blocks\n\nTo test whether the task representation was maintained across blocks (Fig. 4b), we trained a classifier to discriminate between C1 and S1 trials using the last 50 trials of the block. We then tested this classifier in three ways. First, to quantify the information about the identity of C1 versus S1 task, we tested this classifier on withheld trials from last 50 trials of C1 and S1 tasks (Fig. 4b (1)). Second, to quantify how much the representation of C1 and S1 tasks transferred to the C2 task, we tested whether the classifier could discriminate between C1 and S1 tasks during the first 50 trials of the C2 task (that is, comparing C1–C2 versus S1–C2). Third, to quantify how much the representation of C1 and S1 tasks was transferred to C1 or S1 in the next block, we tested the classifier on the first 50 trials after the switch in C1 and S1 tasks.\n\nFor all training and testing, the total number of spikes in the period −400 to 0 ms from stimulus onset for each trial were used to build a pseudopopulation. Only correct trials were included in this analysis, and the training and test trials were balanced for congruent and incongruent stimuli (36 trials: 9 trials for each of the four stimulus congruency types). To compute a null distribution, we generated 1,000 permuted datasets by randomly permuting all task identity values relative to the neural activity and refitting the classifier.\n\nReporting summary\n\nFurther information on research design is available in the Nature Portfolio Reporting Summary linked to this article.",
      "url": "https://www.nature.com/articles/s41586-025-09805-2",
      "source": "Nature",
      "published": "2025-11-28",
      "sentiment_score": 0.85,
      "reasoning": "The article reports a significant neuroscience research breakthrough demonstrating how the brain flexibly performs multiple tasks by compositional combination of neural representations. It involves detailed experimental evidence from primate studies, advanced neural recording, and sophisticated data analysis techniques. This research advances understanding of brain function and neural computation with broad implications for neuroscience, artificial intelligence, and cognitive science. The impact is meaningful for the general public as it can inform future technologies and treatments related to brain function and disorders. The article is focused and provides substantial context and methodological detail.",
      "category": "Technology",
      "personality_title": "Scientists uncover how the brain combines neural signals to perform different tasks",
      "personality_presentation": "**Context** – The brain can do many tasks by using different parts of itself in flexible ways. Scientists want to understand how the brain combines information to switch between tasks smoothly.\n\n**What happened** – Researchers studied two rhesus monkeys trained to perform tasks that required recognizing shapes and colors. The monkeys looked at pictures that blended colors and shapes and responded by moving their eyes to specific spots. Scientists recorded brain activity from several areas while the monkeys performed these tasks. They used advanced methods to analyze how neurons responded to different parts of the task, like color, shape, and movement.\n\n**Impact** – This study shows that the brain uses shared groups of neurons to represent different task elements. Instead of having separate neurons for each task, the brain combines neural signals in a way that can handle multiple tasks by mixing and matching. This finding helps explain how the brain is so flexible and efficient. It also offers clues for creating smarter artificial intelligence systems that can learn and switch between tasks like humans.\n\n**What's next step** – Future research can explore how these neural combinations happen in other brain regions and in humans. Understanding this could lead to better treatments for brain disorders where task switching is difficult. It may also inspire new designs in AI that mimic the brain’s way of composing tasks from shared building blocks.\n\n**One-sentence takeaway** – By studying monkeys, scientists discovered that the brain flexibly performs different tasks by combining shared neural signals, revealing a key feature of how the brain works.\n",
      "personality_title_fr": "Les scientifiques découvrent comment le cerveau combine les signaux neuronaux pour réaliser différentes tâches",
      "personality_presentation_fr": "**Contexte** – Le cerveau peut effectuer de nombreuses tâches en utilisant différentes parties de lui-même de manière flexible. Les scientifiques cherchent à comprendre comment le cerveau combine les informations pour passer facilement d'une tâche à une autre.\n\n**Ce qui s’est passé** – Des chercheurs ont étudié deux singes rhésus entraînés à effectuer des tâches demandant de reconnaître des formes et des couleurs. Les singes regardaient des images mélangeant couleurs et formes et répondaient en déplaçant leurs yeux vers des endroits précis. Les scientifiques ont enregistré l'activité cérébrale dans plusieurs zones pendant que les singes accomplissaient ces tâches. Ils ont utilisé des méthodes avancées pour analyser comment les neurones réagissaient aux différentes parties de la tâche, comme la couleur, la forme et le mouvement.\n\n**Impact** – Cette étude montre que le cerveau utilise des groupes de neurones partagés pour représenter différents éléments de la tâche. Au lieu d'avoir des neurones séparés pour chaque tâche, le cerveau combine les signaux neuronaux d'une manière qui peut gérer plusieurs tâches en les mélangeant. Cette découverte aide à expliquer pourquoi le cerveau est si flexible et efficace. Elle offre aussi des pistes pour créer des systèmes d'intelligence artificielle plus intelligents capables d'apprendre et de changer de tâche comme les humains.\n\n**Étape suivante** – Les recherches futures peuvent explorer comment ces combinaisons neuronales se produisent dans d'autres régions du cerveau et chez l'humain. Comprendre cela pourrait mener à de meilleurs traitements pour les troubles du cerveau où le changement de tâche est difficile. Cela pourrait aussi inspirer de nouvelles conceptions en IA qui imitent la façon dont le cerveau compose les tâches à partir d'éléments communs.\n\n**Phrase clé** – En étudiant des singes, les scientifiques ont découvert que le cerveau réalise différentes tâches en combinant des signaux neuronaux partagés, révélant une caractéristique clé du fonctionnement cérébral.\n",
      "personality_title_es": "Científicos descubren cómo el cerebro combina señales neuronales para realizar diferentes tareas",
      "personality_presentation_es": "**Contexto** – El cerebro puede hacer muchas tareas usando distintas partes de sí mismo de forma flexible. Los científicos quieren entender cómo el cerebro combina información para cambiar entre tareas sin problemas.\n\n**Qué pasó** – Investigadores estudiaron a dos monos rhesus entrenados para realizar tareas que requerían reconocer formas y colores. Los monos miraban imágenes que mezclaban colores y formas y respondían moviendo sus ojos a lugares específicos. Los científicos registraron la actividad cerebral en varias áreas mientras los monos realizaban estas tareas. Usaron métodos avanzados para analizar cómo respondían las neuronas a diferentes partes de la tarea, como color, forma y movimiento.\n\n**Impacto** – Este estudio muestra que el cerebro usa grupos compartidos de neuronas para representar diferentes elementos de la tarea. En lugar de tener neuronas separadas para cada tarea, el cerebro combina señales neuronales de manera que puede manejar múltiples tareas mezclándolas. Este hallazgo ayuda a explicar cómo el cerebro es tan flexible y eficiente. También ofrece pistas para crear sistemas de inteligencia artificial más inteligentes que puedan aprender y cambiar entre tareas como los humanos.\n\n**Próximo paso** – La investigación futura puede explorar cómo ocurren estas combinaciones neuronales en otras regiones del cerebro y en humanos. Entender esto podría conducir a mejores tratamientos para trastornos cerebrales donde cambiar de tarea es difícil. También podría inspirar nuevos diseños en IA que imiten la forma en que el cerebro compone tareas a partir de bloques compartidos.\n\n**Frase clave** – Estudiando monos, los científicos descubrieron que el cerebro realiza diferentes tareas combinando señales neuronales compartidas, revelando una característica clave del funcionamiento cerebral.\n",
      "image_url": "public/images/news_image_Building-compositional-tasks-with-shared-neural-su.png",
      "image_prompt": "Two attentive rhesus macaques symbolized as elegant, stylized silhouettes facing a softly glowing, abstract neural network composed of interwoven, flowing lines and nodes, representing shared neural subspaces; the scene is rendered as a warm, detailed painting with gentle earth tones and muted natural colors, evoking a calm, focused atmosphere of cognitive connection and discovery."
    },
    {
      "title": "Physicists have worked out a universal law for how objects shatter",
      "summary": "Whether it is a cube of sugar or a chunk of a mineral, a mathematical analysis can identify how many fragments of each size any brittle object will break into",
      "content": "Whether it is a cube of sugar or a chunk of a mineral, a mathematical analysis can identify how many fragments of each size any brittle object will break into\n\nHow many pieces will a dropped vase shatter into? Imaginechina Limited / Alamy\n\nA dropped plate, a smashed sugar cube and a broken drinking glass all seem to follow the same law of physics when it comes to how many fragments of a given size they will shatter into.\n\nFor several decades, researchers have known that there is something universal about the process of fragmentation, when an object breaks into many parts when dropped or smashed. If you counted how many fragments existed at each possible size and made a graph of that distribution, it would have the same shape regardless of the object that shattered. Emmanuel Villermaux at Aix-Marseille University in France has now derived an equation that explains that shape, effectively formulating a universal law for how objects break.\n\nInstead of focusing on the details of how cracks appear in an object before it fragments, he took a more zoomed-out approach. Villermaux considered all possible sets of fragments that an object can shatter into. Some sets would include highly specific outcomes, like a vase shattering into four equal pieces. He picked out the most probable set, the one with the highest entropy, which captured breakages that were messy and irregular. This is similar to the way many laws concerning large ensembles of particles were derived in the 19th century, he says. Additionally, Villermaux used a law of physics that describes changes in the total density of fragments when the object is shattering, which he and his colleagues had previously found.\n\nTogether, these two ingredients let him derive a simple equation predicting how many fragments of each size a breaking object should produce. To see how well it worked, Villermaux compared it with a whole slew of past experiments with shattering glass bars, dry spaghetti, plates, ceramic tubes and even plastic fragments in the ocean and waves breaking on choppy seas. Across the board, the way fragmentation showed up in each of these scenarios followed his new law, capturing the ubiquitous graph shape that researchers had seen before.\n\nHe also carried out a set of experiments where he shattered a sugar cube by dropping an object onto it from various heights. “That was a summer project with my daughters. I did this a long time ago when my children were still young and then came back to the data, because they were illustrating my point well,” says Villermaux. The equation doesn’t work in cases where there is no randomness and the fragmentation process is too regular, for example when a jet of liquid breaks up into many droplets of uniform size following deterministic laws of fluid physics, and in some cases where fragments interact with each other during shattering, he says.\n\nSubscriber-only newsletter Sign up to Lost in Space-Time Untangle mind-bending physics, maths and the weirdness of reality with our monthly, special-guest-written newsletter. Sign up to newsletter\n\nFerenc Kun at the University of Debrecen in Hungary says that because the graph shape that Villermaux’s analysis explained is so ubiquitous, it isn’t surprising that it stems from a bigger principle. At the same time, it is amazing how broadly it works and how it can be amended in some cases where there are additional constraints, such as in plastic where cracks can sometimes “heal”, he says.\n\nFragmentation isn’t just an interesting physics problem. Understanding it better could have real implications for how energy is spent on shattering ore in industrial mining, for example, or how we prepare for rockfalls that are increasingly happening in mountainous regions as global temperatures rise, says Kun.\n\nGoing forward, Kun says it may be interesting to consider the distribution of not just the sizes of fragments, but also their shapes. Additionally, it is an open question to determine what the smallest possible size of a fragment could be, says Villermaux.",
      "url": "https://www.newscientist.com/article/2505669-physicists-have-worked-out-a-universal-law-for-how-objects-shatter/?utm_campaign=RSS%7CNSNS&utm_source=NSNS&utm_medium=RSS&utm_content=home",
      "source": "New Scientist - Home",
      "published": "2025-11-27",
      "sentiment_score": 0.8,
      "reasoning": "The article reports a significant scientific breakthrough: a universal law mathematically describing how brittle objects shatter into fragments. This discovery has broad implications across multiple fields, including industrial mining and environmental safety related to rockfalls. The story is focused, detailed, and highlights real-world applications of the research, making it inspiring and impactful for society.",
      "category": "Technology",
      "personality_title": "Scientists discover universal law explaining how objects break apart",
      "personality_presentation": "**Context** – When brittle objects like glass, sugar cubes, or minerals break, they shatter into many pieces of different sizes. For years, scientists noticed that the pattern of these pieces was similar across many materials but did not fully understand why.\n\n**What happened** – Emmanuel Villermaux, a physicist at Aix-Marseille University in France, created a new mathematical equation that predicts how many fragments of each size will form when a brittle object breaks. He used a unique approach that looks at all possible ways an object can break and picked the most likely pattern. Villermaux tested his equation with many experiments, including smashing sugar cubes and breaking glass bars, and found it matched the real results well.\n\n**Impact** – This discovery is important because it explains a natural pattern seen in many breaking objects with one simple law. It helps scientists understand fragmentation better and can improve practical tasks like mining, where breaking rocks efficiently saves energy, or predicting rockfalls in mountains, which is important for safety as the climate changes.\n\n**What’s next step** – Researchers want to explore not only the size but also the shapes of fragments after breaking. They also aim to find out what the smallest fragment size can be. These questions will deepen our understanding of how materials break and could lead to new applications.\n\n**One-sentence takeaway** – A new universal equation now explains how brittle objects shatter into pieces, helping science and industry understand and use this process better.",
      "personality_title_fr": "Des scientifiques découvrent une loi universelle expliquant comment les objets se brisent",
      "personality_presentation_fr": "**Contexte** – Lorsque des objets fragiles comme le verre, les cubes de sucre ou les minéraux se cassent, ils se brisent en de nombreux morceaux de tailles différentes. Depuis longtemps, les scientifiques ont remarqué que le motif de ces morceaux était similaire dans de nombreux matériaux, mais ils ne comprenaient pas complètement pourquoi.\n\n**Ce qui s’est passé** – Emmanuel Villermaux, physicien à l’Université Aix-Marseille en France, a créé une nouvelle équation mathématique qui prédit combien de fragments de chaque taille se formeront lorsqu’un objet fragile se casse. Il a utilisé une approche unique qui considère toutes les façons possibles dont un objet peut se briser et a choisi le motif le plus probable. Villermaux a testé son équation avec de nombreuses expériences, notamment en cassant des cubes de sucre et des barres de verre, et a constaté qu’elle correspondait bien aux résultats réels.\n\n**Impact** – Cette découverte est importante car elle explique un motif naturel observé dans de nombreux objets cassés avec une seule loi simple. Elle aide les scientifiques à mieux comprendre la fragmentation et peut améliorer des tâches pratiques comme l’extraction minière, où casser les roches efficacement économise de l’énergie, ou la prédiction des chutes de pierres en montagne, ce qui est important pour la sécurité face aux changements climatiques.\n\n**Prochaine étape** – Les chercheurs veulent explorer non seulement la taille, mais aussi la forme des fragments après la casse. Ils cherchent aussi à déterminer quelle peut être la plus petite taille possible d’un fragment. Ces questions approfondiront notre compréhension de la façon dont les matériaux se brisent et pourraient mener à de nouvelles applications.\n\n**Résumé en une phrase** – Une nouvelle équation universelle explique désormais comment les objets fragiles se brisent en morceaux, aidant la science et l’industrie à mieux comprendre et utiliser ce processus.",
      "personality_title_es": "Científicos descubren una ley universal que explica cómo se rompen los objetos",
      "personality_presentation_es": "**Contexto** – Cuando objetos frágiles como el vidrio, cubos de azúcar o minerales se rompen, se fragmentan en muchos pedazos de diferentes tamaños. Durante años, los científicos notaron que el patrón de estos pedazos era similar en muchos materiales, pero no entendían completamente por qué.\n\n**Qué pasó** – Emmanuel Villermaux, físico de la Universidad Aix-Marsella en Francia, creó una nueva ecuación matemática que predice cuántos fragmentos de cada tamaño se formarán cuando un objeto frágil se rompa. Usó un enfoque único que considera todas las formas posibles en que un objeto puede romperse y escogió el patrón más probable. Villermaux probó su ecuación con muchos experimentos, incluyendo romper cubos de azúcar y barras de vidrio, y encontró que coincidía bien con los resultados reales.\n\n**Impacto** – Este descubrimiento es importante porque explica un patrón natural visto en muchos objetos rotos con una sola ley simple. Ayuda a los científicos a entender mejor la fragmentación y puede mejorar tareas prácticas como la minería, donde romper rocas eficientemente ahorra energía, o predecir deslizamientos de rocas en montañas, algo importante para la seguridad debido al cambio climático.\n\n**Próximo paso** – Los investigadores quieren estudiar no solo el tamaño, sino también la forma de los fragmentos después de romperse. También buscan descubrir cuál puede ser el tamaño más pequeño posible de un fragmento. Estas preguntas profundizarán nuestro entendimiento de cómo se rompen los materiales y podrían llevar a nuevas aplicaciones.\n\n**Conclusión en una frase** – Una nueva ecuación universal ahora explica cómo los objetos frágiles se rompen en pedazos, ayudando a la ciencia y la industria a entender y usar mejor este proceso.",
      "image_url": "public/images/news_image_Physicists-have-worked-out-a-universal-law-for-how.png",
      "image_prompt": "A warm, detailed painting of various shattered objects—a sugar cube, a broken ceramic plate, and fragments of glass—scattered gently on a soft, neutral-toned surface, with delicate lines and subtle shading illustrating the diverse sizes and irregular shapes of the fragments, symbolizing the universal pattern of fragmentation discovered by physicists."
    }
  ]
}