{
  "personality": null,
  "timestamp": "2025-10-14T04:36:47.023704",
  "category": "Technology",
  "news_summary": "Today's technology news highlights breakthroughs in AI recovering lost scientific knowledge, advancements in quantum computing through modernizing classical equations, and the evolving impact of AI on bioinformatics careers.",
  "news_summary_fr": "L'actualité technologique d'aujourd'hui met en lumière les percées de l'IA dans la récupération de connaissances scientifiques perdues, les progrès de l'informatique quantique grâce à la modernisation des équations classiques et l'évolution de l'impact de l'IA sur les carrières en bioinformatique.",
  "news_summary_es": "Las noticias tecnológicas de hoy destacan los avances de la IA en la recuperación de conocimientos científicos perdidos, los progresos de la computación cuántica mediante la modernización de las ecuaciones clásicas y la evolución del impacto de la IA en las carreras de bioinformática.",
  "articles": [
    {
      "title": "90% of science is lost. This new AI just found it",
      "summary": "Vast amounts of valuable research data remain unused, trapped in labs or lost to time. Frontiers aims to change that with FAIR² Data Management, a groundbreaking AI-driven system that makes datasets reusable, verifiable, and citable. By uniting curation, compliance, peer review, and interactive visualization in one platform, FAIR² empowers scientists to share their work responsibly and gain recognition.",
      "content": "Most scientific data never reach their full potential to drive new discoveries.\n\nOut of every 100 datasets produced, about 80 stay within the lab, 20 are shared but seldom reused, fewer than two meet FAIR standards, and only one typically leads to new findings.\n\nThe consequences are significant: slower progress in cancer treatment, climate models that lack sufficient evidence, and studies that cannot be replicated.\n\nTo change this, the open-science publisher Frontiers has introduced Frontiers FAIR² Data Management, described as the world's first comprehensive, AI-powered research data service. It is designed to make data both reusable and properly credited by combining all essential steps -- curation, compliance checks, AI-ready formatting, peer review, an interactive portal, certification, and permanent hosting -- into one seamless process. The goal is to ensure that today's research investments translate into faster advances in health, sustainability, and technology.\n\nFAIR² builds on the FAIR principles (Findable, Accessible, Interoperable and Reusable) with an expanded open framework that guarantees every dataset is AI-compatible and ethically reusable by both humans and machines. The FAIR² Data Management system is the first working implementation of this model, arriving at a moment when research output is growing rapidly and artificial intelligence is reshaping how discoveries are made. It turns high-level principles into real, scalable infrastructure with measurable impact.\n\nDr. Kamila Markram, co-founder and CEO of Frontiers, explains:\n\n\"Ninety percent of science vanishes into the void. With Frontiers FAIR² Data Management, no dataset and no discovery need ever be lost again -- every contribution can now fuel progress, earn the credit it deserves, and unleash science.\"\n\nAI at the Core\n\nWork that once required months of manual effort -- from organizing and verifying datasets to generating metadata and publishable outputs -- is now completed in minutes by the AI Data Steward, powered by Senscience, the Frontiers venture behind FAIR².\n\nResearchers who submit their data receive four integrated outputs: a certified Data Package, a peer-reviewed and citable Data Article, an Interactive Data Portal featuring visualizations and AI chat, and a FAIR² Certificate. Each element includes quality controls and clear summaries that make the data easier to understand for general users and more compatible across research disciplines.\n\nTogether, these outputs ensure that every dataset is preserved, validated, citable, and reusable, helping accelerate discovery while giving researchers proper recognition. Frontiers FAIR² also enhances visibility and accessibility, supporting responsible reuse by scientists, policymakers, practitioners, communities, and even AI systems, allowing society to extract greater value from its investment in science.\n\nFlagship Pilot Datasets\n\nSARS-CoV-2 Variant Properties -- Covering 3,800 spike protein variants, this dataset links structural predictions from AlphaFold2 and ESMFold with ACE2 binding and expression data. It offers a powerful resource for pandemic preparedness, enabling deeper understanding of variant behavior and fitness.\n\nPreclinical Brain Injury MRI -- A harmonized dataset of 343 diffusion MRI scans from four research centers, standardized across protocols and aligned for comparability. It supports reproducible biomarker discovery, robust cross-site analysis, and advances in preclinical traumatic brain injury research.\n\nEnvironmental Pressure Indicators (1990-2050) -- Combining observed data and modeled forecasts across 43 countries over six decades, this dataset tracks emissions, waste, population, and GDP. It underpins sustainability benchmarking and evidence-based climate policy planning.\n\nIndo-Pacific Atoll Biodiversity -- Spanning 280 atolls across five regions, this dataset integrates biodiversity records, reef habitats, climate indicators, and human-use histories. It provides an unprecedented basis for ecological modeling, conservation prioritization, and cross-regional research on vulnerable island ecosystems.\n\nResearchers testing the pilots noted that Frontiers FAIR² not only preserves and shares data but also builds confidence in its reuse -- through quality checks, clear summaries for non-specialists, and the reliability to combine datasets across disciplines, all while ensuring scientists receive credit.\n\nAll pilot datasets comply with the FAIR² Open Specification, making them responsibly curated, reusable, and trusted for long-term human and machine use so today's data can accelerate tomorrow's solutions to society's most pressing challenges.\n\nRecognition and Reuse\n\nEach reuse multiplies the value of the original dataset, ensuring that no discovery is wasted, every contribution can spark the next breakthrough, and researchers gain recognition for their work.\n\nDr. Sean Hill, co-founder and CEO of Senscience, the Frontiers AI venture behind FAIR² Data Management, notes:\n\n\"Science invests billions generating data, but most of it is lost -- and researchers rarely get credit. With Frontiers FAIR², every dataset is cited, every scientist recognized -- finally rewarding the essential work of data creation. That's how cures, climate solutions, and new technologies will reach society faster -- this is how we unleash science.\"\n\nWhat Researchers Are Saying\n\nDr. Ángel Borja, Principal Researcher, AZTI, Marine Research, Basque Research and Technology Alliance (BRTA):\n\n\"I highly [recommend using] this kind of data curation and publication of articles, because you can generate information very quickly and it's useful formatting for any end users.\"\n\nErik Schultes, Senior Researcher, Leiden Academic Centre for Drug Research (LACDR); FAIR Implementation Lead, GO FAIR Foundation:\n\n\"Frontiers FAIR² captured the scientific aspects of the project perfectly.\"\n\nFemke Heddema, Researcher and Health Data Systems Innovation Manager, PharmAccess:\n\n\"Frontiers FAIR² makes the execution of FAIR principles smoother for researchers and digital health implementers, proving that making datasets like MomCare reusable doesn't have to be complex. By enabling transparent, accessible, and actionable data, Frontiers FAIR² opens the door to new opportunities in health research.\"\n\nDr. Neil Harris, Professor in Residence, Department of Neurosurgery, Brain Injury Research Center, University of California, Los Angeles (UCLA):\n\n\"Implementation of [Frontiers] FAIR² can provide an objective check on data for both missingness and quality that is useful on so many levels. These types of unbiased assessments and data summaries can aid understanding by non-domain experts to ultimately enhance data sharing. As the field progresses to using big data in more disparate sub-disciplines, these data checks and summaries will become crucial to maintaining a good grasp of how we might use and combine the multitude of already acquired data within our current analyses.\"\n\nMaryann Martone, Chief Editor, Open Data Commons:\n\n\"[Frontiers] FAIR² is one of the easiest and most effective ways to make data FAIR. Every PI wants their data to be findable, accessible, comparable, and reusable -- in the lab, with collaborators, and across the scientific community. The real bottleneck has always been the time and effort required. [Frontiers] FAIR² dramatically lowers that barrier, putting truly FAIR data within reach for most labs.\"\n\nDr. Vincent Woon Kok Sin, Assistant Professor, Carbon Neutrality and Climate Change Thrust, Society Hub, The Hong Kong University of Science and Technology (HKUST):\n\n\"[Frontiers] FAIR² makes our global waste dataset more visible and accessible, helping researchers worldwide who often struggle with scarce and fragmented data. I hope this will broaden collaboration and accelerate insights for sustainable waste management.\"\n\nDr. Sebastian Steibl, Postdoctoral Researcher, Naturalis Biodiversity Center and the University of Auckland:\n\n\"True data accessibility goes beyond just uploading datasheets to a repository. It means making data easy to view, explore, and understand without necessarily requiring years of training. The [Frontiers] FAIR² platform, with an AI chatbot and interactive visual data exploration and summary tools, makes our biodiversity and environmental data broadly accessible and usable not just to scholars, but also practitioners, policymakers, and local community initiatives.\"",
      "url": "https://www.sciencedaily.com/releases/2025/10/251013040314.htm",
      "source": "Latest Science News -- ScienceDaily",
      "published": "2025-10-13",
      "sentiment_score": 0.95,
      "reasoning": "The article describes a significant AI-driven innovation that addresses a major problem in scientific research: the loss and underutilization of valuable datasets. By implementing the FAIR² Data Management system, it enables broad, responsible reuse of research data across disciplines, accelerating progress in health, environment, and technology. The story has broad societal impact, clear substance, and detailed context about the technology and its benefits.",
      "category": "Technology",
      "personality_title": "New AI system helps save and share 90% of lost scientific data",
      "personality_presentation": "**Context** – Most scientific research data never gets fully used. About 80% of datasets stay locked in labs, and only a tiny fraction meet standards that make them easy to find and reuse. This slows down progress in areas like medicine and climate science.\n\n**What happened** – Frontiers, an open-science publisher, launched Frontiers FAIR² Data Management. This is the first all-in-one AI-powered system that organizes, checks, reviews, and shares research data in a way that makes it reusable and easy to understand. It creates certified data packages, peer-reviewed articles, interactive portals, and certificates for every dataset.\n\n**Impact** – This system turns complicated science data into useful resources for researchers, policymakers, and even AI systems. It helps scientists get credit for their work and makes it easier to combine data from different fields. Pilot projects include data on COVID-19 variants, brain injury scans, climate indicators, and island biodiversity, showing how FAIR² supports important research.\n\n**What's next step** – As more scientists use FAIR², more datasets will become accessible and trustworthy. This could speed up discoveries in health, environment, and technology. The system also encourages sharing and reuse by making data easier to find and understand, which could lead to new breakthroughs.\n\n**One-sentence takeaway** – Frontiers FAIR² uses AI to rescue lost scientific data, making it reusable and recognized, so research can lead to faster discoveries.",
      "personality_title_fr": "Un nouveau système d'IA sauve et partage 90 % des données scientifiques perdues",
      "personality_presentation_fr": "**Contexte** – La plupart des données scientifiques ne sont jamais pleinement utilisées. Environ 80 % des ensembles de données restent enfermés dans les laboratoires, et une très faible fraction répond aux normes qui facilitent leur recherche et leur réutilisation. Cela ralentit les progrès en médecine et en sciences du climat.\n\n**Ce qui s'est passé** – Frontiers, un éditeur en science ouverte, a lancé Frontiers FAIR² Data Management. C’est le premier système complet alimenté par l’IA qui organise, vérifie, évalue et partage les données de recherche pour les rendre réutilisables et faciles à comprendre. Il crée des paquets de données certifiés, des articles évalués par des pairs, des portails interactifs et des certificats pour chaque ensemble de données.\n\n**Impact** – Ce système transforme des données scientifiques complexes en ressources utiles pour les chercheurs, décideurs et même les systèmes d’IA. Il aide les scientifiques à obtenir du crédit pour leur travail et facilite la combinaison de données issues de différents domaines. Les projets pilotes incluent des données sur les variants du COVID-19, des scans cérébraux, des indicateurs climatiques et la biodiversité insulaire, montrant comment FAIR² soutient la recherche importante.\n\n**Prochaine étape** – À mesure que davantage de scientifiques utiliseront FAIR², plus d’ensembles de données deviendront accessibles et fiables. Cela pourrait accélérer les découvertes en santé, environnement et technologie. Le système encourage aussi le partage et la réutilisation en rendant les données plus faciles à trouver et à comprendre, ce qui pourrait mener à de nouvelles avancées.\n\n**Résumé en une phrase** – Frontiers FAIR² utilise l’IA pour sauver les données scientifiques perdues, les rendant réutilisables et reconnues, afin que la recherche mène à des découvertes plus rapides.",
      "personality_title_es": "Nuevo sistema de IA salva y comparte el 90 % de datos científicos perdidos",
      "personality_presentation_es": "**Contexto** – La mayoría de los datos científicos nunca se usan completamente. Alrededor del 80 % de los conjuntos de datos permanecen en los laboratorios, y solo una pequeña parte cumple con los estándares que facilitan su búsqueda y reutilización. Esto ralentiza el avance en medicina y ciencias climáticas.\n\n**Qué pasó** – Frontiers, una editorial de ciencia abierta, lanzó Frontiers FAIR² Data Management. Es el primer sistema integral impulsado por IA que organiza, verifica, revisa y comparte datos de investigación para hacerlos reutilizables y fáciles de entender. Crea paquetes de datos certificados, artículos revisados por pares, portales interactivos y certificados para cada conjunto de datos.\n\n**Impacto** – Este sistema convierte datos científicos complejos en recursos útiles para investigadores, responsables políticos e incluso sistemas de IA. Ayuda a los científicos a obtener reconocimiento por su trabajo y facilita combinar datos de diferentes campos. Los proyectos piloto incluyen datos sobre variantes de COVID-19, escáneres cerebrales, indicadores climáticos y biodiversidad de atolones, mostrando cómo FAIR² apoya investigaciones importantes.\n\n**Próximos pasos** – A medida que más científicos usen FAIR², más conjuntos de datos serán accesibles y confiables. Esto podría acelerar los descubrimientos en salud, medio ambiente y tecnología. El sistema también fomenta el intercambio y la reutilización al hacer que los datos sean más fáciles de encontrar y entender, lo que podría conducir a nuevos avances.\n\n**Resumen en una frase** – Frontiers FAIR² usa IA para rescatar datos científicos perdidos, haciéndolos reutilizables y reconocidos, para que la investigación conduzca a descubrimientos más rápidos.",
      "image_url": "public/images/news_image_90-of-science-is-lost-This-new-AI-just-found-it.png",
      "image_prompt": "A warm, detailed painting of a glowing, intricate network of interconnected data nodes shaped like open books and scientific instruments, seamlessly merging into a radiant, translucent AI core at the center that illuminates and preserves diverse streams of vibrant, flowing datasets symbolized by colorful, ethereal ribbons weaving together, set against a soft, natural palette of blues, greens, and warm earth tones."
    },
    {
      "title": "This 250-year-old equation just got a quantum makeover",
      "summary": "A team of international physicists has brought Bayes’ centuries-old probability rule into the quantum world. By applying the “principle of minimum change” — updating beliefs as little as possible while remaining consistent with new data — they derived a quantum version of Bayes’ rule from first principles. Their work connects quantum fidelity (a measure of similarity between quantum states) to classical probability reasoning, validating a mathematical concept known as the Petz map.",
      "content": "How likely you think something is to happen depends on what you already believe about the situation. This simple idea forms the basis of Bayes' rule, a mathematical approach to calculating probabilities first introduced in 1763. Now, an international group of scientists has demonstrated how Bayes' rule can also apply in the quantum realm.\n\n\"I would say it is a breakthrough in mathematical physics,\" said Professor Valerio Scarani, Deputy Director and Principal Investigator at the Centre for Quantum Technologies, and member of the team. His co-authors on the work published on 28 August 2025 in Physical Review Letters are Assistant Professor Ge Bai at the Hong Kong University of Science and Technology in China, and Professor Francesco Buscemi at Nagoya University in Japan.\n\n\"Bayes' rule has been helping us make smarter guesses for 250 years. Now we have taught it some quantum tricks,\" said Prof Buscemi.\n\nAlthough other researchers had previously suggested quantum versions of Bayes' rule, this team is the first to derive a true quantum Bayes' rule based on a core physical principle.\n\nConditional probability\n\nBayes' rule takes its name from Thomas Bayes, who described his method for calculating conditional probabilities in \"An Essay Towards Solving a Problem in the Doctrine of Chances.\"\n\nImagine someone who tests positive for the flu. They might have suspected illness already, but this new result changes their assessment of the situation. Bayes' rule provides a systematic way to update that belief, factoring in the likelihood of the test being wrong as well as the person's prior assumptions.\n\nThe rule treats probabilities as measures of belief rather than absolute facts. This interpretation has sparked debate among statisticians, with some arguing that probability should represent objective frequency rather than subjective confidence. Still, when uncertainty and belief play a role, Bayes' rule is widely recognized as a rational framework for decision-making. It underpins countless applications today, from medical testing and weather forecasting to data science and machine learning.\n\nPrinciple of minimum change\n\nWhen calculating probabilities with Bayes' rule, the principle of minimum change is obeyed. Mathematically, the principle of minimum change minimizes the distance between the joint probability distributions of the initial and updated belief. Intuitively, this is the idea that for any new piece of information, beliefs are updated in the smallest possible way that is compatible with the new facts. In the case of the flu test, for example, a negative test would not imply that the person is healthy, but rather that they are less likely to have the flu.\n\nIn their work, Prof Scarani, who is also from NUS Department of Physics, Asst Prof Bai, and Prof Buscemi began with a quantum analogue to the minimum change principle. They quantified change in terms of quantum fidelity, which is a measure of the closeness between quantum states.\n\nResearchers always thought a quantum Bayes' rule should exist because quantum states define probabilities. For example, the quantum state of a particle provides the probability of it being found at different locations. The goal is to determine the whole quantum state, but the particle is only found at one location when a measurement is performed. This new information will then update the belief, boosting the probability around that location.\n\nThe team derived their quantum Bayes' rule by maximizing the fidelity between two objects that represent the forward and the reverse process, in analogy with a classical joint probability distribution. Maximizing fidelity is equivalent to minimizing change. They found in some cases their equations matched the Petz recovery map, which was proposed by Dénes Petz in the 1980s and was later identified as one of the most likely candidates for the quantum Bayes' rule based just on its properties.\n\n\"This is the first time we have derived it from a higher principle, which could be a validation for using the Petz map,\" said Prof Scarani. The Petz map has potential applications in quantum computing for tasks such as quantum error correction and machine learning. The team plans to explore whether applying the minimum change principle to other quantum measures might reveal other solutions.",
      "url": "https://www.sciencedaily.com/releases/2025/10/251013040333.htm",
      "source": "Latest Science News -- ScienceDaily",
      "published": "2025-10-13",
      "sentiment_score": 0.8,
      "reasoning": "The article reports a significant scientific breakthrough in mathematical physics where researchers derived a quantum version of Bayes' rule from first principles, connecting classical probability reasoning with quantum mechanics. This advancement has broad implications for quantum computing, quantum error correction, and machine learning, potentially benefiting technology and society at large. The story is focused, detailed, and highlights a meaningful progress in understanding and applying quantum information theory.",
      "category": "Technology",
      "personality_title": "Scientists create a quantum version of Bayes’ rule after 250 years",
      "personality_presentation": "**Context** – Bayes’ rule is a 250-year-old mathematical formula used to update what we believe about a situation when new information arrives. It helps in many areas like medical testing and weather forecasting by calculating the chance something is true based on what we already know.\n\n**What happened** – A group of scientists from different countries have now created a quantum version of Bayes’ rule. They used a principle called the “minimum change,” which means updating beliefs as little as possible but still fitting the new facts. Their work was published on August 28, 2025, and it links classical probability with quantum physics, confirming a mathematical tool called the Petz map.\n\n**Impact** – This is the first time a true quantum Bayes’ rule has been derived from basic physical ideas instead of guesses. It connects how we understand uncertainty in the everyday world with the strange rules of quantum physics. The discovery could improve quantum computing, help fix errors in quantum machines, and make quantum machine learning smarter.\n\n**What's next step** – The researchers want to see if the minimum change idea can be used with other quantum concepts to find more useful rules. This could lead to better ways to handle information in quantum technologies.\n\n**One-sentence takeaway** – Scientists have successfully adapted Bayes’ rule for quantum physics, opening new paths to improve quantum technology and our understanding of probabilities in the quantum world.",
      "personality_title_fr": "Des scientifiques créent une version quantique de la règle de Bayes après 250 ans",
      "personality_presentation_fr": "**Contexte** – La règle de Bayes est une formule mathématique vieille de 250 ans qui sert à mettre à jour nos croyances quand de nouvelles informations arrivent. Elle aide dans de nombreux domaines comme les tests médicaux ou les prévisions météo en calculant la probabilité qu’une chose soit vraie selon ce que l’on sait déjà.\n\n**Ce qui s’est passé** – Un groupe de scientifiques internationaux a créé une version quantique de la règle de Bayes. Ils ont utilisé un principe appelé “changement minimum”, qui signifie modifier les croyances le moins possible tout en tenant compte des nouveaux faits. Leur travail, publié le 28 août 2025, relie la probabilité classique à la physique quantique et confirme un outil mathématique nommé la carte de Petz.\n\n**Impact** – C’est la première fois qu’une vraie règle de Bayes quantique est déduite à partir d’idées physiques fondamentales, et non de suppositions. Cette découverte relie notre compréhension de l’incertitude dans le monde quotidien aux règles étranges de la physique quantique. Elle pourrait améliorer l’informatique quantique, aider à corriger les erreurs dans les machines quantiques, et rendre l’apprentissage automatique quantique plus efficace.\n\n**Prochaine étape** – Les chercheurs veulent vérifier si le principe du changement minimum peut s’appliquer à d’autres concepts quantiques pour trouver d’autres règles utiles. Cela pourrait conduire à de meilleures façons de gérer l’information dans les technologies quantiques.\n\n**Phrase clé** – Des scientifiques ont adapté avec succès la règle de Bayes à la physique quantique, ouvrant la voie à de nouvelles avancées dans la technologie quantique et la compréhension des probabilités quantiques.",
      "personality_title_es": "Científicos crean una versión cuántica de la regla de Bayes tras 250 años",
      "personality_presentation_es": "**Contexto** – La regla de Bayes es una fórmula matemática de 250 años que se usa para actualizar lo que creemos cuando llega nueva información. Ayuda en muchas áreas como pruebas médicas y pronósticos del clima al calcular la probabilidad de que algo sea cierto según lo que ya sabemos.\n\n**Qué pasó** – Un grupo de científicos de varios países creó ahora una versión cuántica de la regla de Bayes. Usaron un principio llamado “cambio mínimo”, que significa actualizar las creencias lo menos posible pero ajustándose a los nuevos hechos. Su trabajo, publicado el 28 de agosto de 2025, conecta la probabilidad clásica con la física cuántica y confirma una herramienta matemática llamada mapa de Petz.\n\n**Impacto** – Esta es la primera vez que se deriva una verdadera regla de Bayes cuántica desde ideas físicas básicas y no de suposiciones. Este descubrimiento une cómo entendemos la incertidumbre en el mundo normal con las extrañas reglas de la física cuántica. Puede mejorar la computación cuántica, ayudar a corregir errores en máquinas cuánticas y hacer que el aprendizaje automático cuántico sea más inteligente.\n\n**Próximo paso** – Los investigadores quieren ver si la idea del cambio mínimo puede usarse con otros conceptos cuánticos para encontrar reglas útiles. Esto podría llevar a mejores formas de manejar la información en tecnologías cuánticas.\n\n**Resumen en una frase** – Científicos han adaptado con éxito la regla de Bayes a la física cuántica, abriendo nuevos caminos para mejorar la tecnología cuántica y nuestra comprensión de las probabilidades en el mundo cuántico.",
      "image_url": "public/images/news_image_This-250-year-old-equation-just-got-a-quantum-make.png",
      "image_prompt": "A warm, detailed painting of an elegant, glowing mathematical equation transforming into intertwined quantum waveforms and softly illuminated probability clouds, symbolizing the fusion of classical Bayes' rule with quantum mechanics, set against a calm, abstract backdrop of gently shifting light and subtle geometric shapes in natural, muted tones."
    },
    {
      "title": "‘Am I redundant?’: how AI changed my career in bioinformatics",
      "summary": "Nature, Published online: 13 October 2025; doi:10.1038/d41586-025-03135-zA run-in with some artefact-laden AI-generated analyses convinced Lei Zhu that machine learning wasn’t making his role irrelevant, but more important than ever.",
      "content": "The rise of agentic AI tools caused Lei Zhu to rethink his role in bioinformatics.Credit: Lei Zhu\n\nWhen I began my graduate studies, the first thing I needed to do was choose a research direction. The laboratory I joined focused on two main areas: functional assays and bioinformatics. This was more than a decade ago, and the typical workflow involved bioinformatics researchers analysing large data sets to identify genes associated with specific phenotypes or diseases, which would then be handed over to the functional-assay team for validation.\n\nAt the time, bioinformatics was a new, promising field, so I chose this path without hesitation. But, I did not have a programming background, so it was tough to get started. I began studying programming languages — first Perl, then R and Python.\n\nLooking back, I’m happy with my choice. It was an exciting time, and the rapid growth of high-throughput technologies and new techniques — such as transcriptomics and genomics, and later single-cell biology — gave me plenty of data to work with. Solving biological problems through the code I wrote gave me a sense of self-worth.\n\nThen, artificial intelligence (AI) tools, including ChatGPT, Manus and Grok, emerged. Their ability to spit out functional code threatened to make me redundant, but I wasn’t concerned at first because AI-generated code often contains errors that only appear during testing and require manual debugging. New ‘agentic’ modes of operation, however, were potential game-changers. These allow tools such as Manus to first generate code and then run it directly in the cloud, creating a seamless loop: from me asking questions, to the tool writing and executing code, to me receiving results. That was when I started to worry: in this age of AI, am I still necessary?\n\nToday’s AI tools can efficiently write code to perform biological analyses. I need only upload my data and provide a simple prompt, such as, ‘Assume you are a bioinformatics expert. Could you create ten visuals to represent different data based on your understanding of the data set above? Display the plots one by one, with a brief introduction.’ The AI provides the answers I need, sometimes exceeding my expectations. So, what is my role in this process?\n\nI found out during a study of lung cancer. We had hundreds of tumour tissue gene-expression profiles, and I asked the AI to set up the analysis. It worked quickly, and even produced a tidy report. The preliminary results looked great — almost too good. The AI identified a statistically significant difference in gene-expression levels before and after a specific time point. But as I dug deeper, I saw that, halfway through the study, the lab had changed how the data were collected. The model had picked up on that difference — not one due to biology. What had looked like a breakthrough was actually just an artefact. Once I adjusted for that change, the difference became less dramatic but reflected real biology.\n\nI realized that my role had shifted from scripting to supervising. What matters now is stating the question clearly, spotting problems that the computer cannot see and taking responsibility for the answer.\n\nTop tips for AI supervisors\n\nPeople tell me that I could make the AI smarter by ‘putting more context into the prompt’, but my AI always seems to play dumb. No matter how detailed my request, it finds ways to misunderstand. Over the past few years, I’ve developed some methods to double-check its work.\n\nCreate a validation set. Keep a small data set that you understand well — a subset of previously published or manually validated data, for instance — as a positive control. Before applying a new AI-generated pipeline to your data, test it on this set. If the AI produces unexpected or inconsistent results, you’ll know immediately that either the prompt or the algorithm needs refinement.\n\nShuffle the data. AI models can easily overfit data or be swayed by technical artefacts, as happened during the lung-cancer study. To test whether a finding is biologically meaningful, shuffle sample labels, perturb values slightly or otherwise introduce synthetic noise. If the ‘significant’ pattern persists, it’s probably an artefact, rather than a true signal.\n\nSubset the analysis. If a data set is big enough, I will ask the AI to perform the same analysis on random subsets of it. Consistency across subsets increases confidence: if the results vary wildly from one subset to the next, the finding might not hold up.",
      "url": "https://www.nature.com/articles/d41586-025-03135-z",
      "source": "Nature",
      "published": "2025-10-14",
      "sentiment_score": 0.8,
      "reasoning": "The article highlights a positive real-world impact of AI in bioinformatics, showing how AI tools enhance research capabilities rather than replace human expertise. It emphasizes the evolving role of scientists as supervisors who ensure data integrity and meaningful interpretation, which is significant for the broader scientific community. The story provides detailed context on the integration of AI in biological data analysis and practical methods to validate AI outputs, illustrating a meaningful advancement in technology with broad implications.",
      "category": "Technology",
      "personality_title": "How AI is changing bioinformatics but not replacing scientists",
      "personality_presentation": "**Context** – Over the last decade, bioinformatics has grown as a field where researchers use computer programs to study biological data, like genes related to diseases. Lei Zhu started his career in this area, learning programming to analyze large data sets.\n\n**What happened** – Recently, new AI tools emerged that can write and run code automatically, making it easier to analyze biological data. Lei used these tools for a lung cancer study and found that while AI could quickly produce results, it sometimes made mistakes by missing important details in the data. This showed him that his role had shifted from writing code to carefully checking the AI’s work.\n\n**Impact** – This story shows that AI helps scientists work faster and handle more data, but it does not replace human judgment. Scientists like Lei are now supervisors who make sure AI results are accurate and meaningful. They spot errors and guide the AI to better answers, which is important for reliable scientific discoveries.\n\n**What's next step** – Lei developed ways to test AI results, like using known data sets to check accuracy, mixing up data to find errors, and repeating analyses on smaller groups. These methods help make AI tools more trustworthy. Going forward, scientists will keep improving how they work with AI to get the best results from big biological data.\n\n**One-sentence takeaway** – AI tools speed up bioinformatics research but still need scientists to guide and verify their findings for accurate and useful insights.",
      "personality_title_fr": "Comment l’IA transforme la bioinformatique sans remplacer les scientifiques",
      "personality_presentation_fr": "**Contexte** – Au cours de la dernière décennie, la bioinformatique s’est développée comme un domaine où les chercheurs utilisent des programmes informatiques pour étudier des données biologiques, comme des gènes liés à des maladies. Lei Zhu a commencé sa carrière dans ce domaine en apprenant la programmation pour analyser de grandes séries de données.\n\n**Ce qui s’est passé** – Récemment, de nouveaux outils d’IA sont apparus, capables d’écrire et d’exécuter automatiquement du code, facilitant ainsi l’analyse des données biologiques. Lei a utilisé ces outils pour une étude sur le cancer du poumon et a découvert que, bien que l’IA puisse produire rapidement des résultats, elle commettait parfois des erreurs en ne prenant pas en compte certains détails importants des données. Cela lui a montré que son rôle avait évolué, passant de l’écriture de code à la vérification minutieuse du travail de l’IA.\n\n**Impact** – Cette histoire montre que l’IA aide les scientifiques à travailler plus vite et à traiter plus de données, mais ne remplace pas le jugement humain. Des scientifiques comme Lei sont désormais des superviseurs qui s’assurent de la précision et de la pertinence des résultats de l’IA. Ils détectent les erreurs et orientent l’IA vers de meilleures réponses, ce qui est essentiel pour des découvertes scientifiques fiables.\n\n**Prochaine étape** – Lei a développé des méthodes pour tester les résultats de l’IA, comme utiliser des jeux de données connus pour vérifier la précision, mélanger les données pour détecter les erreurs, et répéter les analyses sur des sous-groupes. Ces méthodes rendent les outils d’IA plus fiables. À l’avenir, les scientifiques continueront d’améliorer leur collaboration avec l’IA pour obtenir les meilleurs résultats à partir de grandes données biologiques.\n\n**Une phrase clé** – Les outils d’IA accélèrent la recherche en bioinformatique mais nécessitent toujours que les scientifiques guident et vérifient leurs résultats pour obtenir des informations précises et utiles.",
      "personality_title_es": "Cómo la IA está cambiando la bioinformática sin reemplazar a los científicos",
      "personality_presentation_es": "**Contexto** – En la última década, la bioinformática ha crecido como un campo donde los investigadores usan programas de computadora para estudiar datos biológicos, como genes relacionados con enfermedades. Lei Zhu comenzó su carrera en esta área aprendiendo programación para analizar grandes conjuntos de datos.\n\n**Qué pasó** – Recientemente, surgieron nuevas herramientas de IA que pueden escribir y ejecutar código automáticamente, facilitando el análisis de datos biológicos. Lei usó estas herramientas en un estudio sobre cáncer de pulmón y descubrió que, aunque la IA podía producir resultados rápidamente, a veces cometía errores al no notar detalles importantes en los datos. Esto le mostró que su papel cambió de escribir código a supervisar cuidadosamente el trabajo de la IA.\n\n**Impacto** – Esta historia muestra que la IA ayuda a los científicos a trabajar más rápido y manejar más datos, pero no reemplaza el juicio humano. Científicos como Lei ahora son supervisores que aseguran que los resultados de la IA sean precisos y útiles. Detectan errores y guían a la IA hacia mejores respuestas, lo cual es importante para descubrimientos científicos confiables.\n\n**Próximo paso** – Lei desarrolló métodos para revisar los resultados de la IA, como usar conjuntos de datos conocidos para verificar precisión, mezclar datos para encontrar errores, y repetir análisis en grupos más pequeños. Estos métodos ayudan a hacer las herramientas de IA más confiables. En el futuro, los científicos seguirán mejorando cómo trabajan con la IA para obtener los mejores resultados de grandes datos biológicos.\n\n**Una frase clave** – Las herramientas de IA aceleran la investigación en bioinformática pero aún necesitan que los científicos guíen y verifiquen sus resultados para obtener conclusiones precisas y útiles.",
      "image_url": "public/images/news_image_Am-I-redundant-how-AI-changed-my-career-in-bioinfo.png",
      "image_prompt": "A warm, detailed painting of an intricate network of glowing, interconnected data nodes and gene strands woven together like a tapestry, with a gentle, watchful owl silhouette perched nearby symbolizing careful supervision and wisdom, all rendered in soft, natural earth tones and muted blues."
    },
    {
      "title": "What makes a quantum computer good?",
      "summary": "Claims that one quantum computer is better than another rest on terms like quantum advantage or quantum supremacy, fault-tolerance or qubits with better coherence – what does it all mean? Karmela Padavic-Callaghan sifts through the noise",
      "content": "Claims that one quantum computer is better than another rest on terms like quantum advantage or quantum supremacy, fault-tolerance or qubits with better coherence – what does it all mean? Karmela Padavic-Callaghan sifts through the noise\n\n3D rendering of a quantum computer’s chandelier-like structure Shutterstock / Phonlamai Photo\n\nEleven years ago, I was just getting a start on my PhD in theoretical physics, and to be honest with you I never thought about quantum computers, or writing about them, at all. Meanwhile, New Scientist staff were hard at work putting together the world’s first “Quantum computer buyer’s guide” (we’ve always been ahead of the curve). Looking through it reveals what a different time it was – John Martinis at University of California, Santa Barbara got a shout out for working on an array of only nine qubits, and just last week he was awarded the Nobel Prize in Physics. Meanwhile, quantum computers made from neutral atoms, which have taken the field by storm in recent years, are not even mentioned. This made me wonder: what would a quantum computer buyer’s guide look like today?\n\nThere are currently about 80 companies across the world manufacturing quantum computing hardware. Because I report on quantum computing, I have had a chance to watch it grow as an industry from up close – and to hear an awful lot of sales pitches. If you thought deciding between an iPhone and an Android phone is tough, try being on the press list for dozens of quantum computing start-ups.\n\nSure, a lot of hype comes with marketing, but some of the difficulty in comparing these devices and approaches stems from the fact that there is currently no consensus on the best way to build a quantum computer. For instance, you could opt for qubits made from superconducting circuits, extremely cold ions, light or several other options. How do you weigh the differences between these machines when they have fundamentally different parts? It helps to focus on the performance of each quantum computer.\n\nThis is a notable shift from the early days of quantum computing, where the champions among these novel devices were determined by the number of qubits – the most basic building blocks of quantum information processing – a machine had. Several research teams have now broken the 1000-qubit barrier and the road towards ever larger numbers of qubits seems to look clearer every day. Researchers are now working out how to leverage standard manufacturing techniques, like making qubits made from silicon and even using AI to make their quantum computers bigger – and more powerful.\n\nIn an ideal world, more qubits would always mean more computational power because this would allow the quantum computer to tackle more complex problems. In our actual world, making sure every new qubit you add doesn’t worsen the performance of the ones you already have has proven to be a huge technical challenge. So, it’s not just the number of qubits you have, but also how well they can hold onto information and how well they can “talk” to each other without that information degrading. A quantum computer could have millions of qubits and be essentially useless if those qubits are prone to glitches that introduce errors into calculations.\n\nFree newsletter Sign up to The Daily The latest on what’s new in science and why it matters each day. Sign up to newsletter\n\nThis glitchiness – or noise – can be quantified in metrics such as “gate fidelity”, which captures how accurately you can make a qubit or a pair of qubits do something, and “coherence time”, which puts a number to how long a qubit can stay in a quantum state that is useful to you. But these measures land us right back into the nitty-gritty details of quantum computing hardware. Annoyingly, even if those metrics are great, you still have to worry about how difficult it might be to input data into your quantum computer and start the computation, as well as whether you’ll run into trouble when you try to read out the final result.\n\nPart of the remarkable growth of the quantum computing industry has been due to the rise of companies specialising in qubit control and other parts of quantum computers that deal with the tricky interface between these devices’ quantum innards and their very non-quantum users. A proper 2025 quantum computer buyer’s guide would have to include all these add-ons. You’d have to choose your qubits, but also a qubit control system and some mechanism for correcting those qubits’ errors. I’ve had a chance to speak with researchers who are even developing an operating system for quantum computers, so in a few years that may also become part of your shopping list.\n\nIf I had to assemble a near-term wish list, I would hedge my bets on a machine that can perform at least a million operations – roughly, a quantum computing program that has a million steps – with very low error rates and as much built-in error correction as possible. John Preskill at the California Institute of Technology calls this the “megaquop” machine. Last year he told me he believes such a machine would be just powerful enough to be fault-tolerant, or error-proof, and to make scientifically meaningful discoveries. But we’re not there yet. The quantum computers we have today are running tens of thousands of operations, and have only demonstrated error-correction for relatively small tasks.\n\nIn some sense, today’s quantum computers are in an era of adolescence, maturing towards usefulness but still going through growing pains. Because of this, the question I find myself asking the quantum computer merchants in my inbox most often is: “What can this machine actually do?”\n\nThis is where we have to not only compare different types of quantum computers, but pit them against their conventional counterparts as well. Quantum hardware is costly and difficult to build, so when would it truly be the only viable option for solving a problem?\n\nOne way to answer this question is to try to identify calculations conventional computers could not complete unless they had unlimited amounts of time. Colloquially, this goes by the name of “quantum supremacy”, and it keeps mathematicians and complexity theorists up at night as much as it steals sleep from quantum engineers. Examples of quantum supremacy do exist, but they are troublesome. To be meaningful, they must be practical – you have to be able to build a machine that can execute them – and they must be provable such that you can be sure a clever mathematician could not get a conventional computer to execute them after all.\n\nIn 1994, physicist Peter Shor developed a quantum computing algorithm for factoring large numbers which could be used to easily break the most common encryption methods currently used by, for instance, the world’s banks. A big enough quantum computer that corrects its own errors could practically run this algorithm, but mathematicians have so far not been able to rigorously prove classical computers could never factor large numbers as efficiently. The most notable quantum supremacy claims fall into this category, too – and some of them were eventually bested by classical machines. Quantum supremacy demonstrations that are still standing also do not yet seem to be useful and are primarily designed to showcase the quantumness of the computer that completed them.\n\nOn the opposite side of the spectrum are problems in the mathematical field of “query complexity”, where the supremacy of the quantum approach is rigorously provable, but there are no related algorithms that would be practical to implement or do something unambiguously useful. A recent experiment also introduced the idea of “quantum information supremacy”, where a quantum computer solved a task using fewer qubits than the number of bits required to solve the same problem on a classical computer. Here, the resource the quantum computer needed less of was not time but rather the number of physical building blocks. This may sound promising because it implies a quantum computer could do something without having to be made huge first, but I would not advise you to buy it for one simple reason – the task in question yet again had no obvious uses in the real world.\n\nCertainly, there are real-world problems that seem like a good match for quantum computer algorithms, such as determining the properties of molecules that are important in agriculture and medicine, or solving logistics problems like scheduling flights. But I have to say “seem” because the truth is researchers don’t have all the details down yet.\n\nFor instance, in recent study on possible uses of quantum computing for genomics, Aurora Maurizio at San Raffaele Scientific Institute in Italy and Guglielmo Mazzola at the University of Zurich in Switzerland wrote that conventional computing methods are so good that “quantum computing could offer a speedup in the near future only for a specific subset of hard enough tasks”. The message of their study is that even though at first glance combinatorics problems in genomics look like an area where a quantum computer could accelerate the work, a closer look reveals their use will have to be very targeted and careful.\n\nThe truth is that for many problems not constructed specifically to prove quantum supremacy, even when quantum computers can overcome noisiness and all other technical issues and run algorithms faster than classical computers, there is a spectrum for what “faster” means. Because it doesn’t always mean exponentially faster, the time savings a quantum computer could bring don’t always fully counterbalance the hardware costs. For example, computer scientist Lov Grover’s search algorithm, which is the second-most-famous quantum computing algorithm after Shor’s, only offers a quadratic speed up – it cuts down the run time of the calculation by a square root instead of exponentially. Ultimately, how much faster is fast enough to justify going quantum may be up to each individual quantum computer buyer.\n\nAnd I know, I know, this is a frustrating line to include in a so-called buyer’s guide, but if I have learned anything about quantum computers from talking with experts, it is that there is a lot more we don’t know about what quantum computers could do than we know with certainty. Quantum computers are an expensive and complex technology of the future, and we are barely getting a taste of where they could add value to our lives instead of just adding value to some company’s shareholders. As unsatisfying as that is, I believe it is a marker of how different and novel quantum computers really are; how much they truly are the frontier of computing.\n\nBut if you happen to be reading this because you do have a good chunk of pocket money to spare on as large and as reliable a quantum computer as you can find, please do get it and let your local quantum algorithm nerds mess around with it. In a few years’ time, they could probably give you much better advice.",
      "url": "https://www.newscientist.com/article/2499714-what-makes-a-quantum-computer-good/?utm_campaign=RSS%7CNSNS&utm_source=NSNS&utm_medium=RSS&utm_content=home",
      "source": "New Scientist - Home",
      "published": "2025-10-13",
      "sentiment_score": 0.75,
      "reasoning": "The article provides an in-depth, substantive overview of the current state and challenges of quantum computing, highlighting its potential for significant technological breakthroughs that could benefit society broadly. It explains the progress made, the complexity of the technology, and the hopeful future impact on fields like medicine, agriculture, and logistics, thus offering an inspiring perspective on a cutting-edge innovation with wide-reaching implications.",
      "category": "Technology",
      "personality_title": "Understanding what makes a quantum computer truly powerful",
      "personality_presentation": "**Context** – Quantum computers are advanced machines that use tiny units called qubits to solve problems in new ways. Over the past decade, many companies have developed different types of quantum computers, but it’s still unclear which designs work best and how to measure their real power.\n\n**What happened** – Experts have moved beyond just counting qubits to judge quantum computers. Now, they focus on how well qubits keep information without errors and how accurately they perform tasks. Researchers are also working on ways to control qubits better and fix their mistakes. While some quantum computers have over 1,000 qubits, the challenge is making sure adding more qubits doesn’t cause more errors.\n\n**Impact** – This shift helps us understand that a quantum computer isn’t just about size but about quality and reliability. It also shows why quantum computers today aren’t yet able to solve many real-world problems faster than normal computers. However, scientists believe that future machines with better error correction and control could handle important tasks in medicine, agriculture, and planning, which regular computers struggle with.\n\n**What's next step** – Researchers aim to build quantum computers that can perform millions of operations with very low error rates, a milestone called the “megaquop” machine. They are also developing better software and systems to make quantum computers easier to use. As these advances happen, quantum computers may become practical tools for solving complex problems.\n\n**One-sentence takeaway** – The true strength of a quantum computer depends on how well it controls errors and processes information, not just on the number of qubits it has.\n",
      "personality_title_fr": "Comprendre ce qui rend un ordinateur quantique vraiment puissant",
      "personality_presentation_fr": "**Contexte** – Les ordinateurs quantiques sont des machines avancées qui utilisent de petites unités appelées qubits pour résoudre des problèmes de manière nouvelle. Au cours de la dernière décennie, de nombreuses entreprises ont développé différents types d’ordinateurs quantiques, mais il reste difficile de savoir quelles conceptions fonctionnent le mieux et comment mesurer leur véritable puissance.\n\n**Ce qui s’est passé** – Les experts ne se contentent plus de compter les qubits pour juger les ordinateurs quantiques. Ils se concentrent désormais sur la capacité des qubits à conserver l’information sans erreurs et sur la précision de leurs opérations. Les chercheurs travaillent aussi à mieux contrôler les qubits et à corriger leurs erreurs. Bien que certains ordinateurs quantiques disposent de plus de 1 000 qubits, le défi est de s’assurer que l’ajout de qubits ne crée pas plus d’erreurs.\n\n**Impact** – Ce changement de point de vue permet de comprendre qu’un ordinateur quantique ne dépend pas seulement de sa taille, mais de la qualité et de la fiabilité de ses qubits. Il explique aussi pourquoi les ordinateurs quantiques actuels ne peuvent pas encore résoudre plus rapidement que les ordinateurs classiques de nombreux problèmes réels. Cependant, les scientifiques pensent que les futures machines, avec une meilleure correction d’erreurs et un meilleur contrôle, pourront traiter des tâches importantes en médecine, agriculture et planification.\n\n**Prochaine étape** – Les chercheurs visent à construire des ordinateurs quantiques capables d’exécuter des millions d’opérations avec un très faible taux d’erreurs, un objectif appelé machine « mégaquop ». Ils développent aussi de meilleurs logiciels et systèmes pour faciliter leur utilisation. Ces avancées pourraient bientôt rendre les ordinateurs quantiques des outils pratiques pour résoudre des problèmes complexes.\n\n**Phrase-clé** – La vraie puissance d’un ordinateur quantique dépend de sa capacité à contrôler les erreurs et à traiter l’information, pas seulement du nombre de qubits qu’il possède.\n",
      "personality_title_es": "Entendiendo qué hace realmente poderoso a un ordenador cuántico",
      "personality_presentation_es": "**Contexto** – Los ordenadores cuánticos son máquinas avanzadas que usan pequeñas unidades llamadas qubits para resolver problemas de formas nuevas. En la última década, muchas empresas han desarrollado diferentes tipos de ordenadores cuánticos, pero aún no está claro qué diseños funcionan mejor ni cómo medir su verdadero poder.\n\n**Qué pasó** – Los expertos ya no solo cuentan qubits para evaluar los ordenadores cuánticos. Ahora se enfocan en qué tan bien los qubits mantienen la información sin errores y qué tan precisos son en sus tareas. Los investigadores también trabajan en controlar mejor los qubits y corregir sus errores. Aunque algunos ordenadores cuánticos tienen más de 1,000 qubits, el reto es asegurarse de que agregar más qubits no cause más errores.\n\n**Impacto** – Este cambio ayuda a entender que un ordenador cuántico no depende solo de su tamaño, sino de la calidad y confiabilidad de sus qubits. También explica por qué los ordenadores cuánticos actuales aún no pueden resolver muchos problemas reales más rápido que los ordenadores normales. Sin embargo, los científicos creen que futuras máquinas con mejor corrección de errores y control podrán manejar tareas importantes en medicina, agricultura y planificación.\n\n**Próximo paso** – Los investigadores buscan construir ordenadores cuánticos que puedan realizar millones de operaciones con muy bajos índices de error, un hito llamado la máquina “megaquop”. También están desarrollando mejores programas y sistemas para facilitar su uso. A medida que avancen estos desarrollos, los ordenadores cuánticos podrían convertirse en herramientas prácticas para resolver problemas complejos.\n\n**Frase clave** – La verdadera fuerza de un ordenador cuántico depende de qué tan bien controla los errores y procesa la información, no solo del número de qubits que tiene.\n",
      "image_url": "public/images/news_image_What-makes-a-quantum-computer-good.png",
      "image_prompt": "A warm, detailed painting of a glowing, intricate chandelier-like quantum computer structure made of interconnected luminous qubits resembling delicate, softly pulsing orbs linked by fine, shimmering threads, set against a calm abstract background symbolizing the complex, evolving frontier of quantum computing technology."
    }
  ]
}