{
  "personality": null,
  "timestamp": "2025-12-04T04:47:59.055749",
  "category": "Technology",
  "news_summary": "Today's technology news highlights advanced scientific breakthroughs in computational enzyme design, innovative neutrino research, evolutionary genetics, and the characterization of exotic particles.",
  "news_summary_fr": "L'actualité technologique d'aujourd'hui met en lumière les avancées scientifiques dans la conception computationnelle d'enzymes, la recherche innovante sur les neutrinos, la génétique évolutive et la caractérisation des particules exotiques.",
  "news_summary_es": "Las noticias tecnológicas de hoy destacan los avances científicos en el diseño computacional de enzimas, la investigación innovadora sobre neutrinos, la genética evolutiva y la caracterización de partículas exóticas.",
  "articles": [
    {
      "title": "Computational design of metallohydrolases",
      "summary": "Nature, Published online: 03 December 2025; doi:10.1038/s41586-025-09746-wA generative artificial intelligence-powered method enables de novo design of highly active enzymes based on information about the geometry of residues in the active site, without requiring protein backbone or sequence information.",
      "content": "Metallohydrolases catalyse some of the most difficult hydrolysis reactions in biology by using their bound metal ions to activate a water molecule positioned adjacent to the substrate bond to be cleaved16,17,18. Engineering new metallohydrolases is currently of considerable interest for degrading human-generated environmental pollutants, for which there has not been sufficient time for efficient hydrolytic enzymes to evolve19,20,21. Protein engineering has expanded the scope of substrates that can be hydrolysed by metallohydrolases, but this often requires initial promiscuous activity22,23. De novo enzyme design has been used to generate new metallohydrolases6,10,24, but these have had relatively low activity and efficiency, and have required extensive directed evolution to match the activity and efficiency of native enzymes24. Given an ideal metallohydrolase active site, de novo enzyme design seeks to identify or generate a protein scaffold that positions the catalytic residues, metals, and substrates in optimal catalytic geometries with high accuracy25,26. RFdiffusion has been used successfully to scaffold active sites, but the search has been limited by the need to specify the sequence positions and conformations of the catalytic residues8,9,27.\n\nWe reasoned that a generative artificial intelligence design method that only required the specification of side-chain functional group positions around a reaction transition state, and was capable of sampling over all possible sequence positions and conformations of these residues, could more readily satisfy complex catalytic constraints14,15,28,29. We set out to develop such an approach, and used it to design new metallohydrolases starting from a quantum chemistry-generated active site description with a bound metal cofactor.\n\nTo enable sequence-position and side-chain rotamer-agnostic enzyme design, we developed a generative artificial intelligence flow-matching model called RFdiffusion230. RFdiffusion2 extends the capabilities of RFdiffusion to generate scaffolds that position a set of functional residues (a ‘motif’) in two key ways. First, it enables atomic substructure scaffolding: RFdiffusion can only scaffold backbone-level motifs (with the side-chain and backbone atom N-Cα-C=O positions specified), whereas RFdiffusion2 can scaffold arbitrary atom-level motifs (any subset of amino acid heavy atoms). This is important for enzyme design because it allows users to specify only the positions of the key functional groups that interact with the reaction transition state, rather than the full side-chain and backbone conformation. Second, RFdiffusion2 enables sequence-position-agnostic scaffolding: RFdiffusion requires specification of the primary sequence positions of the motif residues, but RFdiffusion2 can scaffold motifs whose primary sequence positions are unknown. RFdiffusion2 replaces diffusion with flow matching31,32 and achieves sequence-position-agnostic atomic substructure scaffolding by providing randomly selected native atomic coordinates (but not their sequence positions) during training in addition to the partially noised, sequence-labelled atomic coordinates. With these improvements, RFdiffusion2 generates diverse proteins starting directly from catalytic configurations that consist of input functional group positions and substrate coordinates. Allowing the model to resolve the a priori unknown degrees of freedom (that is, the primary sequence positions and side-chain rotamer conformations of the catalytic residues) is considerably more effective at generating self-consistent design solutions than randomly sampling those degrees of freedom before inference, because the space is far too large to enumerate, as was necessitated with RFdiffusion. A detailed description of RFdiffusion2 training and benchmarking results for a wide range of active site scaffolding problems is described elsewhere30.\n\nAs an initial test of RFdiffusion2, we chose to design a zinc metallohydrolase for the hydrolysis of a fluorogenic ester, 4-methylumbelliferyl phenylacetate (4MU-PA), as a target reaction (Fig. 1a). We began by using density functional theory (DFT) to identify the transition-state geometry of the rate-determining Zn(II)-OH nucleophilic attack on the substrate ester. Four distinct catalytic arrangements based on the stereochemistry of the tetrahedral intermediate and the nature of the oxyanion hole were considered (Fig. 1b, Supplementary Figs. 1 and 2, Supplementary Data 1 and Supplementary Methods 4.1). These calculations provide the coordinates of the three Zn(II)-binding imidazole rings, the metal, and the transition state. Our previous RFdiffusion approach required the backbone coordinates and residue positions as inputs, which would require upfront sampling of the rotameric states and sequence position for each histidine. This cannot be done exhaustively: even with relatively coarse sampling around the side-chain chi angles χ 1 , χ 2 , and the backbone torsion angle ψ, there are on the order of 1018 possible choices for the side-chain conformations and sequence placements of our full catalytic site (Fig. 1c and Extended Data Fig. 1). Whereas each RFdiffusion run has to be initialized with a specific (and generally randomly selected) choice from this enormous set of combinations, RFdiffusion2 as described above searches the entire space in each trajectory.\n\nFig. 1: RFdiffusion2 design method. a, Hydrolysis of 4MU-PA yields phenylacetic acid and a fluorescent coumarin product. b, Example theozyme for Zn(II)-hydroxide nucleophilic attack on the 4MU-PA ester. Two-dimensional representation (left) and 3D DFT model (right). Arrows on the 3D model represent sampled conformational flexibility. c, Comparison of scaffold generation around an input theozyme using previous backbone centric RFdiffusion (top row) versus interaction functional group centric RFdiffusion2. RFdiffusion requires explicit upfront sampling of side-chain conformations and residue sequence positions, whereas RFdiffusion2 only requires the transition-state complex and the catalytic side-chain functional groups, implicitly sampling sequence space and rotameric space during inference. d, Snapshots of the global structure and active site from model X T during an RFdiffusion2 inference trajectory. The coordinates of the input transition-state complex and catalytic functional groups stay fixed during inference while the backbone structure, sequence positions, and unspecified atoms of the catalytic side chains are sampled by RFdiffusion2. The Cα atoms that host the catalytic histidines at the end of the trajectory are retrospectively highlighted as red spheres; these Cα atoms are not predetermined but rather move into the frame to host the fixed side chains as the global structure forms around the motifs. Full size image\n\nRFdiffusion2 inference trajectories were used to build protein scaffolds housing the DFT-generated minimal active site configurations, referred to as theozymes2,33. Several snapshots from a representative trajectory are shown in Fig. 1d, transforming random noise on the left into the final backbone on the right (Supplementary Video 1). The Cα atoms of each residue (shown as coloured spheres representing final sequence position) are initially sampled from a Gaussian distribution, and the target functional atom positions (shown in sticks) stay fixed. As the trajectory proceeds from left to right, the global structure takes shape around the motif, with the fixed histidine side chains eventually connecting to Cα atoms of the protein backbone at sequence positions of the network’s choosing. A total of 5,120 RFdiffusion2 inference trajectories were carried out starting from different random seeds and for each of the resulting protein scaffolds, sequences were generated using ProteinMPNN34. The catalytic geometry and interactions with the transition state of those designs for which the AlphaFold235 predicted structure was close to the design model were further optimized using iterative LigandMPNN36 and constrained Rosetta repacking and minimization37 (Extended Data Fig. 2 and Supplementary Methods 4.1). Designs containing a proposed general base positioned to activate the water molecule (that is, Glu, Asp or His within hydrogen bonding distance of the Zn(II)-bound water) and side-chain hydrogen bonds stabilizing the transition-state oxyanion (if applicable), and that AlphaFold2 predicted to adopt the target structure, were characterized with PLACER12 to assess active site preorganization. A total of 96 designs were selected for experimental characterization on the basis of predicted active site geometry and preorganization (Supplementary Fig. 3, Supplementary Data 2 and 3 and Supplementary Methods 4.1).\n\nLinear DNA fragments encoding the 96 designs were cloned into a plasmid encoding a C-terminal Strep-tag and used to transform Escherichia coli, and the resulting proteins were purified using Strep-tag affinity chromatography. Eighty-six out of ninety-six designs were expressed and soluble as judged by SDS–PAGE analysis of the eluants (Supplementary Fig. 4). Purified designs were supplemented with zinc sulfate, and hydrolysis of 4MU-PA was monitored by fluorescence. Five designs (A1, A8, B9, C4 and F7) had activity well above background (Fig. 2b and Supplementary Fig. 5). Sequence-verified single clones for each of these were expressed and purified by affinity chromatography followed by size-exclusion chromatography to obtain pure, monomeric protein fractions (Supplementary Figs. 6 and 7 and Supplementary Table 1). Michaelis–Menten kinetic characterization of the purified variants revealed a k cat /K M of 16,000 ± 2,000 M−1 s−1 for A1, the most active design, and k cat /K M values in the range of 35–140 M−1 s−1 for the other four designs (Fig. 2c,d, Extended Data Fig. 3 and Extended Data Table 1). For comparison, the k cat /K M of previously designed metallohydrolases24 ranged from 3 to 60 M−1 s−1 (Supplementary Table 2). A1 is also a relatively robust enzyme, and retains activity for at least 1,000 turnovers (Fig. 3e and Supplementary Fig. 8). A1 differs considerably from previously described proteins: the most similar structures identified through template modelling (TM) alignment with the Protein Data Bank (PDB) and AlphaFold Protein Structure Database (AFDB) have TM scores38 of 0.41 and 0.49, respectively, and do not have analogous arrangements of catalytic residues (Extended Data Fig. 4a,b). We refer to A1 as zinc metalloesterase 1 (ZETA_1) throughout the remainder of the text.\n\nFig. 2: Activity characterization and PLACER preorganization assessment. a, Design models of the most active designs. Sequence length (in amino acids (aa)) and the secondary structure harbouring each catalytic histidine are indicated below. b, Reaction progress curves. The dashed black line is the buffer background. c, Michaelis–Menten characterization of A1 (ZETA_1). The y axis shows the initial rate v 0 divided by the total enzyme concentration ([E] 0 ). d, Michaelis–Menten parameters of most active designs. e,f, Distribution of PLACER active site preorganization ensemble metrics for the ordered designs. Average design-prediction substrate r.m.s.d. (e) and average catalytic and binding residue design-prediction r.m.s.d. (f) across all predicted ensembles generated by PLACER for each design. g, For ZETA_1, the substrate position in PLACER ensembles is close to the design model, whereas in the inactive design H7, the substrate position fluctuates widely. h, For ZETA_1, the side chains surrounding the active site are largely fixed in positions close to the original design model, whereas in the inactive H8 design, the side-chain positions vary considerably. Note that only the first five randomly generated, unranked ensemble predictions are shown for PLACER in g,h. Data represent the mean ± s.d. of three independent measurements of initial velocity (c) and Michaelis–Menten parameters (d). Full size image\n\nFig. 3: Characterization of ZETA_1 activity. a, ZETA_1 design model (left) with close-up view of the active site showing the catalytic residues (middle) and a surface view of the designed pocket revealing high shape-complementarity to the substrate (right). b, Size-exclusion chromatogram of ZETA_1 showing a single peak corresponding to monomeric protein. c, Circular dichroism spectra of ZETA_1 recorded every 10 °C from 25 °C to 95 °C (viridis colour gradient), and after recooling to 25 °C (grey). The spectra suggest that ZETA_1 has an α-helical secondary structure and that it can refold after heating and partial unfolding. MRE, mean residue ellipticity. d, Circular dichroism signal at 222 nm measured every 1 °C and plotted as a function of temperature. e, [Product]:[enzyme] progress curve shows that ZETA_1 hydrolyses more than 1,000 4MU-PA molecules per enzyme. Note that the background reaction has been subtracted from the spectra so that every turnover can be attributed to the enzyme (Supplementary Fig. 8; further details in Supplementary Information, section 4.3). f, Reaction progress curves for the Zn(II) holo- and zinc-free apo ZETA_1 proteins showing zinc-dependent activity. Adding excess Zn(II) to the apo ZETA_1 sample after 30 min re-establishes the activity, demonstrating that zinc is essential for the catalytic mechanism of ZETA_1. WT, wild type. g, Zinc affinity of wild-type and mutant ZETA_1, measured as the dissociation constant (K D ), where a lower value indicates tighter binding. h, Fluorescence progress curves comparing the activity of wild-type and mutant ZETA_1. i, k cat /K M for active ZETA_1 mutants compared with the wild type. Data represent the mean ± s.d. of three independent measurements of turnover number (e), fluorescence progress curves (f,h), Zn(II)-binding dissociation constant (g) and Michaelis–Menten parameters (i). Full size image\n\nDesign ZETA_1 not only has remarkably high activity but was also the top-ranked design in our in silico ranking. The structure in the absence of substrate was predicted to be very close to the design model by AlphaFold2 (Extended Data Fig. 5a and Supplementary Figs. 9 and 10), and the designed active site of ZETA_1 was predicted to be highly preorganized by PLACER, with the catalytic side chains fixed in place and the substrate held closely in its designed position, adjacent to the proposed Zn(II) site. PLACER12 is a deep neural network that, given a protein backbone containing a substrate, fully randomizes the positions of the substrate and all side chains within a 600-atom sphere, and then generates new coordinates for these groups12; repeated PLACER trajectories generate an ensemble of possible side-chain conformations and small molecule docks. Design ZETA_1 stood out from the other designs in both the extent of catalytic site preorganization (the catalytic side chains were largely fixed in space in catalytically competent conformations) and the positioning of the substrate–transition state in the active site (in the ZETA_1 ensemble, the substrate remained largely fixed in space in the active site, whereas in the inactive designs H7 and H8, it fluctuated considerably) (Fig. 2e–h and Supplementary Videos 2–5). Seven designs based on the same ZETA_1 backbone family were initially filtered out during the design selection phase, as they had suboptimal PLACER metrics; we retrospectively expressed and purified these designs and found that they had very low or no activity, further highlighting the utility of PLACER ensemble calculations for identifying active designs (Supplementary Fig. 11). These findings suggest that combining global structure prediction with detailed PLACER modelling of the active site provides a powerful approach to assessing the catalytic machinery and substrate binding geometry for design selection (Supplementary Fig. 10).\n\nThe ZETA_1 active site consists of a primarily hydrophobic pocket with three histidines binding Zn(II) with their Nε atoms, an aspartate as a potential general base, and an asparagine that forms a hydrogen bond to the coumarin ring (Fig. 3a). As in the original theozyme model used to generate ZETA_1, the Zn(II) ion also acts as an oxyanion hole, stabilizing the developing negative charge at the transition state; there are no nearby side-chain hydrogen bond donors (Extended Data Fig. 5). Zinc is absolutely critical for ZETA_1 activity: extraction of bound Zn(II) by dialysis in the presence of the chelator 1,10-phenanthroline completely eliminated activity, and activity was subsequently restored by addition of zinc to the solution (Fig. 3f). Zinc titration experiments measured a dissociation constant (K D ) for Zn(II) of 41 ± 5 nM, which is similar to those of previous designed zinc enzymes26,39, but higher than native zinc hydrolases18,40,41,42, which typically have K D values less than 10 nM.\n\nWe carried out mutagenesis experiments to probe the contributions of the designed catalytic residues to Zn(II)-binding and catalysis (Fig. 3g–i and Supplementary Figs. 12–14). In the design model, N17 positions the substrate by hydrogen bonding with the lactone carbonyl of the coumarin moiety and could stabilize the developing negative charge on the leaving group; the N17A mutation led to a 8.1-fold decrease in k cat /K M (Supplementary Fig. 13). Mutation of all three metal-coordinating histidine residues to alanine simultaneously (H118A/H130A/H134A), as well as two of the three single histidine-to-alanine substitutions (H118A/H134A), completely inactivated the enzyme, as expected. Mutating the third Zn(II)-coordinating residue to alanine (H130A) resulted in a decrease of only 13-fold in k cat /K M , and mutation of the proposed general base D67 to alanine had little effect on k cat /K M and increased Zn(II)-binding affinity. These results suggest that H134/H118/H130 and H134/H118/D67 may be competing Zn(II)-binding sites owing to the close proximity of the coordinating side chains of H130 and D67, which was corroborated by Chai-1 (ref. 13) predictions of the protein–Zn(II)–substrate complex (Extended Data Fig. 5b,c); the D67A mutation may confine the zinc to the originally designed coordination sphere with the three histidines, which is more catalytically competent. In the H130A mutant, D67 is likely to coordinate Zn(II) and maintain binding, albeit in a less optimal binding geometry, lowering the zinc affinity and enzyme activity.\n\nGuided by these observations, we started from new DFT theozymes explicitly containing the catalytic base, and generated protein structures scaffolding these theozymes using a newer version of RFdiffusion2 trained from random weight initialization on a threefold-larger dataset (previous versions were fine-tuned from structure prediction weights) (Fig. 4a, Supplementary Data 1 and Supplementary Methods 4.2). Designs whose Chai-1 predictions of the protein–Zn(II)–substrate phosphonate ester complex, mimicking the reaction transition state, closely matched the design models with high confidence were identified by PLACER to have highly preorganized active sites (Supplementary Figs. 15 and 16). Ninety-six such designs spanning 37 RFdiffusion2-generated backbones were selected for experimental characterization (Supplementary Fig. 17 and Supplementary Data 2 and 3). Eighty-five of the 96 designs were expressed and soluble (Supplementary Fig. 18), and 11 designs spanning 3 different folds had substantial zinc-dependent 4MU-PA hydrolysis activity (Fig. 4b,c and Supplementary Fig. 19). Michaelis–Menten analysis revealed that 5 designs had a k cat /K M greater than 104 M−1 s−1 and 6 designs had a k cat /K M greater than 103 M−1 s−1 (Fig. 4d, Extended Data Fig. 6 and Extended Data Table 1). The most active designs for each backbone had a k cat /K M = 53,000 ± 5,000 M−1 s−1 (ZETA_2), k cat /K M = 19,000 ± 2,000 M−1 s−1 (ZETA_3), and k cat /K M = 1,100 ± 200 M−1 s−1 (ZETA_4) (Fig. 4f–h and Supplementary Fig. 20). ZETA_2 has a k cat = 1.5 ± 0.1 s−1, a threefold increase over the k cat of ZETA_1, and close to that of the metallohydrolase MID1sc10 obtained after 10 rounds of directed evolution24. RFdiffusion2 enables specification of the position of the substrate relative to the centre of mass of the designed protein; for ZETA_2 and ZETA_3, the protein was centred near the phenylacetate and 4-methylumbelliferyl moieties, respectively, of 4MU-PA, resulting in opposite substrate binding modes in the design models (that is, the 4-methylumbelliferyl is exposed in ZETA_2 and the phenylacetate is exposed in ZETA_3) (Extended Data Fig. 7).\n\nFig. 4: Characterization of second round designs. a, Second round DFT theozymes containing the catalytic base. Zn(II) is coordinated by the His Nε atoms in these theozymes, and thus C β is explicitly modelled. b, Reaction progress curves coloured by scaffold family. c, Sequence length and catalytic residue positioning for the top design in each scaffold, named ZETA_2-4. These designs differ from each other and from ZETA_1. d,e, Steady-state k cat /K M and k cat parameters for the 11 second round hits, with colours corresponding to scaffolds using the scheme in b,c. Activities are higher, on average, than in the first round of designs (blue). f–h, Design model (left) with close-up view of the active site (middle) for ZETA_2 (f), ZETA_3 (g) and ZETA_4 (h). Right, Michaelis–Menten plots and parameters. Data represent the mean ± s.d. of three independent measurements of Michaelis–Menten parameters (d,e) and initial velocity (f–h). Full size image\n\nThe success rate in the second design campaign was considerably higher than the first campaign (11 out of 96 versus 1 out of 96 designs with k cat /K M greater than 103 M−1 s−1), supporting the conclusions from the first round analysis (Supplementary Figs. 21–26, Supplementary Table 3, Supplementary Discussion 2 and Supplementary Methods 4.2). Circular dichroism experiments confirmed that all active enzyme scaffolds from both design campaigns possess secondary structures consistent with their design models, indicating proper folding (Supplementary Fig. 21). The structures of ZETA_1-4 are rather different from each other and previously known metallohydrolases (Extended Data Fig. 4). The sequence positions of the catalytic residues in each of these enzymes are also very different, highlighting the diversity of RFdiffusion2 generated design solutions (Fig. 4c and Supplementary Tables 4 and 5).\n\nWe determined the structure of ZETA_2, the most active design, in the apo state at 3.5 Å using X-ray crystallography (PDB: 9PYJ; Fig. 5). The experimental structure is in good agreement with the design model, with nearly superimposable backbones (Cα root mean squared deviation (r.m.s.d.) = 1.1 Å) and the catalytic residues preorganized in the designed geometry (Fig. 5a,b). The binding pocket is complementary to the superimposed transition state from the design model (Fig. 5c). We also solved a 2.1 Å structure after soaking ZETA_2 in Zn(II) (PDB: 9PYL; Extended Data Fig. 8); whereas the backbone was again nearly superimposable with the design model (Cα r.m.s.d. = 0.8 Å) and a Zn(II) ion was present with 100% occupancy at the designed location (r.m.s.d. = 1.7 Å), one of the Zn(II)-coordinating histidines (H110) was flipped out to interact with a Zn(II) ion bound at the surface of the protein, probably because of the high Zn(II) concentration in the crystal soaking buffer (250 mM) (Extended Data Fig. 8).",
      "url": "https://www.nature.com/articles/s41586-025-09746-w",
      "source": "Nature",
      "published": "2025-12-04",
      "sentiment_score": 0.9,
      "reasoning": "The article reports a significant breakthrough in enzyme design using generative AI (RFdiffusion2) to create highly active metallohydrolases with broad potential applications, including environmental pollutant degradation. The work demonstrates a novel, efficient method for de novo enzyme design with high catalytic activity and structural validation, representing a major advance in protein engineering with broad scientific and societal impact.",
      "category": "Technology",
      "personality_title": "AI designs powerful new enzymes to break down tough chemicals",
      "personality_presentation": "**Context** – Metallohydrolases are special enzymes that help break down very difficult chemical bonds using metal ions. Scientists want to create new versions of these enzymes to help clean up pollutants, but designing them from scratch has been very hard and slow.\n\n**What happened** – Researchers developed a new artificial intelligence (AI) method called RFdiffusion2. This AI can design enzymes by focusing only on the important parts of the enzyme that do the chemical work, without needing to know the whole protein structure in advance. Using this method, they created new zinc-based enzymes that break down a test chemical efficiently. They tested 96 designs, found several that worked well, and confirmed their shapes matched the design using advanced lab methods.\n\n**Impact** – This is the first time AI has designed such active and efficient enzymes from scratch without needing detailed protein information upfront. One enzyme, named ZETA_1, worked faster than previous designs and stayed active through many uses. This method can speed up making enzymes to tackle pollution or other chemical problems, showing a new way to build useful proteins faster and better.\n\n**What's next step** – Scientists plan to use this AI method to design enzymes for other important reactions, including breaking down different pollutants. They also want to improve the enzymes further and explore how this design approach can be used in medicine, industry, and environmental cleanup.\n\n**One-sentence takeaway** – A new AI tool has successfully designed powerful enzymes from scratch, opening the door to faster creation of proteins that can break down tough chemicals.\n",
      "personality_title_fr": "Une IA conçoit de nouvelles enzymes puissantes pour décomposer des substances difficiles",
      "personality_presentation_fr": "**Contexte** – Les métallohydrolases sont des enzymes qui cassent des liaisons chimiques très difficiles grâce à des ions métalliques. Les scientifiques souhaitent créer de nouvelles versions pour nettoyer les polluants, mais leur conception est complexe et lente.\n\n**Ce qui s'est passé** – Des chercheurs ont développé une nouvelle méthode d'intelligence artificielle (IA) appelée RFdiffusion2. Cette IA conçoit des enzymes en se concentrant uniquement sur les parties importantes pour la réaction chimique, sans connaître toute la structure de la protéine à l'avance. Grâce à cette méthode, ils ont créé des enzymes à base de zinc très efficaces pour décomposer une molécule test. Sur 96 designs, plusieurs ont bien fonctionné, et leur structure a été confirmée en laboratoire.\n\n**Impact** – C'est la première fois qu'une IA conçoit des enzymes aussi actives et efficaces sans informations détaillées préalables sur la protéine. Une enzyme, nommée ZETA_1, est plus rapide que les précédentes et reste active après de nombreuses utilisations. Cette méthode accélère la création d'enzymes pour traiter la pollution ou d'autres problèmes chimiques.\n\n**Prochaine étape** – Les scientifiques veulent utiliser cette IA pour concevoir des enzymes pour d'autres réactions importantes, notamment pour dégrader différents polluants. Ils souhaitent aussi améliorer ces enzymes et explorer leur utilisation en médecine, industrie et environnement.\n\n**Message clé en une phrase** – Un nouvel outil d'IA a conçu avec succès des enzymes puissantes à partir de zéro, ouvrant la voie à une création plus rapide de protéines capables de décomposer des substances difficiles.\n",
      "personality_title_es": "Una IA diseña nuevas enzimas potentes para descomponer químicos difíciles",
      "personality_presentation_es": "**Contexto** – Las metalohidrolasas son enzimas que rompen enlaces químicos muy difíciles usando iones metálicos. Los científicos quieren crear nuevas versiones para limpiar contaminantes, pero diseñarlas desde cero es complicado y lento.\n\n**Qué pasó** – Investigadores desarrollaron un nuevo método de inteligencia artificial (IA) llamado RFdiffusion2. Esta IA diseña enzimas enfocándose solo en las partes clave para la reacción química, sin necesitar saber toda la estructura de la proteína antes. Con este método, crearon enzimas con zinc que descomponen eficientemente un químico de prueba. De 96 diseños, varios funcionaron bien y confirmaron sus estructuras en el laboratorio.\n\n**Impacto** – Es la primera vez que una IA diseña enzimas tan activas y eficientes sin información detallada previa de la proteína. Una enzima llamada ZETA_1 fue más rápida que diseños anteriores y mantuvo su actividad tras muchos usos. Este método puede acelerar la creación de enzimas para tratar contaminación y otros problemas químicos.\n\n**Próximo paso** – Los científicos planean usar esta IA para diseñar enzimas para otras reacciones importantes, incluyendo la degradación de distintos contaminantes. También buscarán mejorar las enzimas y explorar su uso en medicina, industria y limpieza ambiental.\n\n**Mensaje en una frase** – Una nueva herramienta de IA diseñó con éxito enzimas potentes desde cero, abriendo la puerta a crear proteínas que pueden descomponer químicos difíciles más rápido.\n",
      "image_url": "public/images/news_image_Computational-design-of-metallohydrolases.png",
      "image_prompt": "An intricate, warm-toned painting of a glowing zinc ion cradled by three stylized histidine-shaped branches within an elegant, abstract protein scaffold that gently unfolds around a luminous, tetrahedral-shaped water molecule poised to cleave a delicate ester bond, all rendered with soft natural colors and fine, detailed brushwork evoking the precision and harmony of enzyme design."
    },
    {
      "title": "Sterile-neutrino search based on 259 days of KATRIN data",
      "summary": "Nature, Published online: 03 December 2025; doi:10.1038/s41586-025-09739-9The analysis of the energy spectrum of 36 million tritium β-decay electrons recorded in 259 measurement days within the last 40 eV below the endpoint challenges the Neutrino-4 claim.",
      "content": "Experimental setup\n\nThe KATRIN experiment measures the electron energy spectrum of tritium β-decay near the kinetic endpoint E 0 ≈ 18.6 keV. The 70-m-long setup comprises a high-activity gaseous tritium source, a high-resolution spectrometer using the MAC-E-filter principle and a silicon p-i-n diode detector2,30.\n\nMolecular tritium gas with high isotopic purity (up to 99%; ref. 48) is continuously injected into the middle of the WGTS, in which it streams freely to both sides. At the ends of the 10-m-long beam tube, more than 99% of the tritium gas is pumped out using differential pumping systems. With a throughput of up to 40 g day−1, an activity close to 100 GBq can be achieved49,50. The β-electrons are guided adiabatically by a 2.5 T magnetic field through the WGTS51. Before entering the spectrometer, they traverse two chicanes, in which the residual tritium flow is reduced by more than 12 orders of magnitude through differential52 and cryogenic pumping53. The β-electrons flowing in the upstream direction of the beamline reach the gold-plated rear wall, where they are absorbed. Besides separating the WGTS from the rear part of the KATRIN experiment that houses calibration tools, the rear wall also controls the source potential by a tunable voltage of \\({\\mathcal{O}}\\) (100 mV).\n\nElectrons guided towards the detector are subjected to energy filtering by two spectrometers. A smaller pre-spectrometer first rejects low-energy β-particles. The precise energy selection with \\({\\mathcal{O}}\\) (1 eV) resolution is then performed by the 23-m-long and 10-m-wide main spectrometer. In both spectrometers, only electrons with a kinetic energy above the threshold energy qU are transmitted (high-pass filter). The electron momenta \\({\\bf{p}}={{\\bf{p}}}_{\\perp }+{{\\bf{p}}}_{\\parallel }\\) are collimated adiabatically such that the transverse momentum is reduced to a minimum towards the axial filter direction by gradually decreasing the magnetic field towards the so-called analysing plane. In the main spectrometer, the magnetic field is reduced by four orders of magnitude to B ana ≲ 6.3 × 10−4 T. Behind the main spectrometer exit, the magnetic field is increased to its maximum value of B max = 4.2 T, resulting in a maximum acceptance angle of \\({\\theta }_{\\max }=\\arcsin (\\sqrt{{B}_{{\\rm{S}}}/{B}_{\\max }})\\approx 5{1}^{^\\circ }\\), where θ is the initial pitch angle of the electron. The filtered electrons are counted by the FPD, which is a silicon p-i-n diode segmented into 148 pixels (ref. 40). This detector is regularly calibrated with a 241Am source to ensure stable performance. Its efficiency is about 95%, with only small variations between pixels that remain constant over time. Effects that do not depend on the retarding potential are accounted for by the free normalization combined fit across groups or patches of pixels54.\n\nBackground electrons are indistinguishable from tritium β-decay electrons and thus contribute to the overall count rate at the detector. There are different sources and mechanisms that generate the background events. The majority originates from the spectrometer section of the experiment. Secondary electrons are created by cosmic muons and ambient gamma radiation on the inner spectrometer surface55,56 but are mitigated by magnetic shielding and a wire electrode system30. The decay of 219Rn and 220Rn inside the main spectrometer volume is reduced by cryogenic copper baffles that are installed in the pumping ducts of the main spectrometer57. Another source of background is electrons from radioactive decays produced in the low-magnetic-field part of the spectrometer58. These primary electrons can be trapped magnetically, ionizing residual-gas molecules and producing secondary electrons that are correlated in time, leading to a background component with a non-Poisson distribution. The dominating part of the background stems from the ionization of neutral atoms in highly excited states, which enter the main spectrometer volume in sputtering processes (decay of residual 210Pb) at the inner surface of the spectrometer. The low-energy electrons emitted in this process are accelerated to signal-electron energies and guided to the detector. The magnitude of this background component scales with the flux tube volume in the re-acceleration part of the main spectrometer. A re-configuration of the electromagnetic fields in the main spectrometer, called the SAP setting, reduces the background by a factor of 2 by shifting the plane of minimal magnetic field from the nominal position in the centre of the spectrometer towards the detector while compressing the flux tube at the same time42,59. After successful tests, this configuration was set as the new standard (see the next section). There is also the possibility of a Penning trap being formed between the pre- and main spectrometer. The stored particles can increase the background rate. To counter this effect, a conductive wire is inserted between scan steps into the beam tube to remove the stored particles41. Because the duration of the scan steps differs, this creates a scan-time-dependent background. Reducing the time between particle removal events and lowering the pre-spectrometer potential enabled a full mitigation of the scan-step-duration-dependent background starting with the KNM5 measurement period. Moreover, the transmission and detection probabilities of background electrons may slightly depend on the retarding-potential setting, potentially causing a retarding-potential-dependent background. This effect is addressed in the analysis through an additional slope component, constrained by dedicated background measurements31.\n\nKNM12345 dataset\n\nThe data collected by KATRIN is organized into distinct measurement periods, referred to as KATRIN Neutrino Measurement (KNM) campaigns. The integral β-spectrum is measured through a sequence of defined retarding-energy set points, which we refer to as a scan. Each dataset comprises several hundred β-spectrum scans, with individual scan durations ranging from 125 min to 195 min. The measurement time distribution (MTD), shown in Extended Data Fig. 1 (bottom), determines the time allocated for each qU i , optimized for maximizing sensitivity to a neutrino-mass signal, where the index i corresponds to different retarding energy settings. The energy interval typically spans E 0 − 300 eV ≤ qU i ≤ E 0 + 135 eV, following an increasing, decreasing or random sequence. The analysis presented in this work uses set points ranging from E 0 − 40 eV to E 0 + 135 eV and is based on the first five measurement campaigns (KNM1–KNM5), summarized in Extended Data Table 1. Extended Data Fig. 2 shows the electron energy spectra for each of the five individual campaigns.\n\nThe first measurement campaign of the KATRIN experiment, KNM1, started in April 2019 with a relatively low source-gas density of ρd = (1.08 ± 0.01) × 1021 m−2 compared with the design value of ρd = 5 × 1021 m−2. Here, ρ denotes the average density of the source gas, and d = 10 m is the length of the tritium source. The source was operated at a temperature of 30 K. The measurement in KNM1 lasted for 35 days, recording a total of about 2 million electrons. The sterile-neutrino analysis of this dataset was published in ref. 33.\n\nAfter a maintenance break, the next campaign, KNM2, started in October of the same year and lasted for 45 days, only 10 days longer than KNM1. However, more than double the number of electrons was measured because of the increased source-gas density. The density was raised to ρd = (4.20 ± 0.04) × 1021 m−2, which corresponds to 84% of the design value. Although the background rate was reduced from 0.29 count per second (cps) in KNM1 to 0.22 cps, it was still above the anticipated design value of 0.01 cps (ref. 2). The sterile-neutrino analysis of this dataset, in combination with KNM1, was published in ref. 32.\n\nTo further reduce the background rate, a new electromagnetic-field configuration42 was tested in the next campaign, starting in June 2020. KNM3 was split into two parts to validate the new shifted-analysing-plane (SAP) setting in contrast to the nominal (NAP) setting. Before the start of the measurement, the source temperature was increased to 79 K. This allowed the co-circulation of gaseous krypton 83mKr with the tritium gas to perform simultaneous calibration measurements and β-scans60,61. In the first part of KNM3, KNM3-SAP, the β-spectrum was measured in the SAP setting for 14 days, and the background was subsequently reduced to 0.12 cps. Although the SAP setting reduced the background almost by a factor of 2, the increased inhomogeneities in the magnetic and electric fields require the segmentation of the detector analysis into 14 patches with 14 individual models59. In KNM3-NAP, it was demonstrated that switching between both settings works, and the background rate increased to 0.22 cps, as expected. The second part also lasted 14 days, and about 2.5 million electrons were measured in all of KNM3.\n\nBefore launching a new measurement campaign, a target source-gas density of ρd = 3.8 × 1021 m−2 was chosen to keep the same experimental conditions for future measurements. With the start of KNM4 in September 2020, one of the challenges was to reduce the background caused by charged particles accumulated in the Penning trap between the main and pre-spectrometer. The accumulation was time dependent, as the trap was emptied after each qU i . To reduce this background, the run durations were shortened to empty the trap more often. Finally, the Penning-trap effect was eliminated by lowering the pre-spectrometer voltage. Furthermore, the MTD was optimized during the campaign to increase the neutrino-mass sensitivity. This sequence defines the split into the first part, KNM4-NOM, and the second part, with an optimized MTD, KNM4-OPT. In all of KNM4, more than 10 million electrons were recorded in 79 measurement days.\n\nThe final dataset used in this analysis is KNM5, which started in April 2021. It contains more than 16 million electrons recorded in 72 measurement days. Before the start of the measurement, the rear wall was cleaned of its residual tritium with ozone. This step was necessary to address the accumulation of tritium on the rear wall, which produced an additional spectrum on top of the tritium β-spectrum, introducing new systematic uncertainties31.\n\nModel description\n\nGeneral model\n\nThe KATRIN experiment measures the neutrino mass by analysing the distortions near the endpoint of the tritium β-decay spectrum. The model used for this analysis is described in the main text and follows the formalism outlined in equations (2) and (3). It includes the theoretical β-spectrum, the experimental response function and parameters such as the active-neutrino mass squared \\({m}_{{\\rm{\n\nu }}}^{2}\\), sterile-neutrino mass squared \\({m}_{4}^{2}\\) and the mixing parameter sin2(θ ee ). The simulated imprint of a fourth mass eigenstate, with a mass of 10 eV and mixing of 0.05, is shown in Extended Data Fig. 1. To address the computational challenge posed by 1,609 data points requiring \\({\\mathcal{O}}(1{0}^{3})\\) evaluations of \\({R}_{{\\rm{model}}}({A}_{{\\rm{S}}},{E}_{0},{m}_{{\\rm{\n\nu }}}^{2},{m}_{4}^{2},{\\theta }_{{\\rm{ee}}},q{U}_{i})\\) per minimization step, an optimized direct-model calculation62 and a neural-network-based fast model prediction63 were used, as detailed in sections ‘The KaFit analysis framework’ and ‘The Netrium analysis framework’, respectively.\n\nThe KaFit analysis framework\n\nKaFit (KATRIN Fitter) is a C++-based fitting tool developed for the KATRIN data analysis. It is part of KASPER—the KATRIN Analysis and Simulations Package64. KaFit uses the MIGRAD algorithm from the MINUIT numerical minimization library65 to fit KATRIN data. The fit minimizes the likelihood function −2log(L) in equation (4) with respect to parameters of interest such as the squared active-neutrino mass \\({m}_{{\\rm{\n\nu }}}^{2}\\), \\({m}_{4}^{2}\\), sin2(θ ee ) and nuisance parameters Θ (ref. 62).\n\nModel evaluation is done by the Source Spectrum Calculation (SSC) module, which returns the predicted electron counts for a given set of input parameters. Within SSC, the integrated β-decay spectrum is calculated by numerically integrating the differential spectrum after convolving it with the experimental response function, as in equation (3). Using the measurement times and the experimental parameters, the SSC module computes the prediction for the number of counts for each retarding energy qU i .\n\nKaFit computes the negative logarithm of likelihood using the experimental and model electron counts. Twice the negative log-likelihood is referred to as χ2 in equation (4). KaFit minimizes the χ2 and returns the best-fit parameters and the corresponding \\({\\chi }_{\\min }^{2}\\). A typical minimization requires \\({\\mathcal{O}}(1{0}^{4})\\) evaluations of the χ2 function. For computational efficiency, KaFit and SSC make use of multiprocessor computing. The recalculation of all components of the spectrum model at each minimization step is avoided through caching, thereby reducing the overall minimization time by a factor of approximately \\({\\mathcal{O}}(1{0}^{3})\\). Moreover, caching the response function f(E, qU i ) (equation (3)) and reusing it across systematic studies further reduces redundant calculations, enhancing the overall efficiency. KaFit has been optimized to enhance the computational speed for sterile-neutrino analysis while maintaining sufficient precision for physics searches within the current statistical and systematic budget.\n\nThe Netrium analysis framework\n\nIn the second approach to perform the analysis, a software framework called Netrium63 is used. In this case, the KATRIN physics model is approximated in a fast and highly accurate way using a neural network. It can predict the model spectrum \\({R}_{{\\rm{model}}}({A}_{{\\rm{S}}},{E}_{0},{m}_{{\\rm{\n\nu }}}^{2},{m}_{4}^{2},{\\theta }_{{\\rm{ee}}},q{U}_{i})\\) (equation (3)) for all main input parameters and improves the computational speed by about three orders of magnitude with respect to the numerical-model calculation. The neural network features an input and an output layer with two fully connected hidden layers in between. During the training process, \\({\\mathcal{O}}(1{0}^{6})\\) sample spectra, calculated using the analytical model, are used to optimize the weights of the neural network.\n\nComparison\n\nCross-validation between the two different analysis approaches to calculate the model is a key component of the blinding strategy of KATRIN (see the next section) and serves as quality assurance for the analysis. In previous sterile-neutrino searches with the KATRIN experiment32,33, the two described analysis approaches were benchmarked with the analysis software Simulation and Analysis with MATLAB for KATRIN (Samak)66, not used for the current analysis. It is based on the covariance matrix approach and designed to perform high-level analyses of tritium β-spectra measured by the KATRIN experiment. For this comparison, the results of the combined sterile analysis, based on data from the first five measurement campaigns, are shown in Extended Data Fig. 3. The active-neutrino mass is set to \\({m}_{{\\rm{\n\nu }}}^{2}=0\\,{{\\rm{eV}}}^{2}\\). An excellent agreement is achieved for the exclusion contour and the best-fit parameter between the analyses conducted with KaFit and Netrium.\n\nBlinding strategy and unblinding process\n\nKATRIN applies a rigorous blinding scheme to minimize human bias and ensure the integrity of the analysis process. This approach also enforces robust software validation to avoid potential programming errors. The benchmark for validation is established by fixing the active squared neutrino mass to \\({m}_{{\\rm{\n\nu }}}^{2}=0\\,{{\\rm{eV}}}^{2}\\) in all steps of the analysis.\n\nThe process began with the generation of twin Asimov datasets for each measurement campaign, in which all input values are set to their expected means67. These datasets were independently evaluated by two teams using the KaFit and Netrium software frameworks. Cross-validations confirmed an agreement within a few per cent of sin2(θ ee ) between the results from the two teams. Only after this validation step was the combined analysis of all twin Asimov datasets performed. Once the validation using the Asimov data was completed, the real data were analysed on a campaign-by-campaign basis. Again, results from both teams were compared to ensure consistency. Only when the campaign-wise results demonstrated agreement did the analysis proceed to the final step, in which the complete, unblinded dataset was evaluated.\n\nDuring the sterile-neutrino analysis of the KNM4 campaign, a closed contour with 99.98% CL was unexpectedly identified. This result was anomalous, given that all other campaigns, contributing the bulk of the statistics, consistently yielded open contours. The anomaly prompted a temporary suspension of the unblinding procedure and initiated a detailed technical investigation. The investigation uncovered a technical error in the combination of sub-campaigns within KNM4. Specifically, data from periods with different MTDs and effective endpoints cannot be directly stacked together. To address this, the dataset was divided into two distinct periods: KNM4-NOM and KNM4-OPT, reflecting the respective sub-campaign configurations. Apart from correcting the data-combination process, the analysis framework was improved with updated inputs and enhanced validation checks to prevent similar issues in the future. The identification of this error during the sterile-neutrino search also led to a re-evaluation of the neutrino-mass analysis, which was based on the same dataset. Detailed descriptions of all modifications and their impact can be found in ref. 31.\n\nResults for individual campaigns\n\nThere are seven individual datasets (with KNM3 and KNM4 each split into two), corresponding to the first five measurement campaigns (KNM1–KNM5), which are described in detail in the section ‘KNM12345 dataset’. Each of the datasets can be used separately to search for a sterile neutrino. For campaigns measured in the NAP setting, the counts of all active pixels are summed up to one spectrum. By contrast, datasets in the SAP setting feature a patch-wise data structure (14 patches), in which, for each patch, nine pixels with similar electromagnetic fields are grouped together. Therefore, each dataset in the SAP setting contains 14 different spectra.\n\nA sterile-neutrino search using the grid-scan method is applied to the seven individual datasets, whose exclusion contours at 95% CL are shown in Extended Data Fig. 4. The differences in the accumulated signal statistics and their variations lead to different excluded regions in the sterile parameter space. The overall shape of the exclusion contours is very similar for all of the campaigns. The KNM4-NOM campaign enables the exclusion of a more extensive region of the sterile-neutrino parameter space associated with high sterile-neutrino masses; for low sterile-neutrino masses, the KNM5 campaign dominates.\n\nResults for combined campaigns\n\nAcross the seven campaigns, a total of 68,237 scan steps were recorded, collecting approximately 36 million counts within the analysis window. As described in the section ‘KNM12345 dataset’, data were grouped primarily into seven sets according to the experimental conditions and FPD pixel configurations. Within each dataset, electrons collected at a particular retarding potential were summed over all the scans. This is possible given the ppm-level (10−6) high-voltage reproducibility of the main spectrometer68. For each of the NAP datasets (KNM1, KNM2 and KNM3-NAP), the counts from all the pixels were summed up to obtain datasets that feature counts per retarding potential. For the SAP datasets (KNM3-SAP, KNM4-NOM, KNM-OPT and KNM5), the detector pixels were grouped into 14 ring-like detector patches of nine pixels each, to account for variations in the electric potential and magnetic fields across the analysing plane42. Pixel grouping was determined based on transmission similarity. For each of the SAP datasets, counts per patch per retarding potential were obtained by summing over all scans and grouping pixels. These so-called merged datasets, containing 1,609 data points, were used in the sterile-neutrino analysis.\n\nThe combined data were analysed using the following definition of χ2:\n\n$$\\begin{array}{l}{\\chi }^{2}({\\rm{\\theta }})\\,=\\,-\\,2lnL({\\rm{\\theta }})\\\\ \\,\\,\\,\\,=\\,-\\,2ln\\mathop{\\prod }\\limits_{i=0}^{I}f\\left(\\begin{array}{l}{N}_{{\\rm{obs}},i}\\,| \\\\ {N}_{{\\rm{theo}}}(q{U}_{i},{\\rm{\\theta }})\\end{array}\\right)\\\\ \\,\\,\\,\\,=\\,-\\,2\\,\\mathop{\\sum }\\limits_{i=0}^{I}ln\\left(f\\left(\\begin{array}{l}{N}_{{\\rm{obs}},i}\\,| \\\\ {N}_{{\\rm{theo}}}(q{U}_{i},{\\rm{\\theta }})\\end{array}\\right)\\right),\\end{array}$$ (4)\n\nwhere θ is the vector of fit parameters, I denotes the total number of data points or retarding energy setpoints qU i , N obs,i is the observed count at the ith setpoint, N theo (qU i , θ) is the theoretically predicted count at the ith setpoint and \\(f({N}_{{\\rm{obs}},{\\rm{i}}}| {N}_{{\\rm{theo}}}(q{U}_{i},{\\rm{\\theta }}))\\) represents the likelihood function, modelled using a Poisson distribution. The θ parameter values were inferred by minimizing χ2(θ).\n\nFor the combined KNM1–KNM5 analysis, further merging of the datasets is not possible because of the significant differences in the experimental conditions in which they were measured. The combined χ2 function incorporates contributions from both Gaussian and Poissonian likelihoods. Negative log-likelihoods of Gaussian models (\\({\\chi }_{{\\rm{G}}}^{2}\\)) were used for the NAP campaigns as the mean value of the counts summed over all pixels was large, allowing the counts to be modelled by a Gaussian distribution31. Negative log-likelihoods of Poissonian models (\\({\\chi }_{{\\rm{P}}}^{2}\\)) were applied to the SAP campaigns, as the mean value of the counts summed over pixels in each patch was not large enough. The combined χ2 function is expressed as\n\n$$\\begin{array}{l}{\\chi }_{{\\rm{comb}}}^{2}(\\Theta )\\,=\\,\\sum _{k\\in {{\\mathcal{I}}}_{{\\rm{G}}}}{\\chi }_{{\\rm{G}},k}^{2}\\left(\\begin{array}{l}{m}_{{\\rm{\n\nu }}}^{2},\\,{m}_{4}^{2},\\,{\\sin }^{2}{\\theta }_{{\\rm{ee}}},\\\\ {E}_{0k},\\,{{\\rm{Sig}}}_{k},\\,{{\\rm{Bg}}}_{k},\\,\\xi \\end{array}\\right)\\\\ \\,\\,\\,\\,\\,\\,+\\,\\sum _{l\\in {{\\mathcal{I}}}_{{\\rm{P}}}}\\mathop{\\sum }\\limits_{p=0}^{13}{\\chi }_{{\\rm{P}},lp}^{2}\\left(\\begin{array}{l}{m}_{{\\rm{\n\nu }}}^{2},\\,{m}_{4}^{2},\\,{\\sin }^{2}{\\theta }_{{\\rm{ee}}},\\\\ {E}_{0lp},\\,{{\\rm{Sig}}}_{lp},\\,{{\\rm{Bg}}}_{lp},\\,\\xi \\end{array}\\right)\\\\ \\,\\,\\,\\,\\,\\,+\\,{(\\widehat{\\xi }-\\xi )}^{{\\rm{T}}}{\\varSigma }^{-1}(\\widehat{\\xi }-\\xi ).\\end{array}$$ (5)\n\nIn this formulation, Θ is the vector of the physics parameters and the nuisance parameters of all the campaigns. Sig k and Sig lp correspond to A S in equation (3) and Bg i and Bg jp represent the energy-independent background rate \\({R}_{{\\rm{bg}}}^{{\\rm{base}}}\\). Correlations among nuisance parameters ξ were captured by the covariance matrix Σ, with the mean values \\(\\widehat{\\xi }\\) determined from systematic measurements. In equation (5), \\({{\\mathcal{I}}}_{{\\rm{G}}}=1,2,3\\text{-NAP}\\) represents the KNM-NAP campaigns modelled with Gaussian likelihoods, whereas \\({{\\mathcal{I}}}_{{\\rm{P}}}=\\text{3-SAP, 4-NOM, 4-OPT, 5}\\) corresponds to the KNM-SAP campaigns modelled with Poissonian likelihoods. The exclusion bounds in Extended Data Fig. 5 compare results from the first two campaigns (KNM1–KNM2) to all five campaigns (KNM1–KNM5). The analysis highlights how increased statistics significantly improve exclusion limits across all \\({m}_{4}^{2}\\) values.\n\nSystematic uncertainty\n\nAn accurate and detailed accounting of the systematic uncertainties is essential for obtaining robust estimates of \\({m}_{4}^{2}\\) and sin2(θ ee ) from the tritium β-spectrum. Several systematic effects arise within the KATRIN beamline, influencing the shape of the measured spectrum. The systematic effects and inputs considered for this analysis are the same as in the neutrino-mass analysis31 and are treated as nuisance parameters. The central values \\(\\widehat{\\xi }\\) and covariance Σ of the systematics parameters were determined from calibration measurements and simulations. To include these effects in the likelihood function (equation (4)), they are modelled as a multivariate normal distribution \\({\\mathcal{N}}(\\widehat{\\xi },\\varSigma )\\). Subsequently, the likelihood function is multiplied by the normal distribution of each systematic effect. It decreases the likelihood function if the parameters deviate from their mean values \\(\\widehat{\\xi }\\).\n\nTo assess the impact of individual systematic uncertainties, a separate scan of the sterile parameter space is performed for each systematic effect, considering only the statistical uncertainty and the specific systematic uncertainty in each case. This is done on the simulated Asimov twin data with \\({m}_{{\\rm{\n\nu }}}^{2}=0\\,{{\\rm{eV}}}^{2}\\). The impact of individual systematic uncertainties on the sensitivity contour is quite small. Therefore, to evaluate the influence of systematic uncertainties more quantitatively, a raster scan is performed. In this approach, for each fixed value of \\({m}_{4}^{2}\\) in the parameter grid, the 1σ sensitivity on the mixing sin2(θ ee ) is calculated independently. By performing a raster scan, the number of degrees of freedom is effectively reduced to one at each grid point. The contribution of each systematic effect to the total uncertainty is determined using \\({\\sigma }_{{\\rm{syst}}}=\\sqrt{{\\sigma }_{{\\rm{stat}}+{\\rm{syst}}}^{2}-{\\sigma }_{{\\rm{stat}}}^{2}}\\). The raster scan results for the combined KNM1–KNM5 dataset are presented in Extended Data Fig. 6 for three values of \\({m}_{4}^{2}\\) as well as for all values of \\({m}_{4}^{2}\\) in Extended Data Fig. 7. We can see that the statistical uncertainty dominates over all systematic effects. Owing to increasing statistics at lower retarding energies, the ratio of statistical to systematic uncertainties depends on the value of \\({m}_{4}^{2}\\). Source-related uncertainties dominate the overall systematic uncertainty. The largest impact stems from the uncertainty on the density of the molecular tritium gas in the source column. It is followed by the uncertainty of the energy-loss function, which describes the probability that electrons will lose a certain amount of energy when scattering with the molecules in the source. Depending on the value of \\({m}_{4}^{2}\\), variations in the source potential and the scan-time-dependent background also significantly affect the overall systematic uncertainty. The uncertainty from the scan-step-duration-dependent background is the first contribution of a non-source-related systematic effect.\n\nFinal-state distribution systematics\n\nOne source of uncertainty affecting the shape of the β-electron spectrum in KATRIN arises from the final-state distribution (FSD) of molecular tritium. During the decay process, part of the available energy can excite the tritium molecule into rotational, vibrational (ro-vibrational) and electronic states, thereby reducing the energy transferred to the emitted electron. Consequently, the energy distribution of these states directly influences the differential β-decay spectrum used in the analysis.\n\nIn previous KATRIN campaigns, the systematic contribution of the FSD uncertainty was estimated by comparing two ab initio calculations—one from ref. 34 and an earlier version from ref. 69—leading to a rather conservative estimation. For the analysis presented here, the FSD-related uncertainty was re-evaluated following the procedure derived for the neutrino-mass analysis36, which allows the estimation of the impact of FSD on the total uncertainty by considering the details of the calculation of the final-states distribution, such as the validity of theoretical approximations and corrections, the uncertainty of experimental inputs and physical constants, and the convergence of the calculation itself.\n\nThe same FSDs used in the neutrino-mass analysis, generated by perturbing the nominal set of input parameters entering the calculation, were used to create the twin Asimov datasets of the null hypothesis (no sterile neutrinos). Each dataset was then fitted with Netrium, trained on a single nominal FSD, with different values of m 4 and sin2(θ ee ). By comparing the resulting sensitivity contours at the 95% CL (\\(\\Delta {\\chi }_{{\\rm{crit}}}^{2}=5.99\\)) with that from the nominal case, the impact of FSD uncertainties can be quantified.\n\nThis analysis was performed using the KNM5 campaign alone, due to differences between the models used to generate FSDs in different campaigns. Nonetheless, the result of this analysis, presented in Extended Data Fig. 8, demonstrates that the various FSD effects have a negligible impact on the sensitivity contours, justifying the use of the nominal FSD for fitting with sufficient accuracy.\n\nApplicability of Wilks’ theorem\n\nThe contours and their associated confidence regions reported in earlier sections were based on the critical threshold values \\(\\Delta {\\chi }_{{\\rm{crit}}}^{2}\\) corresponding to a desired CL. The sterile-neutrino search uses the Δχ2 test statistic:\n\n$$\\Delta {\\chi }^{2}={\\chi }^{2}({H}_{0})-{\\chi }^{2}({H}_{1}),$$ (6)\n\nwhere H 0 is the assumed truth for the sterile-neutrino mass and mixing angle \\([{m}_{4}^{2},{\\sin }^{2}({\\theta }_{{\\rm{ee}}})]\\), and H 1 represents the global best-fit hypothesis based on the observations. According to Wilks’ theorem44, as H 0 represents a special case of H 1 , Δχ2 will follow a chi-squared distribution with two degrees of freedom in the large sample limit. Wilks’ theorem thus provides the critical threshold value \\(\\Delta {\\chi }_{{\\rm{crit}}}^{2}\\) corresponding to a desired CL, which allows us to quickly determine whether H 0 is compatible with H 1 . This simplifies the sterile-neutrino analysis by providing a critical threshold \\(\\Delta {\\chi }_{{\\rm{crit}}}^{2}=5.99\\), which corresponds to a 5% probability of obtaining such a deviation purely because of random fluctuations under the null hypothesis. Thus, Δχ2 > 5.99 indicates a sterile-neutrino signal at 95% CL.\n\nIf Wilks’ theorem were not applicable, the cumulative distribution function (CDF) of the Δχ2 test statistic would need to be computed for multiple combinations of the sterile-neutrino mass and mixing angle \\([{m}_{4}^{2},{\\sin }^{2}({\\theta }_{{\\rm{ee}}})]\\) using Monte Carlo simulations, which would require several thousand times more computational effort. To ensure the applicability of Wilks’ theorem to the analysis, the CDF of Δχ2 is numerically validated by performing calculations using Monte Carlo simulations for two sets of parameters. In the first case, \\([{m}_{4}^{2}=0,{\\sin }^{2}({\\theta }_{{\\rm{ee}}})=0]\\) corresponds to the null hypothesis, whereas in the second case \\([{m}_{4}^{2}=55.66\\,{{\\rm{e}}{\\rm{V}}}^{2},{\\sin }^{2}({\\theta }_{{\\rm{e}}{\\rm{e}}})=0.013]\\) represents the best fit for the combined KNM1–KNM5 dataset. The CDFs and \\(\\Delta {\\chi }_{{\\rm{crit}}}^{2}\\) values obtained through Monte Carlo simulations were compared with those predicted by Wilks’ theorem.\n\nFor each of the sterile-neutrino mass and mixing-angle pairs, \\({\\mathcal{O}}(1{0}^{3})\\) tritium β-decay spectra were generated by adding Poissonian fluctuations to the counts calculated using the model described in the section ‘General model’. For each spectrum, the grid-scan method described in the main text is used to find the best-fit point and to compute Δχ2 using equation (6). For the case with \\({m}_{4}^{2}=0\\,{{\\rm{eV}}}^{2}\\) and sin2(θ ee ) = 0, the numerically computed or empirical CDF closely follows the analytical CDF of the chi-squared distribution, as shown in Extended Data Fig. 9. Similarly, in the scenario with \\({m}_{4}^{2}=55.66\\,{{\\rm{eV}}}^{2}\\) and sin2(θ ee ) = 0.013, the empirical CDF also aligns closely with the analytical CDF. In both cases, the observed threshold values at the 95% CL are consistent with the theoretical expectation of Δχ2 = 5.99. Specifically, for \\({m}_{4}^{2}=0\\,{{\\rm{eV}}}^{2}\\) and sin2(θ ee ) = 0, the critical Δχ2 at 95% CL is 6.07 ± 0.17, and for \\({m}_{4}^{2}=55.66\\,{{\\rm{eV}}}^{2}\\) and sin2(θ ee ) = 0.013, it is 6.07 ± 0.20. The uncertainties on the threshold values are calculated using the bootstrapping method. The combination of the CDF and the threshold values confirms the applicability of Wilks’ theorem to the sterile-neutrino analysis described in the main text.\n\nResults compared with expected sensitivity\n\nFigure 3 shows that the sensitivity contour, derived from simulations, intersects with the exclusion contour, obtained from the KNM1–KNM5 experimental data. For \\(\\Delta {m}_{41}^{2} < 30\\,{{\\rm{e}}{\\rm{V}}}^{2}\\), the exclusion contour extends beyond the sensitivity contour, whereas at higher \\(\\Delta {m}_{41}^{2}\\) values, the exclusion contour oscillates around the sensitivity contour.\n\nFrom the methodology outlined in the previous section and the description of \\({\\mathcal{O}}(1{0}^{3})\\) fluctuated Asimov datasets generated under the null hypothesis in the context of Wilks’ theorem, grid scans were performed to compute 95% CL contours. These scans distinguish between open contours (where Δχ2 < 5.99) and closed contours in which Δχ2 ≥ 5.99 and the best-fit value would be distinguishable from the null hypothesis. The statistical band, encompassing the sensitivity contour obtained from Asimov simulations along with its 1σ and 2σ fluctuations, was subsequently reconstructed. As shown in Extended Data Fig. 10, the KNM1–KNM5 exclusion contour, derived from experimental data, falls within the 95% confidence region of the fluctuated simulated open contours.\n\nThis test confirms that the observed intersection between the sensitivity and exclusion contours is consistent with expected statistical fluctuations in the experimental data.\n\nAnalysis with a free neutrino mass\n\nIn the main text, a hierarchical scenario is assumed with m 4 ≫ m ν . Under this assumption, for the sterile-neutrino analysis, \\({m}_{{\\rm{\n\nu }}}^{2}\\) is set to 0 eV2. However, for a small sterile-neutrino mass and large mixing, the active and sterile branches can become degenerate. A strong negative correlation between the active and sterile-neutrino mass for small \\({m}_{4}^{2}\\le 30\\,{{\\rm{eV}}}^{2}\\) was reported in Sec. VI C of ref. 32. Hence, for small sterile-neutrino masses, it is interesting to perform the analysis considering the active-neutrino mass as another free parameter.\n\nIn Extended Data Fig. 11a, the 95% CL exclusion contours for the combined KNM1–KNM5 dataset are presented with statistical and systematic uncertainties incorporated. Two cases are analysed: the hierarchical scenario with the active-neutrino mass fixed to 0 eV2 (scenario I, solid blue) and the scenario treating the squared active-neutrino mass \\({m}_{{\\rm{\n\nu }}}^{2}\\) as an unconstrained nuisance parameter (scenario II, dashed orange). The overlaid colour map indicates the best-fit values of \\({m}_{{\\rm{\n\nu }}}^{2}\\) across the parameter space in scenario II. For sterile-neutrino masses below 40 eV2, partial degeneracy between the active- and sterile-neutrino branches weakens the exclusion bounds in scenario II relative to scenario I, with negative best-fit \\({m}_{{\\rm{\n\nu }}}^{2}\\) values highlighting the negative correlation. For larger sterile-neutrino masses, the exclusion contours of scenario II converge with those of scenario I as the correlation between the active- and sterile-neutrino masses decreases.\n\nApart from scenarios I and II, two additional scenarios were considered with specific restrictions on the squared active-neutrino mass. In scenario III, \\({m}_{{\\rm{\n\nu }}}^{2}\\) is treated as a free parameter but penalized by a ±1 eV2 pull term centred around 0 eV2. In scenario IV, \\({m}_{{\\rm{\n\nu }}}^{2}\\) is constrained by \\(0\\le {m}_{{\\rm{\n\nu }}}^{2} < {m}_{4}^{2}\\), known as the technical constraint, first proposed by ref. 45. In Extended Data Fig. 11b, the 95% CL exclusion contours for the combined KNM1–KNM5 dataset are presented, incorporating only the statistical uncertainties.\n\nThe technical constraint in scenario IV (yellow dashed line) is of interest as its contour follows that of scenario I for lower squared sterile-neutrino masses (<40 eV2) and that of scenario II for higher masses. Similar to Extended Data Fig. 11a, for only statistical uncertainties, the best-fit squared active-neutrino mass in scenario II is negative for sterile masses below 40 eV2. As the squared active-neutrino mass is restricted to positive values in scenario IV, the resulting best-fit value is 0 eV2, causing the contour of scenario IV to align with that of scenario I for low sterile-neutrino masses.\n\nKATRIN final sensitivity forecast\n\nA projected final sensitivity for the KATRIN experiment at 95% CL is estimated using a net measurement time of 1,000 days, following the same prescriptions as in ref. 32. The background rate is expected to be 130 mcps for 117 active pixels, and the systematic uncertainties are based on the design configuration2. The primary update from the design configuration is the background rate, reflecting the current value. Moreover, the statistical variation is calculated using \\({\\mathcal{O}}(1{0}^{3})\\) randomized tritium β-decay spectra with counts fluctuating according to a Poisson distribution. Sensitivity contours are calculated for each random dataset, and the 1σ and 2σ allowed regions that define the statistical fluctuations of the projected dataset are identified. A comparison with the exclusion contour from KATRIN for the first five measurement campaigns, see Extended Data Fig. 12, shows that for sterile masses \\(\\Delta {m}_{41}^{2} < 2\\,{{\\rm{eV}}}^{2}\\) the final sensitivity contour is surpassed but not beyond the 1σ statistical uncertainty band. For \\(\\Delta {m}_{41}^{2} > 2\\,{{\\rm{eV}}}^{2}\\), the sensitivity projection highlights the potential of KATRIN to probe the unexplored sterile-neutrino parameter space and to reach sensitivities of sin2(2θ ee ) < 0.01. Given the observed difference between our current exclusion and the median sensitivity, future data may also yield less stringent limits due to statistical fluctuations. This is accounted for in our sensitivity projections, as shown by the 1σ and 2σ bands in Extended Data Fig. 12, which illustrate the expected range of statistical variation around the median.",
      "url": "https://www.nature.com/articles/s41586-025-09739-9",
      "source": "Nature",
      "published": "2025-12-04",
      "sentiment_score": 0.85,
      "reasoning": "The article reports on a significant scientific experiment (KATRIN) that advances the search for sterile neutrinos, a fundamental particle physics question with broad implications for our understanding of the universe. The work involves sophisticated experimental techniques and data analysis, improving sensitivity and setting new exclusion limits on sterile-neutrino parameters. This represents a meaningful breakthrough in particle physics, with potential long-term impact on cosmology and fundamental science. The article is detailed, focused on a single major scientific effort, and describes the experimental setup, data analysis, and results comprehensively.",
      "category": "Technology",
      "personality_title": "KATRIN experiment sets new limits in search for sterile neutrinos",
      "personality_presentation": "**Context** – Scientists are searching for sterile neutrinos, a type of particle that could help explain mysteries about the universe. The KATRIN experiment in Germany studies tiny particles called electrons emitted from tritium, a radioactive gas, to find signs of these sterile neutrinos.\n\n**What happened** – Over 259 days, KATRIN collected data from 36 million electrons released during tritium decay. Using a 70-meter-long setup with precise detectors and special magnetic fields, researchers measured the energy of these electrons very carefully. They analyzed the data with two advanced computer programs to check for signs of sterile neutrinos.\n\n**Impact** – The experiment improved the search by combining data from five measurement campaigns and reducing background noise that could hide signals. It set new limits on the possible properties of sterile neutrinos, ruling out certain masses and mixing strengths. This narrows down where scientists should look next and challenges earlier claims about finding sterile neutrinos.\n\n**What's next step** – KATRIN plans to continue collecting data up to 1,000 days, which will increase its sensitivity. This means it could detect even weaker signals or rule out more possibilities for sterile neutrinos. The improved methods and careful checks will help ensure future results are reliable.\n\n**One-sentence takeaway** – After analyzing data from 259 days, the KATRIN experiment has set stronger limits on sterile neutrinos, refining our understanding of these elusive particles and guiding future research.\n",
      "personality_title_fr": "L'expérience KATRIN fixe de nouvelles limites dans la recherche des neutrinos stériles",
      "personality_presentation_fr": "**Contexte** – Les scientifiques cherchent des neutrinos stériles, un type de particule qui pourrait expliquer certains mystères de l'univers. L'expérience KATRIN en Allemagne étudie de minuscules particules appelées électrons émises par le tritium, un gaz radioactif, pour détecter des signes de ces neutrinos stériles.\n\n**Ce qui s'est passé** – Pendant 259 jours, KATRIN a collecté des données de 36 millions d'électrons libérés lors de la désintégration du tritium. Avec un dispositif de 70 mètres de long équipé de détecteurs précis et de champs magnétiques spéciaux, les chercheurs ont mesuré très précisément l'énergie de ces électrons. Ils ont analysé les données avec deux programmes informatiques avancés pour rechercher des signes de neutrinos stériles.\n\n**Impact** – L'expérience a amélioré la recherche en combinant des données issues de cinq campagnes de mesures et en réduisant le bruit de fond qui pourrait masquer les signaux. Elle a établi de nouvelles limites sur les propriétés possibles des neutrinos stériles, excluant certaines masses et forces de mélange. Cela réduit les zones à étudier et remet en question des affirmations antérieures sur la découverte de ces particules.\n\n**Prochaine étape** – KATRIN prévoit de continuer à collecter des données jusqu'à 1 000 jours, ce qui augmentera sa sensibilité. Cela signifie qu'elle pourrait détecter des signaux plus faibles ou éliminer davantage de possibilités pour les neutrinos stériles. Les méthodes améliorées et les contrôles rigoureux garantiront la fiabilité des futurs résultats.\n\n**Résumé en une phrase** – Après avoir analysé 259 jours de données, l'expérience KATRIN a renforcé les limites sur les neutrinos stériles, affinant notre compréhension de ces particules mystérieuses et orientant les recherches futures.\n",
      "personality_title_es": "El experimento KATRIN establece nuevos límites en la búsqueda de neutrinos estériles",
      "personality_presentation_es": "**Contexto** – Los científicos buscan neutrinos estériles, un tipo de partícula que podría ayudar a explicar misterios del universo. El experimento KATRIN en Alemania estudia pequeñas partículas llamadas electrones emitidos por el tritio, un gas radiactivo, para detectar señales de estos neutrinos estériles.\n\n**Qué pasó** – Durante 259 días, KATRIN recolectó datos de 36 millones de electrones liberados durante la desintegración del tritio. Usando un equipo de 70 metros de largo con detectores precisos y campos magnéticos especiales, los investigadores midieron cuidadosamente la energía de estos electrones. Analizaron los datos con dos programas informáticos avanzados para buscar señales de neutrinos estériles.\n\n**Impacto** – El experimento mejoró la búsqueda combinando datos de cinco campañas de medición y reduciendo el ruido de fondo que podría ocultar señales. Estableció nuevos límites sobre las propiedades posibles de los neutrinos estériles, descartando ciertas masas y niveles de mezcla. Esto reduce las áreas donde los científicos deben buscar y desafía reclamaciones anteriores sobre la detección de neutrinos estériles.\n\n**Próximo paso** – KATRIN planea seguir recolectando datos hasta completar 1,000 días, lo que aumentará su sensibilidad. Esto significa que podría detectar señales más débiles o descartar más posibilidades para los neutrinos estériles. Los métodos mejorados y las revisiones cuidadosas asegurarán que los futuros resultados sean confiables.\n\n**Frase clave** – Tras analizar 259 días de datos, el experimento KATRIN ha establecido límites más fuertes sobre los neutrinos estériles, refinando nuestro entendimiento de estas partículas difíciles de detectar y guiando la investigación futura.\n",
      "image_url": "public/images/news_image_Sterile-neutrino-search-based-on-259-days-of-KATRI.png",
      "image_prompt": "A detailed, warm-toned painting of an intricate, elongated scientific apparatus resembling a glowing, translucent tube filled with swirling, luminous particles symbolizing electrons, surrounded by softly glowing magnetic field lines gently guiding the particles through layered, semi-transparent filters and segmented detectors shaped like delicate, golden honeycomb patterns, all set against a calm, neutral background conveying precision and discovery."
    },
    {
      "title": "Dated gene duplications elucidate the evolutionary assembly of eukaryotes",
      "summary": "Nature, Published online: 03 December 2025; doi:10.1038/s41586-025-09808-zAnalysis of eukaryotic gene sequences using a relaxed molecular clock methodology indicate that eukaryotes emerged 3.0–2.25 billion years ago as a result of mitochondrial endosymbiosis with complex archaea that already possessed an elaborated cytoskeleton, membrane trafficking, endomembrane, phagocytotic machinery and a nucleus.",
      "content": "Previous analyses of pre-LECA gene duplications have delivered contradictory conclusions, suggesting either that genes of archaeal2 or bacterial3 ancestry accumulated more duplications along their respective eukaryotic stems. Our analysis focuses on pre-LECA duplications involved in eukaryotic apomorphies for which we were able to obtain alignments and gene family trees of sufficient quality for molecular clock analyses. We found many more genes of archaeal than bacterial origin with those qualities, consistent with previous work showing that eukaryotic genes of archaeal origin are in general more highly conserved36, although this pattern may simply reflect a greater number of gene duplications on the archaeal versus alphaproteobacterial eukaryotic stem lineages.\n\nComparing molecular clocks to other methods\n\nPrevious studies have used gene tree branch lengths, expressed in number of substitutions per site and normalized by relative evolutionary rates post-LECA, to infer the relative timing of events during eukaryogenesis, finding support for a relatively late mitochondrial endosymbiosis2,12. Relaxed clock methods provide crucial additional information for investigating these questions. First, clock methods link sequence divergence to the geological record, constraining the timing of key steps in eukaryogenesis to absolute time and, therefore, environmental context. Second, clock models provide a more flexible way to model variation in evolutionary rate through time, based on all of the available calibrations and sequence data—although we acknowledge that the real pattern of rate heterogeneity during eukaryogenesis and its associated HGT and duplications is likely to be more complex than captured by any current methodology. Finally, the Bayesian relaxed clock framework provides a natural way to propagate time information from the dated species tree—estimated using more calibrations and sequence data—to the individual gene trees, greatly ameliorating the difficulties resulting from the limited signal of short single-gene alignments. We note that this hierarchical framework implies that the inferred ages of gene duplications are informed by the ages of nodes on the dated species tree. For example, the inference that mFECA is younger than nFECA (Fig. 1) supports younger ages among duplications of alphaproteobacterial origin than those of Asgard origin, although sensitivity analyses demonstrate that this conclusion is robust to substantial variation in species tree ages (Supplementary Note 3).\n\nTiming eukaryogenesis\n\nThe timescale of lineage divergence in which we estimated the timing of gene duplications is broadly consistent with previous molecular dating analyses (Fig. 5a). Our estimates for the age of nFECA at 3.05–2.79 Ga and mFECA at 2.37–2.13 Ga are among the oldest, compared to a range of 2.90–2.09 Ga and 2.70–0.91 Ga, respectively, from previous studies19,37,38,39,40. Our 1.80–1.67 Ga estimate for LECA falls within the 2.39–0.95 Ga range of previous divergence time estimates19,37,38,39,40,41,42,43. These timescales are all based on relaxed molecular clock methods, but their differences reflect their underpinning data (sequence data), assumptions (clock model) and the nature of the calibrations used to disambiguate rates and times. Our estimates are closest to timescales that are based, as here, on analyses that used topology-based calibrations19,38,40. These enforce across-tree relative age constraints that reflect donor–recipient HGT and endosymbiosis events, spreading the limited temporal information from traditional node calibrations across the tree. As such, these timescales might be considered among the most realistic and, among them, our study has used among the most calibration constraints and sequence data. Our timescale infers a rapid radiation of the eukaryotic supergroups within about 300 million years (Myr) of LECA, consistent with the ‘Big Bang’ hypothesis of crown eukaryote diversification44.\n\nFig. 5: Timeline of development for eukaryotic key apomorphies. a, Our time-resolved species tree enables us to set a timeline for eukaryogenesis. Compared with other studies, our dates for nFECA and mFECA are among the oldest, whereas our date for LECA is intermediate19,37,38,39,40,42,43,69,70,71. b, Based on duplications in specific eukaryotic systems, we suggest a timeline for the emergence of these features. Vertical lines are suggested minimum limits for the emergence of features, and dashed horizontal lines denote the period of time for possible development and emergence. c, A tentative model that considers the interdependency of these features (arrowheads imply dependency; lines without arrowheads imply co-emergence but with as yet undetermined order). Data in a,b are aligned to the time axis, whereas in c, the nodes are grouped in relation to the nFECA and mitochondrial endosymbiosis boundaries. EGT, endosymbiotic gene transfer; ER, endoplasmic reticulum; MTOC, microtubule-organizing centre. Full size image\n\nGiven that our analysis is constrained by geologic evidence, it is pertinent to reflect on what aspect of eukaryote diversification the geologic record represents, not least because so little of it is used in calibration. Sterane records from the Neoarchaean Fortescue Supergroup of Australia45 have been attributed to contamination46 and, indeed, pre-Proterozoic biomarker records are generally considered questionable47. Large vesicles, compatible with (but not deterministic of) eukaryote affinity, are known from the Mesoarchean Moodies Group of South Africa48. Otherwise, the oldest widely accepted fossil eukaryotes are Dictyosphaera, Shuiyousphaeridium, Tappania and Valeria from the late Palaeoproterozoic (around 1.78 Ga) McDermott Formation of the Northern Territory, Australia49 and the (approximately 1.64 Ga) Changcheng and Ruyang groups of North China50,51,52,53,54,55, though their eukaryote affinity is based largely on inference of an actin cytoskeleton which evolved among archaeal ancestors22. Qingshania, also from the Chuanlinggou Formation, has a greater claim on eukaryote affinity, interpreted as a multicellular archaeplastid and, therefore, a late Palaeoproterozoic (approximately 1.63 Ga) crown eukaryote56. Otherwise, there is clear evidence of archaeplastids from the latest Mesoproterozoic57 and earliest Neoproterozoic58, among others59, and possible Amorphea from the latest Mesoproterozoic60. Given the sparse nature of the fossil record through the Archaean to much of the Mesoproterozoic, we should not anticipate that these oldest records approximate clade age, but there is good fossil evidence for archaeplastids and, therefore, crown eukaryotes having diverged deep in the Proterozoic, as our timescale suggests. Although some of these records may be of metabolically active cells61, the majority are cysts compatible with the prevalence of encystment among crown eukaryotes, the challenging environmental redox conditions that prevailed during eukaryogenesis and, evidently, crown eukaryote diversification.\n\nThere has been considerable debate about the environmental context of eukaryogenesis and of eukaryote diversification. It has long been argued that oxygenation of the biosphere, the Great Oxidation Event62 (GOE; 2.43–2.22 Ga), was an environmental driver underpinning mitochondrial endosymbiosis and the origin of eukaryotes and, indeed, our evolutionary timeline precludes a syntrophic association between nFECA and mFECA prior to the GOE. However, the formative stages of eukaryogenesis are likely to have taken place under anoxia or hypoxia, as oxic conditions were limited to the surface waters of the Earth’s oceans for much of the Proterozoic8,9. Specifically, our analyses point to an origin among archaea in the late Archaean, mitochondrial endosymbiosis almost coincident with the GOE, and diversification of crown eukaryotes in the late Palaeoproterozoic.\n\nTesting hypotheses of eukaryogenesis\n\nFrom dated duplications in genes underpinning eukaryotic features, we suggest a model (Fig. 5b) in which the archaeal host progressively complexified through the sequential evolution of various characters before mitochondrial endosymbiosis. Our data suggest that duplications in archaeal membrane and cytoskeletal protein families first developed internal membrane compartments with a role in membrane biogenesis, and later in combination with cytoskeletal development and endocytosis. Although this does not imply that a host cell of crown eukaryote size and system-complexity existed before mitochondrial endosymbiosis, our results suggest that the host had the prerequisites for nuclear compartmentalization (development of the nuclear localization system), possessed an endomembrane system (from the diversification of compartment-specific vesicle trafficking proteins) and had an evolved cytoskeleton (with branched actin-specific paralogues) capable of endocytosis before mitochondrial endosymbiosis. Bacterial genes from the mitochondrial endosymbiont and other sources seem to have driven the membrane transition and further development of the endolysosomal system and to have added to existing nuclear processes such as DNA repair and gene regulation and new ones such as meiosis (Supplementary Discussion 1 and Supplementary Fig. 4).\n\nThe relative timing of mitochondrial endosymbiosis stands out as the model-defining feature of contemporary hypotheses of eukaryogenesis. In mitochondria-early models, such as the hydrogen hypothesis5, the mitochondrial acquisition was the first step in eukaryotic evolution, and is suggested to have provided the energy needed to create a large cell and the eukaryotic apomorphies it supported63. By contrast, the phagocytosing archaeon33, syntrophy6,7 and serial endosymbiosis hypotheses2,12 can be defined as ‘mitochondria-late’, since they defer mitochondrial acquisition until after the majority of other eukaryotic apomorphies and, in the case of syntrophy and serial endosymbiosis hypotheses, after the interaction with other bacterial partners. Other models such as the reverse-flow30, inside-out29 and E3 hypotheses25 can be considered ‘mitochondria intermediate’1, as they do not make strong assumptions about when mitochondrial acquisition happened during eukaryogenesis.\n\nContemporary hypotheses of eukaryogenesis are principally evolutionary scenarios based on arguments of plausibility that serve to fill the large gaps in evidence between the living lineages from which data have been derived. In particular, Asgard relatives of eukaryotes encode various eukaryotic signature proteins, some of which have been shown to functionally resemble eukaryotic homologues23. Consistent with this genetic complement, the first cultivated Asgard archaea display remarkable cell biological features such as cytoskeleton-supported protrusions25. Furthermore, although fluorescence imaging indicates spatial separation of the genome from protein synthesis64, in terms of overall size and complexity, the Asgard archaea cultivated so far resemble typical prokaryotes25. We have sought to bridge these gaps in our understanding of eukaryogenesis by establishing the timing and sequence of evolutionary innovations between the living lineages. Our results are not fully compatible with any of the competing evolutionary scenarios; many of the gene duplications that underpin eukaryotic apomorphies had already occurred in the archaeal nuclear lineage before the divergence of the mitochondrial endosymbiont from its closest sampled relatives, and so argue against mitochondria-early scenarios. Whereas several hypotheses have proposed that the development of the nucleus was a response to mitochondrial endosymbiosis29, the results of our analyses suggest that nuclear development was underway before endosymbiosis. Other possible selective drivers for the evolution of the nucleus include protection of the nuclear genome65, a means of preventing premature translation of non-spliced pre-mRNAs29,66, or defence against viral infections67. The reverse-flow30, inside-out29 and E3 hypotheses25 all suggest early cytoskeletal development in the archaeal host, consistent with our observations and the gene repertoires of extant Asgard archaea. The inside-out hypothesis goes further, possibly supporting the development of a nuclear localization mechanism before endosymbiosis. The phagocytosing archaeon hypothesis33 also suggests a complex archaeal host, but differs from the other models in suggesting phagocytosis as the cell biological mechanism by which the mitochondrion was internalized. Our dating analyses suggest that the endolysosomal system evolved contemporaneously with the earliest alphaproteobacterial gene duplications (and therefore mitochondrial endosymbiosis), but we cannot discriminate which came first. Consequently, our results do not allow us to discriminate between the origin of phagocytosis and the timing of mitochondrial endosymbiosis, because the phagocytosing-archeaon and mitochondrial-intermediate scenarios lack sufficient detail on the ordering of acquisition of the diversity of eukaryote novelties. Nevertheless, our results argue against the serial endosymbiosis hypothesis because the duplication ages of the genes implicated suggest that the majority of non-alphaproteobacterial genes began to duplicate after mitochondrial acquisition. Together, our results are most consistent with hypotheses that propose a complex host capable of endocytosis entering into a symbiotic relationship with the mitochondrion. We characterize models that agree with this order of events as complexified archaeon, late-mitochondrion (CALM) models of eukaryogenesis. Although it has been argued that the alphaproteobacterial endosymbiont may have been an energetic requirement for the evolution of eukaryotic complexity63, our results are instead consistent with views in which the early development of eukaryotic cell complexity was not dependent upon a mitochondrial partner, in agreement with recent theoretical work68.\n\nConclusions\n\nDebate over the process of eukaryogenesis has been poorly constrained because there are no extant lineages representative of the component steps in building a eukaryotic cell. However, we have gleaned insights by establishing the ages of genes implicated in eukaryote novelties that have duplicated along the eukaryote stem lineages. Our results indicate that the process of eukaryogenesis began in the Mesoarchaean with divergence from archaeal relatives, involved acquisition of a mitochondrion in the early Palaeoproterozoic and culminated in LECA in the late Palaeoproterozoic. Furthermore, our data suggest that the host cell was already equipped with a cytoskeleton, endomembrane system and a nucleus (or protonucleus), or at the least with the physical separation of translation and transcription, prior to mitochondrial endosymbiosis. Our inferences on the relative timing of steps within eukaryogenesis do not support the early acquisition of mitochondria, but are compatible with most mitochondrial-intermediate and mitochondrial-late hypotheses. Mitochondrial endosymbiosis is inferred to have occurred shortly after the GOE. Given that Proterozoic Earth’s oceans would have remained largely anoxic for more than a billion years, our inferences are compatible with eukaryogenesis models that invoke syntrophic interactions as a key driver underlying the symbiosis between the archaeal host and bacterial endosymbiont.",
      "url": "https://www.nature.com/articles/s41586-025-09808-z",
      "source": "Nature",
      "published": "2025-12-04",
      "sentiment_score": 0.85,
      "reasoning": "The article reports a significant scientific breakthrough in understanding the evolutionary origins of eukaryotes, clarifying the timeline and sequence of key cellular innovations. This discovery has broad implications for biology and evolutionary science, providing a clearer picture of the origin of complex life, which is inspiring and intellectually uplifting for society at large.",
      "category": "Technology",
      "personality_title": "New study reveals timeline of key steps in the origin of complex cells",
      "personality_presentation": "**Context** – Scientists have long debated how complex cells called eukaryotes first evolved. Eukaryotes include all plants, animals, and fungi. A major question is when and how a simpler cell joined with a bacteria to form the first eukaryotic cell.\n\n**What happened** – Researchers used a method called relaxed molecular clock to study gene duplications in early eukaryotes. They found that eukaryotes began forming between 3.0 and 2.25 billion years ago. The host cell was a complex archaeon with structures like a cytoskeleton and nucleus before it absorbed a mitochondrion, a bacteria that provides energy. This mitochondrial partnership happened shortly after the Great Oxidation Event around 2.4 billion years ago.\n\n**Impact** – This research clarifies the order of important steps in eukaryote evolution. It shows that the host cell was already complex before gaining mitochondria, which challenges earlier ideas that mitochondria came first. The findings help explain how complex life evolved on Earth by linking genetic changes to geological events.\n\n**What's next step** – Future work will investigate how these early cellular features developed in detail and explore more fossil and genetic evidence. Understanding these steps better may reveal how complexity arose in other life forms and improve our knowledge of Earth's early environment.\n\n**One-sentence takeaway** – Scientists dated gene changes to show that complex cell features appeared before mitochondria joined, reshaping our understanding of how eukaryotes evolved billions of years ago.",
      "personality_title_fr": "Une nouvelle étude révèle la chronologie des étapes clés de l'origine des cellules complexes",
      "personality_presentation_fr": "**Contexte** – Les scientifiques débattent depuis longtemps sur la manière dont les cellules complexes appelées eucaryotes ont évolué. Les eucaryotes comprennent toutes les plantes, animaux et champignons. Une question importante est de savoir quand et comment une cellule plus simple s'est associée à une bactérie pour former la première cellule eucaryote.\n\n**Ce qui s'est passé** – Des chercheurs ont utilisé une méthode appelée horloge moléculaire détendue pour étudier les duplications de gènes chez les premiers eucaryotes. Ils ont découvert que les eucaryotes ont commencé à se former entre 3,0 et 2,25 milliards d'années. La cellule hôte était un archée complexe avec des structures comme un cytosquelette et un noyau avant d'absorber une mitochondrie, une bactérie fournissant de l'énergie. Ce partenariat mitochondrial est survenu peu après le Grand Événement d'Oxygénation il y a environ 2,4 milliards d'années.\n\n**Impact** – Cette recherche clarifie l'ordre des étapes importantes dans l'évolution des eucaryotes. Elle montre que la cellule hôte était déjà complexe avant d'acquérir les mitochondries, ce qui remet en question les idées précédentes selon lesquelles les mitochondries seraient apparues en premier. Ces résultats aident à expliquer comment la vie complexe a évolué sur Terre en reliant les changements génétiques aux événements géologiques.\n\n**Prochaine étape** – Les travaux futurs étudieront plus en détail le développement de ces premiers traits cellulaires et exploreront davantage de preuves fossiles et génétiques. Mieux comprendre ces étapes pourrait révéler comment la complexité est apparue chez d'autres formes de vie et améliorer nos connaissances sur l'environnement primitif de la Terre.\n\n**Conclusion en une phrase** – Les scientifiques ont daté des changements génétiques montrant que les caractéristiques des cellules complexes sont apparues avant l'arrivée des mitochondries, modifiant notre compréhension de l'évolution des eucaryotes il y a des milliards d'années.",
      "personality_title_es": "Nuevo estudio revela la cronología de pasos clave en el origen de células complejas",
      "personality_presentation_es": "**Contexto** – Los científicos han debatido durante mucho tiempo cómo evolucionaron las células complejas llamadas eucariotas. Los eucariotas incluyen plantas, animales y hongos. Una pregunta importante es cuándo y cómo una célula más simple se unió con una bacteria para formar la primera célula eucariota.\n\n**Qué pasó** – Investigadores usaron un método llamado reloj molecular relajado para estudiar duplicaciones genéticas en los primeros eucariotas. Encontraron que los eucariotas comenzaron a formarse entre 3.0 y 2.25 mil millones de años atrás. La célula anfitriona era un arqueón complejo con estructuras como citoesqueleto y núcleo antes de absorber una mitocondria, una bacteria que provee energía. Esta asociación mitocondrial ocurrió poco después del Gran Evento de Oxigenación hace unos 2.4 mil millones de años.\n\n**Impacto** – Esta investigación aclara el orden de pasos importantes en la evolución de los eucariotas. Muestra que la célula anfitriona ya era compleja antes de adquirir mitocondrias, lo que desafía ideas anteriores de que las mitocondrias llegaron primero. Los hallazgos ayudan a explicar cómo evolucionó la vida compleja en la Tierra vinculando cambios genéticos con eventos geológicos.\n\n**Próximo paso** – Los trabajos futuros investigarán cómo se desarrollaron estos primeros rasgos celulares en detalle y explorarán más evidencia fósil y genética. Entender mejor estos pasos podría revelar cómo surgió la complejidad en otras formas de vida y mejorar nuestro conocimiento sobre el ambiente temprano de la Tierra.\n\n**Conclusión en una frase** – Científicos dataron cambios genéticos para mostrar que las características de células complejas aparecieron antes de que las mitocondrias se unieran, cambiando nuestra comprensión de cómo evolucionaron los eucariotas hace miles de millones de años.",
      "image_url": "public/images/news_image_Dated-gene-duplications-elucidate-the-evolutionary.png",
      "image_prompt": "A detailed, warm-toned painting showing an ancient, evolving microbial world beneath a soft glowing sky, featuring symbolic intertwined spirals and branching structures representing archaeal and bacterial lineages merging; delicate, translucent compartments and filament-like networks illustrate early cellular complexity and endomembrane systems, all set within a calm, primordial aquatic environment with subtle hints of rising oxygen levels symbolized by gentle shimmering light."
    },
    {
      "title": "Determination of the spin and parity of all-charm tetraquarks",
      "summary": "Nature, Published online: 03 December 2025; doi:10.1038/s41586-025-09711-7The CMS Collaboration reports the measurement of the spin, parity, and charge conjugation properties of all-charm tetraquarks, exotic fleeting particles formed in proton–proton collisions at the Large Hadron Collider.",
      "content": "Modelling of hadron properties\n\nTo investigate the spin-parity properties of the resonant structure within the J/Ψ J/Ψ invariant mass spectrum in the range of 6.2–8.0 GeV, we use an approach designed to minimize model dependence. This approach relies on the observed J/Ψ J/Ψ invariant mass spectrum and the momentum of the J/Ψ J/Ψ system in both transverse and longitudinal directions with respect to the beam, while remaining independent of the polarization of the system by relying solely on decay angular information. For a spin-zero state, polarization is not relevant. For states with nonzero spin, we assume the state is produced unpolarized, but vary the polarization to evaluate potential small residual effects on the decay angular distributions due to detector acceptance.\n\nThe analysis uses a model that ensures consistency with the observations made by CMS, by using simulation adjustments to accurately capture the observed transverse and longitudinal motion, and parameters of the resonances and backgrounds extracted from ref. 32 and shown in Fig. 1. The background arises from nonresonant contributions, single-parton scattering (SPS) and double-parton scattering, plus an empirical term parameterizing the background near the threshold32. The background is parameterized using MC simulation, with adjustments applied to better match the observed data in both the signal and sideband regions.\n\nWe start by considering the spin-0 hypothesis for the X states, which are produced without polarization. The helicity amplitudes \\({A}_{{\\lambda }_{1}{\\lambda }_{2}}\\) of the two J/Ψ mesons are listed in Table 1. For the pseudoscalar state with JP = 0−, the amplitudes satisfy A ++ = −A −− and A 00 = 0. By contrast, for the scalar state with JP = 0+, both A ++ = A −− and A 00 amplitudes contribute, with no specific prediction for the relative magnitude of A 00 . We adopt the general amplitude approach35,46,51, in which the spin-0 state amplitude can be written as a sum of three Lorentz-invariant structures,\n\n$$\\begin{array}{l}A({{\\rm{X}}}_{J=0}\\to {V}_{1}{V}_{2})={a}_{1}({q}^{2}){m}_{V}^{2}{{\\epsilon }}_{1}^{* }{{\\epsilon }}_{2}^{* }+{a}_{2}({q}^{2}){f}_{{\\rm{\\mu }}\n\nu }^{* (1)}{f}^{* (2),{\\rm{\\mu }}\n\nu }\\\\ \\,+{a}_{3}({q}^{2}){f}_{{\\rm{\\mu }}\n\nu }^{* (1)}{\\mathop{f}\\limits^{ \\sim }}^{* (2),{\\rm{\\mu }}\n\nu },\\end{array}$$ (1)\n\nand where the field strength tensor of a vector boson V i with momentum q i and polarization vector ϵ i is defined as \\({f}^{(i),{\\rm{\\mu }}\n\nu }={{\\epsilon }}_{i}^{{\\rm{\\mu }}}{q}_{i}^{\n\nu }-{{\\epsilon }}_{i}^{\n\nu }{q}_{i}^{{\\rm{\\mu }}}\\), the conjugate field strength tensor is \\({\\mathop{f}\\limits^{ \\sim }}^{(i),{\\rm{\\mu }}\n\nu }=1/2{{\\epsilon }}^{{\\rm{\\mu }}\n\nu {\\rm{\\alpha }}{\\rm{\\beta }}}{f}_{{\\rm{\\alpha }}{\\rm{\\beta }}}\\), and q = q 1 + q 2 .\n\nAs the X decay proceeds by the strong interaction, which conserves parity, the first two terms in equation (1) can be interpreted as interactions involving scalar particles, corresponding to models \\({0}_{{\\rm{m}}}^{+}\\) and \\({0}_{{\\rm{h}}}^{+}\\) with couplings a 1 and a 2 , respectively. The third term represents the interaction of a pseudoscalar particle, 0−, associated with the coupling a 3 . At m 4μ = 6.9 GeV, \\({| {A}_{00}| }^{2}=52 \\% \\) in model \\({0}_{{\\rm{m}}}^{+}\\) and \\({| {A}_{00}| }^{2}=19 \\% \\) in model \\({0}_{{\\rm{h}}}^{+}\\). In the following, we analyse the two models, \\({0}_{{\\rm{m}}}^{+}\\) and \\({0}_{{\\rm{h}}}^{+}\\), separately. Moreover, we consider their combination, denoted as \\({0}_{{\\rm{mix}}}^{+}\\), in which the relative magnitudes and signs of the couplings a 1 and a 2 are varied. A positive relative sign results in constructive interference, whereas a negative relative sign leads to destructive interference.\n\nThe polarization of the spin-1 states varies depending on the production mechanism, in which we consider either unpolarized production or polarization with J z or \\({J}_{{z}^{{\\prime} }}=\\pm 1\\) (ref. 46) because quark-initiated production dominates. The z and \\(z{\\prime} \\) axes are defined in Extended Data Figure 1, where the motion of the four-muon system within the laboratory frame leads to appearance of noncollinear proton collisions and defines the \\(z{\\prime} \\) axis, while the z-axis approximates the proton beam line. In the decay process, four helicity amplitudes contribute, and their relationships are shown in Table 1. This is equivalent to two Lorentz-invariant structures in the decay amplitude\n\n$$\\begin{array}{l}A({{\\rm{X}}}_{J=1}\\to {V}_{1}{V}_{2})={b}_{1}({q}^{2})[({{\\epsilon }}_{1}^{* }q)({{\\epsilon }}_{2}^{* }{{\\epsilon }}_{{\\rm{X}}})+({{\\epsilon }}_{2}^{* }q)({{\\epsilon }}_{1}^{* }{{\\epsilon }}_{{\\rm{X}}})]\\\\ \\,+{b}_{2}({q}^{2}){{\\epsilon }}_{\\alpha \\mu\n\nu \\beta }{{\\epsilon }}_{{\\rm{X}}}^{\\alpha }{{\\epsilon }}_{1}^{* ,\\mu }{{\\epsilon }}_{2}^{* ,\n\nu }{\\widetilde{q}}^{\\beta },\\end{array}$$ (2)\n\nwhere \\(\\widetilde{q}={q}_{1}-{q}_{2}\\) and ϵ X is the polarization vector of the spin-1 resonance X. The b 1 and b 2 are the couplings to a vector state with JP = 1− and an axial vector with JP = 1+, respectively.\n\nThe polarization of the spin-2 states also depends on the production mechanism, in which we consider either unpolarized production or polarization with J z or \\({J}_{{z}^{{\\prime} }}=0\\) or ±2 (ref. 46) because gluon-initiated production is expected to dominate. In the decay process, all nine helicity amplitudes \\({A}_{{\\lambda }_{1}{\\lambda }_{2}}\\) contribute. This corresponds to the Lorentz-invariant structures in the decay amplitude found in refs. 35,46. There are two degrees of freedom in the 2−+ case, and we use the \\({2}_{{\\rm{m}}}^{-}\\) and \\({2}_{{\\rm{h}}}^{-}\\) models, corresponding to \\({g}_{8}^{(2)}\n\ne 0\\) and \\({g}_{10}^{(2)}\n\ne 0\\) in refs. 35,46, respectively. The \\({2}_{{\\rm{m}}}^{-}\\) model corresponds to A ++ = −A −− , with decay angular distributions identical to those of the 0− case. For \\({2}_{{\\rm{h}}}^{-}\\), we have A +0 = A 0+ = −A −0 = −A 0− , with decay angular distributions identical to those of the 1− case.\n\nWe use a single representative model for the JPC = 2++ state, corresponding to model \\({g}_{1}^{(2)}={g}_{5}^{(2)}\n\ne 0\\) denoted as \\({2}_{{\\rm{m}}}^{+}\\) in refs. 35,46. This \\({2}_{{\\rm{m}}}^{+}\\) model is chosen because the composition of \\({A}_{{\\lambda }_{1}{\\lambda }_{2}}\\) amplitudes represents all possible polarizations, in particular those that are unique for spin-2, A +− and A −+ and the tensor structure of interactions is minimal, avoiding the inclusion of higher-dimension operators. At m 4μ = 6.9 GeV, \\(2{| {A}_{++}| }^{2}=9 \\% \\), \\({| {A}_{00}| }^{2}=21 \\% \\), \\(4{| {A}_{+0}| }^{2}=47 \\% \\) and \\(2{| {A}_{+-}| }^{2}=23 \\% \\) in model \\({2}_{{\\rm{m}}}^{+}\\). Therefore, if the data are consistent with \\({J}^{P}={2}_{{\\rm{m}}}^{+}\\) and not with the other models, it will provide an unambiguous determination of J ≥ 2. A higher spin scenario (J > 2) could also be possible, and it would exhibit angular distributions similar to those of J = 2 with the same parity35,46.\n\nAs the JPC = 2++ state has four degrees of freedom in the amplitude composition, three other possibilities could be chosen that lead to the same observable decay distributions as the \\({0}_{{\\rm{m}}}^{+}\\), \\({0}_{{\\rm{h}}}^{+}\\) and 1+ states. Interference between the corresponding 2++ amplitude tensor structures is also possible. Therefore, from a purely decay-based analysis, if the data are consistent with either the 0++ or 1++ model, we cannot rule out a general 2++ model. Although analysing polarization information through production-sensitive angular distributions could help resolve this ambiguity, this analysis is more challenging due to uncertainty in the production mechanism. If the resonance is produced unpolarized, such an analysis would not provide additional information. In this study, we check production-sensitive angular distributions for consistency with an unpolarized case, whereas a more detailed analysis is left for future work.\n\nDetermining the form factors associated with the tensor structure of interactions, represented by a i (q2) in equation (1) for spin-0, b i (q2) in equation (2) for spin-1, or equivalent \\({g}_{i}^{(2)}({q}^{2})\\) in refs. 35,46 for spin-2, with \\({q}^{2}={m}_{4{\\rm{\\mu }}}^{2}\\), is nontrivial and relies on the model of strong interactions. Nevertheless, it is completely separable from the computation of the angular distributions, given a particular tensor structure of interactions. Furthermore, interference between the resonances could result in a complex alteration of the angular distributions. Nonetheless, assuming that all three resonances possess identical quantum numbers and coupling constants, the angular distributions remain unaffected by interference effects observed in the mass spectrum. Thus, in this study, we use an empirical approach to analyse the four-muon mass spectrum observed in the data, separating it from the analysis of angular distributions. This makes our analysis independent of the form factor model. The form factors and interference effects are integrated into the observed mass spectrum, as shown in Fig. 1.\n\nAngular observables\n\nThe angular information used in the analysis is shown in Extended Data Fig. 1 (refs. 35,46). Two production axes are defined, corresponding to the two production mechanisms, either the direct short-distance production in parton collisions or the fragmentation of a single parton into the hadron. The z-axis is parallel to the beam line within the frame boosted along the beam line from the laboratory frame, in which the longitudinal momentum of X is zero, potentially reflecting polarization in the two-parton collisions. The z′-axis aligns with the direction of the X momentum, reflecting potential polarization in the single-parton fragmentation scenario.\n\nThe decay angles Φ, θ 1 and θ 2 are defined with reference to Fig. 2. The production angles θ* and Φ 1 , or alternatively \\({\\theta }^{{\\prime} * }\\) and Φ 1 ′, are defined with respect to the z- or z′-axis, respectively. The angle θ* is defined between the z-axis and the X decay axis in its rest frame, whereas Φ 1 is the angle between the first J/Ψ decay plane and the production plane, defined by the z-axis and the X decay axis. Extended Data Fig. 1 shows only θ* and Φ 1 , whereas \\({\\theta }^{{\\prime} * }\\) and Φ 1 ′ are defined analogously. These production angles are not used directly in the analysis to avoid dependence on the production model, but they are checked for consistency.\n\nThe distributions of the decay and production angles in the range 6.2 < m 4μ < 8.0 GeV are presented in Extended Data Fig. 2, together with the five signal models shown. It is important to note that the one-dimensional angular distributions in Extended Data Fig. 2 do not capture all the information accessible to the optimal discriminant. Correlations between angles are lost in the projections; for instance, the models JPC = 1−+ and 1++ cannot be distinguished in each projection, but they can be separated using the optimal discriminants, which preserve all angular correlations. Furthermore, Extended Data Fig. 2 shows all events in the range 6.2 < m 4μ < 8.0 GeV together, without accounting for the correlation between the angular distributions and m 4μ , or the variation in signal purity with m 4μ due to the resonance structure appearing above the background. As a result, the separation power shown in these illustration plots is considerably diminished compared with that in the full analysis.\n\nTo eliminate the dependence on the initial polarization of nonzero-spin states, the production angles are excluded from the data analysis. The assumption is made that the resonances are produced unpolarized, although the variation of this polarization along the production axes is allowed for states with nonzero spin. This allows for the examination of any residual effects stemming from polarization dependence due to detector acceptance effects. Extended Data Fig. 2 shows that the production angular distributions are consistent with unpolarized resonance states along both the z- and z′-axes.\n\nMatrix element approach\n\nThe analysis of the multidimensional distributions \\({\\mathcal{P}}({\\theta }_{1},{\\theta }_{2},\\varPhi ,{m}_{4{\\rm{\\mu }}})\\) is complicated by the complex description and the nontrivial effects of detector reconstruction. Rather than using the three angular observables directly, we construct a single observable that effectively projects the angular information onto one dimension and is optimal for distinguishing between two hypotheses, \\({J}_{i}^{P}\\) and \\({J}_{j}^{P}\\). Using a matrix element likelihood approach35,46,51, a kinematic discriminant is formulated based on the ratio of probabilities for hypotheses \\({J}_{i}^{P}\\) and \\({J}_{j}^{P}\\):\n\n$${{\\mathcal{D}}}_{ij}({m}_{4{\\rm{\\mu }}},{\\boldsymbol{\\Omega }})=\\frac{{{\\mathcal{P}}}_{j}({m}_{4{\\rm{\\mu }}},{\\boldsymbol{\\Omega }})}{{{\\mathcal{P}}}_{i}({m}_{4{\\rm{\\mu }}},{\\boldsymbol{\\Omega }})+{{\\mathcal{P}}}_{j}({m}_{4{\\rm{\\mu }}},{\\boldsymbol{\\Omega }})},$$ (3)\n\nwhere \\({{\\mathcal{P}}}_{i}\\) is the normalized probability based on the matrix element squared for a given hypothesis \\({J}_{i}^{P}\\). These matrix elements are computed within the same quantum-mechanical formalism as used for the generation of MC events, as detailed in section ‘Angular analysis’.\n\nIn equation (3), we use only the decay angles \\({\\boldsymbol{\\Omega }}=\\{\\cos {\\theta }_{1},\\cos {\\theta }_{2},\\varPhi \\}\\). The analysis is conducted using the two-dimensional distributions of \\(({m}_{4{\\rm{\\mu }}},{{\\mathcal{D}}}_{ij})\\). Production information, including \\(\\{\\cos {\\theta }^{* }\\,,{\\varPhi }_{1}\\}\\) or \\(\\{\\cos {\\theta }^{{\\prime} * }\\,,{\\varPhi }_{1}^{{\\prime} }\\}\\), can be incorporated into a future analysis if a study of the resonance polarization is desired. The distributions of the discriminants, which were calculated to assess alternative models in comparison to the \\({2}_{{\\rm{m}}}^{+}\\) model, are presented in Fig. 3 and Extended Data Fig. 3 for all models defined without considering amplitude mixtures. The kinematic distributions in Fig. 3 and Extended Data Figs. 2 and 3 have also been separately examined in three different ranges of m 4μ , each corresponding to one of the three resonance structures shown in Fig. 1. The data remain consistent with the \\({2}_{{\\rm{m}}}^{+}\\) model across the full mass range as well as within each of the three individual intervals.\n\nThe final statistical analysis of the data is carried out using two-dimensional distributions of events, \\({{\\mathcal{P}}}_{ij}({m}_{4{\\rm{\\mu }}},{{\\mathcal{D}}}_{ij})\\), where the predicted m 4μ distribution is modelled in the same way as in Fig. 1 following ref. 32, and the \\({{\\mathcal{D}}}_{ij}\\) distribution is obtained in bins of 0.05 GeV in m 4μ from the detailed MC simulation discussed earlier in section ‘Angular analysis’. For any given slice of m 4μ , only five bins of \\({{\\mathcal{D}}}_{ij}\\) are used, meaning that the MC simulation can accurately predict probability distributions using an affordable number of simulated events.\n\nSystematic uncertainties\n\nIn the maximum likelihood fit introduced in section ‘Quantum number determination’ and further detailed in the next section, the likelihood function is maximized with respect to the nuisance parameters, which include those representing systematic uncertainties. The fit results encompass systematic variations in the parameterization of both signal and background models. The yields of the signal and each of the background processes are treated as free parameters, to be fully determined by the fit to the data. Further variations are categorized into two groups: variations in mass shapes and variations in discriminants, applied to both signal and background components.\n\nThe study of systematic variations in mass shapes was detailed in ref. 32, which includes resonance parameterization, resolution and efficiency for signal components, as well as different models for background components. All these variations in mass shapes were incorporated into the two-dimensional analysis presented here, through the variation of \\({{\\mathcal{P}}}_{k}({m}_{4{\\rm{\\mu }}})\\) in equation (4). Therefore, all uncertainties associated with the parameterization of the m 4μ spectrum as detailed in ref. 32 are included in the analysis.\n\nOne of the uncertainties arises from the unknown angular distributions of the background contribution near the kinematic threshold32. To allow maximum flexibility, the discriminant parameterization of this contribution, \\({T}_{ijk}({{\\mathcal{D}}}_{ij}| {m}_{4{\\rm{\\mu }}})\\) in equation (4), is expressed as the sum of the SPS background model and the two signal models under investigation, with the relative contributions of all three components determined by freely floating nuisance parameters. This approach is intended to accommodate a broad range of hypotheses for the threshold contribution, including possible variations of the nonresonant background and potential resonance excitations.\n\nOther systematic variations in discriminants account for possible inconsistencies in p T and p z distributions between data and MC simulation. The p T and p z distributions are validated and tuned through comparisons between data and simulation in both the sideband region (8.0–9.0 GeV) and the signal region (6.2–8.0 GeV). Residual discrepancies in the discriminants due to mismodelling of p T and p z are addressed through nuisance parameters, which allow for alternative discriminant parameterizations of \\({T}_{ijk}({{\\mathcal{D}}}_{ij}| {m}_{4{\\rm{\\mu }}})\\) in equation (4). Similarly, uncertainties in the kinematic distributions of the nonresonant background components are evaluated through comparisons between data and MC simulations in the sideband region, in which a satisfactory agreement is observed. Remaining discrepancies in the discriminants are handled through nuisance parameters that permit alternative discriminant parameterizations of \\({T}_{ijk}({{\\mathcal{D}}}_{ij}| {m}_{4{\\rm{\\mu }}})\\).\n\nThe nominal discriminant parameterization of all spin-1 and spin-2 resonances assumes no polarization. However, to evaluate the impact of detector acceptance effects on the results, we introduce an alternative parameterization that assumes polarization of spin-1 and spin-2 resonances along either the z- or z′-axis. This approach models polarized production arising from either two-parton collisions or single-parton fragmentation.\n\nStatistical analysis\n\nWe perform a binned extended maximum likelihood fit in which the probability density function is a sum of contributions from all signal and background processes implemented in the Combine tool (v.10.0.2) (ref. 52). This method mirrors the approach used to determine the spin-parity quantum numbers of the Higgs boson at the LHC53, as detailed in ref. 35. Each process k is characterized by a probability density function \\({{\\mathcal{P}}}_{ijk}\\), used to analyse signal hypotheses i and j. This function depends on two observables, m 4μ and \\({{\\mathcal{D}}}_{ij}\\), and is defined as a template binned in a 36 × 5 grid:\n\n$${{\\mathcal{P}}}_{ijk}({m}_{4{\\rm{\\mu }}},{{\\mathcal{D}}}_{ij})={{\\mathcal{P}}}_{k}({m}_{4{\\rm{\\mu }}}){T}_{ijk}({{\\mathcal{D}}}_{ij}| {m}_{4{\\rm{\\mu }}}),$$ (4)\n\nwhere \\({{\\mathcal{P}}}_{k}({m}_{4{\\rm{\\mu }}})\\) represents the probability density function of the invariant mass m 4μ , which is independent of the hypotheses being tested. The probability density T ijk is a normalized function of \\({{\\mathcal{D}}}_{ij}\\) given a specific value of m 4μ , obtained from simulation and including systematic variations through alternative distributions, as described above. We assume the same quantum numbers and couplings for all signal resonances, enabling the use of a shared T ijk for parameterizing the signal.\n\nTo distinguish between alternative models, the test statistic \\(q=-\\,2{\\rm{ln}}({{\\mathcal{L}}}_{{J}_{j}^{P}}/{{\\mathcal{L}}}_{{J}_{i}^{P}})\\) is defined using the ratio of signal plus background likelihoods for the two signal hypotheses. The likelihood is maximized with respect to the nuisance parameters, which include the yields of signal and background (bkg) processes and constrained parameters describing the systematic uncertainties. To quantify the consistency of the observed test statistic q obs with the model \\({J}_{i}^{P}\\), the probability \\(P=P(q\\le {q}_{{\\rm{obs}}}\\,| \\,{J}_{i}^{P}+\\,\\text{bkg})\\) is determined under the signal-plus-background hypothesis using pseudo-experiments. This probability is then translated into a Z-score, representing the number of standard deviations using the one-sided Gaussian tail integral.\n\nThe consistency of the observed data with the alternative signal hypothesis (\\({J}_{j}^{P}\\)) is assessed from \\(P(q\\ge {q}_{{\\rm{obs}}}\\,| \\,{J}_{j}^{P}+\\,{\\rm{bkg}})\\). The sign is positive if the tail extends away or negative if it extends towards the median of the other hypothesis. The CL s criterion54,55, defined as \\({{\\rm{CL}}}_{{\\rm{s}}}=P(q\\ge {q}_{{\\rm{obs}}}\\,| \\,{J}_{j}^{P}+\\,\\text{bkg}\\,)\\)/\\(P(q\\ge {q}_{{\\rm{obs}}}\\,| \\,{J}_{i}^{P}+\\,{\\rm{bkg}}) < \\alpha ,\\) is used for the final inference of whether a particular alternative hypothesis \\({J}_{j}^{P}\\) is excluded or not with respect to a reference hypothesis \\({J}_{i}^{P}\\) at a given confidence level (1 − α). Figure 3b shows example q distributions for the \\({2}_{{\\rm{m}}}^{+}\\) and 0− models, obtained from repeated pseudo-experiments simulating the expected experimental outcome.\n\nThe pairs of spin-parity models are tested among the 0−, \\({0}_{{\\rm{m}}}^{+}\\), \\({0}_{{\\rm{h}}}^{+}\\), 1−, 1+, \\({2}_{{\\rm{m}}}^{-}\\), \\({2}_{{\\rm{h}}}^{-}\\), and \\({2}_{{\\rm{m}}}^{+}\\) hypotheses. In all tests involving the \\({2}_{{\\rm{m}}}^{+}\\) model, it is preferred. Therefore, these tests are presented in Fig. 4 and Extended Data Table 1, which is an extended version of Table 2. The pairwise tests between the other models do not provide any additional useful information, as the data frequently show inconsistencies with both models. In the case of JPC = 0++ and 2−+ scenarios, additional tests are conducted to account for a possible mixture of two tensor structures, as outlined in section ‘Quantum number determination’.\n\nIt is important to note that the \\({2}_{{\\rm{m}}}^{+}\\) model represents only one specific realization of the JPC = 2++ hypothesis. In practice, a mixture of amplitudes corresponding to the 2++ state, as listed in Table 1, could produce angular distributions that resemble those expected for 0++ or 1++, apart from \\({2}_{{\\rm{m}}}^{+}\\). As a result, even if the true particle is a 2++ state, the P-values reported in Extended Data Table 1 may not remain fully consistent with the \\({2}_{{\\rm{m}}}^{+}\\) model under such admixtures. These mixed scenarios can be explored in future analyses. Taking this into account, all observed data distributions are found to be compatible with the JPC = 2++ hypothesis and show deviations from the predictions of alternative JPC assignments, with confidence levels summarized in Extended Data Table 1.",
      "url": "https://www.nature.com/articles/s41586-025-09711-7",
      "source": "Nature",
      "published": "2025-12-04",
      "sentiment_score": 0.85,
      "reasoning": "The article reports a significant scientific achievement: the precise determination of the spin and parity quantum numbers of all-charm tetraquarks, exotic particles observed at the Large Hadron Collider. This breakthrough advances fundamental understanding of particle physics and quantum chromodynamics, with broad implications for the scientific community and potential future technologies. The study is detailed, focused, and based on real experimental data, fulfilling the criteria for inspiring good news in the domain of technological and scientific progress.",
      "category": "Technology",
      "personality_title": "Scientists determine key properties of rare all-charm tetraquarks at the LHC",
      "personality_presentation": "**Context** – At the Large Hadron Collider (LHC), protons are smashed together at very high speeds to create and study tiny particles. Among these particles are exotic ones called tetraquarks, made of four charm quarks. Until now, scientists had not clearly understood their detailed properties, such as spin and parity, which are important to describe how these particles behave.\n\n**What happened** – The CMS Collaboration at the LHC measured the spin and parity of all-charm tetraquarks by analyzing how these particles decay into pairs of J/Ψ mesons, another type of particle. They used data from proton-proton collisions and studied the angles and energies of the decay products to figure out the particles’ quantum properties. Their analysis showed that the tetraquarks most likely have spin 2 and positive parity, fitting a specific theoretical model called the 2++ state.\n\n**Impact** – This is the first time scientists have precisely determined these quantum numbers for all-charm tetraquarks. Knowing the spin and parity helps confirm the nature of these exotic particles and improves our understanding of how quarks bind together under the strong force. This knowledge offers insights into the fundamental rules of particle physics and helps test theories about the forces inside matter.\n\n**What’s next step** – Researchers plan to continue studying these tetraquarks to explore their production mechanisms and other properties like polarization. They also want to analyze more data to check if there are mixtures of different quantum states. Future studies may reveal even more about how quarks form unusual combinations beyond known particles.\n\n**One-sentence takeaway** – Scientists at the LHC have successfully measured the spin and parity of rare all-charm tetraquarks, revealing they most likely have spin 2 and positive parity, a key step in understanding these exotic particles.\n",
      "personality_title_fr": "Les scientifiques déterminent les propriétés clés des tétraquarks tout-charmes au LHC",
      "personality_presentation_fr": "**Contexte** – Au Grand collisionneur de hadrons (LHC), des protons sont accélérés et entrent en collision à très grande vitesse pour créer et étudier de minuscules particules. Parmi elles, on trouve des particules exotiques appelées tétraquarks, composées de quatre quarks charm. Jusqu’à présent, leurs propriétés détaillées, comme le spin et la parité, n’étaient pas clairement comprises.\n\n**Ce qui s'est passé** – La collaboration CMS au LHC a mesuré le spin et la parité des tétraquarks tout-charmes en analysant la manière dont ces particules se désintègrent en paires de mésons J/Ψ, un autre type de particule. En étudiant les angles et les énergies des produits de désintégration issus des collisions proton-proton, ils ont déterminé les propriétés quantiques des tétraquarks. Leur analyse indique que ces particules ont très probablement un spin 2 et une parité positive, correspondant à un modèle théorique appelé état 2++.\n\n**Impact** – C’est la première fois que les scientifiques déterminent précisément ces nombres quantiques pour les tétraquarks tout-charmes. Connaître le spin et la parité aide à confirmer la nature de ces particules exotiques et améliore notre compréhension de la manière dont les quarks se lient sous la force forte. Ces connaissances offrent un aperçu des règles fondamentales de la physique des particules et permettent de tester des théories sur les forces à l’intérieur de la matière.\n\n**Prochaine étape** – Les chercheurs vont poursuivre l’étude de ces tétraquarks pour explorer leurs mécanismes de production et d’autres propriétés comme la polarisation. Ils souhaitent aussi analyser plus de données pour vérifier l’existence possible de mélanges d’états quantiques. Les futures recherches pourraient révéler davantage sur la formation de combinaisons inhabituelles de quarks au-delà des particules connues.\n\n**Résumé en une phrase** – Des scientifiques du LHC ont mesuré avec succès le spin et la parité des rares tétraquarks tout-charmes, montrant qu’ils ont probablement un spin 2 et une parité positive, une étape clé pour comprendre ces particules exotiques.\n",
      "personality_title_es": "Científicos determinan propiedades clave de los tetraquarks de todo charm en el LHC",
      "personality_presentation_es": "**Contexto** – En el Gran Colisionador de Hadrones (LHC), se hacen chocar protones a muy alta velocidad para crear y estudiar partículas diminutas. Entre estas partículas están los tetraquarks exóticos, formados por cuatro quarks charm. Hasta ahora, no se conocían bien sus propiedades detalladas, como el espín y la paridad, que son importantes para describir cómo se comportan.\n\n**Qué pasó** – La colaboración CMS en el LHC midió el espín y la paridad de los tetraquarks de todo charm analizando cómo estas partículas se desintegran en pares de mesones J/Ψ, otro tipo de partícula. Estudiaron los ángulos y energías de los productos de la desintegración para descubrir las propiedades cuánticas de estas partículas. Su análisis mostró que los tetraquarks probablemente tienen espín 2 y paridad positiva, ajustándose a un modelo teórico llamado estado 2++.\n\n**Impacto** – Es la primera vez que se determinan con precisión estos números cuánticos para los tetraquarks de todo charm. Conocer el espín y la paridad ayuda a confirmar la naturaleza de estas partículas exóticas y mejora nuestra comprensión de cómo se unen los quarks bajo la fuerza fuerte. Esto ofrece una mejor visión de las reglas fundamentales de la física de partículas y ayuda a probar teorías sobre las fuerzas dentro de la materia.\n\n**Próximo paso** – Los investigadores planean seguir estudiando estos tetraquarks para explorar cómo se producen y otras propiedades como la polarización. También quieren analizar más datos para ver si hay mezclas de diferentes estados cuánticos. Los estudios futuros podrían revelar más sobre cómo los quarks forman combinaciones inusuales más allá de las partículas conocidas.\n\n**Resumen en una frase** – Científicos del LHC lograron medir el espín y la paridad de raros tetraquarks de todo charm, mostrando que probablemente tienen espín 2 y paridad positiva, un paso clave para entender estas partículas exóticas.\n",
      "image_url": "public/images/news_image_Determination-of-the-spin-and-parity-of-all-charm-.png",
      "image_prompt": "An intricate, softly glowing cluster of four interlocking, shimmering orbs representing charm quarks, arranged to form a harmonious tetrahedral shape suspended in a gentle cosmic space, with delicate swirling lines symbolizing spin and parity weaving gracefully around them in balanced symmetry, rendered in warm, muted earth tones and soft blues."
    }
  ]
}