{
  "personality": null,
  "timestamp": "2025-10-09T04:39:20.666091",
  "category": "Technology",
  "news_summary": "Today's technology news highlights groundbreaking advances in molecular chemistry, quantum physics, and large-scale data search efficiency, showcasing significant progress in understanding and harnessing complex scientific phenomena.",
  "news_summary_fr": "L'actualité technologique d'aujourd'hui met en lumière des avancées révolutionnaires dans les domaines de la chimie moléculaire, de la physique quantique et de l'efficacité de la recherche de données à grande échelle, ce qui témoigne de progrès significatifs dans la compréhension et l'exploitation de phénomènes scientifiques complexes.",
  "news_summary_es": "Las noticias tecnológicas de hoy destacan avances revolucionarios en química molecular, física cuántica y eficiencia en la búsqueda de datos a gran escala, mostrando un progreso significativo en la comprensión y el aprovechamiento de fenómenos científicos complejos.",
  "articles": [
    {
      "title": "Nobel prize in chemistry awarded for work on molecular architecture",
      "summary": "Susumu Kitagawa, Richard Robson and Omar Yaghi have been honoured for the development of metal-organic frameworks, porous materials that can capture water or pollutants",
      "content": "Susumu Kitagawa, Richard Robson and Omar Yaghi have been honoured for the development of metal-organic frameworks, porous materials that can capture water or pollutants\n\nSusumu Kitagawa, Richard Robson and Omar Yaghi are the winners of the 2025 Nobel prize in chemistry JONATHAN NACKSTRAND/AFP via Getty Images\n\nThe 2025 Nobel prize in chemistry has gone to Susumu Kitagawa, Richard Robson and Omar Yaghi for the development of materials full of cavities that can store and release gases such as carbon dioxide, known as metal-organic frameworks.\n\n“A small amount of such material can be almost like Hermione’s handbag in Harry Potter,” said Heiner Linke, chair of the Nobel Committee for Chemistry. “It can store huge amounts of gas in a tiny volume.”\n\nAdvertisement\n\nTens of thousands of different metal-organic frameworks have now been created. They have many potential applications, from helping to capture CO 2 in chimneys to cleaning up forever chemicals and harvesting water from the air.\n\nIn the late 1980s, Richard Robson at the University of Melbourne in Australia was inspired by the ordered structure of diamonds to create the first metal-organic frameworks. Robson realised that it might be possible to use metal ions as nodes, and link them together with carbon-based, or organic, molecules.\n\nWhen the metal ions and organic molecules are mixed together, they self-assemble into ordered frameworks. While the cavities in the diamond framework are small, the cavities in metal-organic frameworks can be much bigger.\n\nFree newsletter Sign up to The Daily The latest on what’s new in science and why it matters each day. Sign up to newsletter\n\nThe cavities in the metal-organic frameworks created by Robson were filled with water. It was Susumu Kitagawa at Kyoto University in Japan who first created a framework that was stable enough to be dried out and who managed to fill the empty cavities with gas.\n\n“He showed that the gases could be taken up, absorbed by the material, and could also be released from the material,” said Olof Ramström, a member of the Nobel Committee for Chemistry.\n\nKitagawa also went on to create metal-organic frameworks that change shape when gases are added or removed.\n\nOmar Yaghi at the University of California, Berkeley, managed to create frameworks that were even more stable by using metal ion clusters containing zinc and oxygen, and linkers containing carboxylate groups.\n\n“This is an astonishing framework because it was highly stable. It was stable all the way up to 300 degrees Celsius,” said Ramström. “But even more remarkable was that it contains an enormous surface area. So just a few grams of this porous material, roughly the same as a small sugar cube, contains as much surface area as a large football pitch that is several thousands of square meters.”\n\nYaghi also went on to show that the cavities in these materials can be made larger, simply by making the linkers longer.\n\nAfter these fundamental breakthroughs, the field evolved very rapidly, Ramström said. “We see new metal-organic frameworks developed almost every day.”",
      "url": "https://www.newscientist.com/article/2499298-nobel-prize-in-chemistry-awarded-for-work-on-molecular-architecture/?utm_campaign=RSS%7CNSNS&utm_source=NSNS&utm_medium=RSS&utm_content=home",
      "source": "New Scientist - Home",
      "published": "2025-10-08",
      "sentiment_score": 0.9,
      "reasoning": "The article reports a significant scientific breakthrough recognized by the Nobel Prize in Chemistry for the development of metal-organic frameworks, which have broad applications including environmental benefits like capturing CO2 and pollutants, and harvesting water. The story has clear positive real-world impact, broad significance, and detailed context about the innovation and its implications.",
      "category": "Technology",
      "personality_title": "Nobel Prize awarded for breakthrough in materials that trap gases and pollutants",
      "personality_presentation": "**Context** – Scientists have been working to create materials that can hold gases and liquids inside tiny spaces. This helps with problems like cleaning air pollution or collecting water from the air.\n\n**What happened** – In 2025, Susumu Kitagawa, Richard Robson, and Omar Yaghi won the Nobel Prize in Chemistry. They developed metal-organic frameworks (MOFs), which are materials full of tiny holes that can store gases like carbon dioxide. Robson created the first MOFs inspired by diamond structures. Kitagawa made MOFs that could keep gases and release them when needed. Yaghi improved MOFs to be very stable and have huge surface areas, meaning they can hold a lot inside even if they look small.\n\n**Impact** – These materials can help capture harmful gases from factories, clean dangerous chemicals, and even collect water from the air in dry places. The fact that a small amount of MOF can store a large volume of gas is a big step forward in science and technology. This discovery opens new ways to protect the environment and improve resource use.\n\n**What's next step** – Scientists are now creating many new types of MOFs every day. The next steps include making these materials even better and using them in real machines to clean air, water, and help with energy storage.\n\n**One-sentence takeaway** – The Nobel Prize honors three scientists whose work on tiny, powerful materials could help clean pollution and gather water in new ways.",
      "personality_title_fr": "Prix Nobel décerné pour une avancée dans les matériaux qui capturent gaz et polluants",
      "personality_presentation_fr": "**Contexte** – Des scientifiques cherchent à créer des matériaux capables de retenir des gaz et des liquides dans de petits espaces. Cela aide à résoudre des problèmes comme la pollution de l’air ou la collecte d’eau dans l’air.\n\n**Ce qui s’est passé** – En 2025, Susumu Kitagawa, Richard Robson et Omar Yaghi ont reçu le prix Nobel de chimie. Ils ont développé des structures métalliques-organique (MOFs), des matériaux avec de minuscules trous pouvant stocker des gaz comme le dioxyde de carbone. Robson a créé les premiers MOFs en s’inspirant des structures du diamant. Kitagawa a fabriqué des MOFs capables de retenir et de libérer des gaz. Yaghi a amélioré les MOFs pour qu’ils soient très stables et aient une grande surface, ce qui leur permet de contenir beaucoup malgré leur petite taille.\n\n**Impact** – Ces matériaux peuvent aider à capturer les gaz nocifs des usines, nettoyer des produits chimiques dangereux, et même récolter l’eau de l’air dans des endroits secs. Le fait qu’une petite quantité de MOF puisse stocker un grand volume de gaz est une grande avancée scientifique et technologique. Cette découverte ouvre de nouvelles possibilités pour protéger l’environnement et mieux utiliser les ressources.\n\n**Prochaine étape** – Les scientifiques créent désormais de nombreux nouveaux types de MOFs chaque jour. La suite consiste à améliorer ces matériaux et à les utiliser dans des machines réelles pour purifier l’air, l’eau et aider au stockage d’énergie.\n\n**Résumé en une phrase** – Le prix Nobel récompense trois scientifiques dont le travail sur des matériaux minuscules et puissants pourrait aider à nettoyer la pollution et à récolter l’eau autrement.",
      "personality_title_es": "Premio Nobel otorgado por avance en materiales que atrapan gases y contaminantes",
      "personality_presentation_es": "**Contexto** – Los científicos han estado tratando de crear materiales que puedan contener gases y líquidos en espacios pequeños. Esto ayuda a resolver problemas como la contaminación del aire o la recolección de agua del aire.\n\n**Qué pasó** – En 2025, Susumu Kitagawa, Richard Robson y Omar Yaghi recibieron el Premio Nobel de Química. Desarrollaron estructuras metal-orgánicas (MOFs), materiales con pequeños agujeros que pueden almacenar gases como el dióxido de carbono. Robson creó los primeros MOFs inspirándose en la estructura del diamante. Kitagawa hizo MOFs que podían retener gases y liberarlos cuando se necesitara. Yaghi mejoró los MOFs para que fueran muy estables y tuvieran una gran superficie, lo que significa que pueden contener mucho aunque sean pequeños.\n\n**Impacto** – Estos materiales pueden ayudar a capturar gases dañinos de las fábricas, limpiar productos químicos peligrosos e incluso recolectar agua del aire en lugares secos. Que una pequeña cantidad de MOF pueda almacenar un gran volumen de gas es un gran avance en la ciencia y la tecnología. Este descubrimiento abre nuevas formas de proteger el medio ambiente y usar mejor los recursos.\n\n**Próximo paso** – Ahora los científicos están creando muchos tipos nuevos de MOFs cada día. El siguiente paso es mejorar estos materiales y usarlos en máquinas reales para limpiar aire, agua y ayudar en el almacenamiento de energía.\n\n**Frase clave** – El Premio Nobel reconoce a tres científicos cuyo trabajo con materiales pequeños y potentes podría ayudar a limpiar la contaminación y recolectar agua de nuevas formas.",
      "image_url": "public/images/news_image_Nobel-prize-in-chemistry-awarded-for-work-on-molec.png",
      "image_prompt": "A detailed, warm-toned painting of an intricate, glowing lattice structure made of interlinked metal nodes and organic molecules forming spacious, crystalline cavities that gently hold shimmering droplets of water and swirling, translucent gases, set against a soft, natural background that evokes scientific discovery and innovation."
    },
    {
      "title": "Scientists unlock the quantum magic hidden in diamonds",
      "summary": "Researchers have found a way to extract almost every photon from diamond color centers, a key obstacle in quantum technology. Using hybrid nanoantennas, they precisely guided light from nanodiamonds into a single direction, achieving 80% efficiency at room temperature. The innovation could make practical quantum sensors and secure communication devices much closer to reality.",
      "content": "Researchers at the Hebrew University of Jerusalem and the Humboldt University in Berlin have developed a way to capture nearly all the light emitted from tiny diamond defects known as color centers. By placing nanodiamonds into specially designed hybrid nanoantennas with extreme precision, the team achieved record photon collection at room temperature -- a necessary step for quantum technologies such as quantum sensors, and quantum-secured communications. The article was selected as a Featured Article in APL Quantum.\n\nDiamonds have long been prized for their sparkle, but researchers at the Hebrew University of Jerusalem in collaboration with colleagues from the Humboldt University in Berlin are showing they achieve an almost optimal \"sparkling,\" a key requirement for using diamonds also for quantum technology. The team has approached an almost perfect collection of the faintest light signals, single photons, from tiny diamond defects, known as nitrogen-vacancy (NV) centers, which are vital for developing next-generation quantum computers, sensors, and communication networks.\n\nNV centers are microscopic imperfections in the diamond structure that can act like quantum \"light switches.\" They emit single particles of light (photons) that carry quantum information. The problem, until now, has been that much of this light is lost in all directions, making it hard to capture and use.\n\nThe Hebrew University team, together with their research partners from Berlin, solved this challenge by embedding nanodiamonds containing NV centers into specially designed hybrid nanoantennas. These antennas, built from layers of metal and dielectric materials in a precise bullseye pattern, guide the light in a well-defined direction instead of letting it scatter. Using ultra-precise positioning, the researchers placed the nanodiamonds exactly at the antenna center -- within a few billionths of a meter.\n\nFeatured in APL Quantum, the results are significant: the new system can collect up to 80% of the emitted photons at room temperature. This is a dramatic improvement compared to previous attempts, where only a small fraction of the light was usable.\n\nProf. Rapaport explained, \"Our approach brings us much closer to practical quantum devices. By making photon collection more efficient, we're opening the door to technologies such as secure quantum communication and ultra-sensitive sensors.\"\n\nDr. Lubotzky added, \"What excites us is that this works in a simple, chip-based design and at room temperature. That means it can be integrated into real-world systems much more easily than before.\"\n\nThe research demonstrates not just clever engineering, but also the potential of diamonds beyond jewelry. With quantum technologies racing toward real-world applications, this advance could help pave the way for faster, more reliable quantum networks.",
      "url": "https://www.sciencedaily.com/releases/2025/10/251007081833.htm",
      "source": "Latest Science News -- ScienceDaily",
      "published": "2025-10-08",
      "sentiment_score": 0.85,
      "reasoning": "The article reports a significant breakthrough in quantum technology by achieving near-perfect photon collection from diamond color centers at room temperature. This advance has broad implications for practical quantum sensors and secure communication devices, representing a meaningful step toward real-world quantum applications with substantial societal and technological impact.",
      "category": "Technology",
      "personality_title": "Scientists improve diamond light capture for quantum tech",
      "personality_presentation": "**Context** – Diamonds have tiny defects called nitrogen-vacancy (NV) centers that can emit single particles of light, or photons, which are important for quantum technology. However, most of this light has been hard to collect because it scatters in many directions.\n\n**What happened** – Researchers from the Hebrew University of Jerusalem and Humboldt University in Berlin found a way to capture nearly all the photons emitted from these diamond defects. They placed nanodiamonds containing NV centers into special hybrid nanoantennas. These antennas guide the light in one direction with very high precision, reaching 80% efficiency at normal room temperature.\n\n**Impact** – This is a big step because before, only a small part of the light could be collected. With better light capture, quantum devices like sensors and secure communication systems can work more effectively and be easier to build. The fact that it works at room temperature means it can be used in everyday technology.\n\n**What's next step** – Scientists will likely work on integrating this design into real quantum devices and networks. This method could help create faster and more reliable quantum computers, sensors, and communication tools.\n\n**One-sentence takeaway** – A new technique allows most light from diamond defects to be captured efficiently at room temperature, moving quantum technology closer to practical use.",
      "personality_title_fr": "Les scientifiques améliorent la capture de lumière des diamants pour la technologie quantique",
      "personality_presentation_fr": "**Contexte** – Les diamants contiennent de petites imperfections appelées centres de lacunes d'azote (NV) qui émettent des particules de lumière uniques, ou photons, essentielles pour la technologie quantique. Cependant, la plupart de cette lumière était difficile à collecter car elle se dispersait dans toutes les directions.\n\n**Ce qui s'est passé** – Des chercheurs de l'Université hébraïque de Jérusalem et de l'Université Humboldt de Berlin ont trouvé un moyen de capturer presque tous les photons émis par ces défauts dans le diamant. Ils ont placé des nanodiamants contenant des centres NV dans des nanoantennes hybrides spéciales. Ces antennes dirigent la lumière dans une seule direction avec une grande précision, atteignant une efficacité de 80 % à température ambiante.\n\n**Impact** – C'est une avancée importante car auparavant, seule une petite partie de la lumière pouvait être collectée. Avec une meilleure capture de lumière, les dispositifs quantiques comme les capteurs et les systèmes de communication sécurisés peuvent fonctionner plus efficacement et être plus faciles à fabriquer. Le fait que cela fonctionne à température ambiante signifie que cela peut être utilisé dans la technologie quotidienne.\n\n**Prochaine étape** – Les scientifiques vont probablement travailler à intégrer ce design dans de vrais dispositifs et réseaux quantiques. Cette méthode pourrait aider à créer des ordinateurs, capteurs et outils de communication quantiques plus rapides et plus fiables.\n\n**Résumé en une phrase** – Une nouvelle technique permet de capter efficacement la plupart de la lumière des défauts du diamant à température ambiante, rapprochant la technologie quantique d'une utilisation pratique.",
      "personality_title_es": "Científicos mejoran la captura de luz de diamantes para tecnología cuántica",
      "personality_presentation_es": "**Contexto** – Los diamantes tienen pequeñas imperfecciones llamadas centros de vacantes de nitrógeno (NV) que emiten partículas únicas de luz, o fotones, importantes para la tecnología cuántica. Sin embargo, la mayoría de esta luz era difícil de capturar porque se dispersaba en muchas direcciones.\n\n**Qué sucedió** – Investigadores de la Universidad Hebrea de Jerusalén y la Universidad Humboldt de Berlín encontraron una forma de capturar casi todos los fotones emitidos por estos defectos en el diamante. Colocaron nanodiamantes con centros NV en nanoantenas híbridas especiales. Estas antenas guían la luz en una sola dirección con gran precisión, logrando un 80 % de eficiencia a temperatura ambiente.\n\n**Impacto** – Esto es un gran avance porque antes solo se podía capturar una pequeña parte de la luz. Con una mejor captura de luz, los dispositivos cuánticos como sensores y sistemas de comunicación seguros pueden funcionar de manera más efectiva y ser más fáciles de construir. El hecho de que funcione a temperatura ambiente significa que puede usarse en tecnología cotidiana.\n\n**Próximo paso** – Los científicos probablemente trabajarán en integrar este diseño en dispositivos y redes cuánticas reales. Este método podría ayudar a crear computadoras, sensores y herramientas de comunicación cuánticas más rápidas y confiables.\n\n**Conclusión en una frase** – Una nueva técnica permite capturar la mayoría de la luz de los defectos en diamantes con alta eficiencia a temperatura ambiente, acercando la tecnología cuántica a su uso práctico.",
      "image_url": "public/images/news_image_Scientists-unlock-the-quantum-magic-hidden-in-diam.png",
      "image_prompt": "A detailed, warm-toned painting of a radiant nanodiamond glowing at the center of a concentric bullseye-patterned nanoantenna made of layered metallic and dielectric rings, with streams of bright, focused single photons gently emanating outward in a precise, elegant flow against a soft, simple background."
    },
    {
      "title": "A strange quantum metal just rewrote the rules of electricity",
      "summary": "In a remarkable leap for quantum physics, researchers in Japan have uncovered how weak magnetic fields can reverse tiny electrical currents in kagome metals—quantum materials with a woven atomic structure that frustrates electrons into forming complex patterns. These reversals amplify the metal’s electrical asymmetry, creating a diode-like effect up to 100 times stronger than expected. The team’s theoretical explanation finally clarifies a mysterious phenomenon first observed in 2020, revealing that quantum geometry and spontaneous symmetry breaking are key to this strange behavior.",
      "content": "Quantum metals are metals where quantum effects -- behaviors that normally only matter at atomic scales -- become powerful enough to control the metal's macroscopic electrical properties.\n\nResearchers in Japan have explained how electricity behaves in a special group of quantum metals called kagome metals. The study is the first to show how weak magnetic fields reverse tiny loop electrical currents inside these metals. This switching changes the material's macroscopic electrical properties and reverses which direction has easier electrical flow, a property known as the diode effect, where current flows more easily in one direction than the other.\n\nNotably, the research team found that quantum geometric effects amplify this switching by about 100 times. The study, published in Proceedings of the National Academy of Sciences, provides the theoretical foundation that could eventually lead to new electronic devices controlled by simple magnets.\n\nScientists had observed this strange magnetic switching behavior in experiments since around 2020 but could not explain why it happened and why the effect was so strong. This study provides the first theoretical framework explaining both.\n\nWhen frustrated electrons cannot settle\n\nThe name \"kagome metal\" comes from the Japanese word \"kagome,\" meaning \"basket eyes\" or \"basket pattern,\" which refers to a traditional bamboo weaving technique that creates interlocking triangular designs.\n\nThese metals are special because their atoms are arranged in this unique basket-weave pattern that creates what scientists call \"geometric frustration\" -- electrons cannot settle into simple, organized patterns and are forced into more complex quantum states that include the loop currents.\n\nWhen the loop currents inside these metals change direction, the electrical behavior of the metal changes. The research team showed that loop currents and wave-like electron patterns (charge density waves) work together to break fundamental symmetries in the electronic structure. They also discovered that quantum geometric effects -- unique behaviors that only occur at the smallest scales of matter -- significantly enhance the switching effect.\n\n\"Every time we saw the magnetic switching, we knew something extraordinary was happening, but we couldn't explain why,\" Hiroshi Kontani, senior author and professor from the Graduate School of Science at Nagoya University, recalled.\n\n\"Kagome metals have built-in amplifiers that make the quantum effects much stronger than they would be in ordinary metals. The combination of their crystal structure and electronic behavior allows them to break certain core rules of physics simultaneously, a phenomenon known as spontaneous symmetry breaking. This is extremely rare in nature and explains why the effect is so powerful.\"\n\nThe research method involved cooling the metals to extremely low temperatures of about -190°C. At this temperature, the kagome metal naturally develops quantum states where electrons form circulating currents and create wave-like patterns throughout the material. When scientists apply weak magnetic fields, they reverse the direction these currents spin, and as a result, the preferred direction of current flow in the metal changes.\n\nNew materials meet new theory\n\nThis breakthrough in quantum physics was not possible until recently because kagome metals were only discovered around 2020. While scientists quickly observed the mysterious electrical switching effect in experiments, they could not explain how it worked.\n\nThe quantum interactions involved are very complex and require advanced understanding of how loop currents, quantum geometry, and magnetic fields work together -- knowledge that has only developed in recent years. These effects are also very sensitive to impurities, strain, and external conditions, which makes them difficult to study.\n\n\"This discovery happened because three things came together at just the right time: we finally had the new materials, the advanced theories to understand them, and the high-tech equipment to study them properly. None of these existed together until very recently, which is why no one could solve this puzzle before now,\" Professor Kontani added.\n\n\"The magnetic control of electrical properties in these metals could potentially enable new types of magnetic memory devices or ultra-sensitive sensors. Our study provides the fundamental understanding needed to begin developing the next generation of quantum-controlled technology,\" he said.",
      "url": "https://www.sciencedaily.com/releases/2025/10/251007081829.htm",
      "source": "Latest Science News -- ScienceDaily",
      "published": "2025-10-08",
      "sentiment_score": 0.85,
      "reasoning": "The article reports a significant scientific breakthrough in understanding quantum metals, specifically kagome metals, revealing how weak magnetic fields can control electrical currents in a way that could lead to new electronic devices. This discovery has broad implications for future technology development, such as magnetic memory devices and ultra-sensitive sensors, representing a meaningful advancement in quantum physics with potential large-scale technological benefits.",
      "category": "Technology",
      "personality_title": "Japanese scientists reveal how magnetic fields control electricity in kagome metals",
      "personality_presentation": "**Context** – Kagome metals are special materials with atoms arranged in a pattern like a woven basket. This unique design makes electrons inside them behave in complex ways, creating tiny loop currents that affect how electricity flows.\n\n**What happened** – Researchers in Japan discovered how weak magnetic fields can reverse these tiny electrical loops inside kagome metals. This reversal changes the direction where electricity flows more easily, creating a diode-like effect much stronger than expected. Their study explains a strange behavior first seen in 2020 by showing how quantum geometry and broken symmetry inside the metal work together.\n\n**Impact** – This finding is important because it reveals how small magnetic fields can control electrical properties in new ways. The effect is about 100 times stronger than usual, thanks to the metal’s special structure and quantum effects. This understanding could help create new electronic devices like magnetic memory or sensitive sensors that use less energy and are more precise.\n\n**What's next step** – Scientists will use this new knowledge to explore building devices that can switch electrical currents using magnetic fields. They also plan to study other materials with similar properties to find more ways quantum physics can improve technology.\n\n**One-sentence takeaway** – Weak magnetic fields can strongly control electricity flow in kagome metals, opening the door to new quantum-based electronic devices.",
      "personality_title_fr": "Des scientifiques japonais dévoilent comment les champs magnétiques contrôlent l'électricité dans les métaux kagome",
      "personality_presentation_fr": "**Contexte** – Les métaux kagome sont des matériaux spéciaux dont les atomes sont disposés selon un motif semblable à un tissage en panier. Cette structure unique fait que les électrons à l’intérieur se comportent de façon complexe, créant de petits courants en boucle qui influencent le passage de l’électricité.\n\n**Ce qui s’est passé** – Des chercheurs japonais ont découvert comment de faibles champs magnétiques peuvent inverser ces petits courants électriques dans les métaux kagome. Ce renversement change la direction où l’électricité circule plus facilement, créant un effet de diode bien plus fort que prévu. Leur étude explique un phénomène étrange observé pour la première fois en 2020 en montrant comment la géométrie quantique et la rupture de symétrie à l’intérieur du métal fonctionnent ensemble.\n\n**Impact** – Cette découverte est importante car elle montre comment de petits champs magnétiques peuvent contrôler les propriétés électriques de façons nouvelles. L’effet est environ 100 fois plus fort que d’habitude, grâce à la structure spéciale du métal et aux effets quantiques. Cette compréhension pourrait aider à créer de nouveaux appareils électroniques comme des mémoires magnétiques ou des capteurs sensibles qui consomment moins d’énergie et sont plus précis.\n\n**Prochaine étape** – Les scientifiques utiliseront cette nouvelle connaissance pour explorer la fabrication d’appareils capables de changer les courants électriques avec des champs magnétiques. Ils prévoient aussi d’étudier d’autres matériaux aux propriétés similaires pour trouver d’autres façons dont la physique quantique peut améliorer la technologie.\n\n**En une phrase** – De faibles champs magnétiques peuvent contrôler fortement le flux électrique dans les métaux kagome, ouvrant la voie à de nouveaux appareils électroniques basés sur la physique quantique.",
      "personality_title_es": "Científicos japoneses revelan cómo los campos magnéticos controlan la electricidad en metales kagome",
      "personality_presentation_es": "**Contexto** – Los metales kagome son materiales especiales cuyos átomos están organizados en un patrón parecido a un tejido de cesta. Esta estructura única hace que los electrones dentro se comporten de manera compleja, creando pequeñas corrientes en bucle que afectan cómo fluye la electricidad.\n\n**Qué pasó** – Investigadores en Japón descubrieron cómo campos magnéticos débiles pueden invertir estas pequeñas corrientes eléctricas en metales kagome. Esta inversión cambia la dirección en la que la electricidad fluye más fácilmente, creando un efecto tipo diodo mucho más fuerte de lo esperado. Su estudio explica un comportamiento extraño visto por primera vez en 2020, mostrando cómo la geometría cuántica y la ruptura de simetría dentro del metal trabajan juntas.\n\n**Impacto** – Este hallazgo es importante porque revela cómo campos magnéticos pequeños pueden controlar propiedades eléctricas de nuevas formas. El efecto es unas 100 veces más fuerte de lo normal, gracias a la estructura especial del metal y efectos cuánticos. Este conocimiento puede ayudar a crear nuevos dispositivos electrónicos como memorias magnéticas o sensores sensibles que usan menos energía y son más precisos.\n\n**Qué sigue** – Los científicos usarán este nuevo conocimiento para explorar la construcción de dispositivos que puedan cambiar corrientes eléctricas usando campos magnéticos. También planean estudiar otros materiales con propiedades similares para encontrar más formas en que la física cuántica puede mejorar la tecnología.\n\n**Resumen en una frase** – Campos magnéticos débiles pueden controlar fuertemente el flujo eléctrico en metales kagome, abriendo la puerta a nuevos dispositivos electrónicos basados en la física cuántica.",
      "image_url": "public/images/news_image_A-strange-quantum-metal-just-rewrote-the-rules-of-.png",
      "image_prompt": "A detailed, warm-toned painting of an intricate, glowing kagome lattice pattern made of interwoven golden bamboo-like strands forming triangular \"basket weave\" shapes, with delicate, shimmering loops of electric currents flowing in opposite directions around the lattice, subtly reversed by a soft, ethereal magnetic field represented as gentle swirling light waves enveloping the structure, set against a simple, muted background of cool blues and grays to evoke low temperature and quantum mystery."
    },
    {
      "title": "Efficient and accurate search in petabase-scale sequence repositories",
      "summary": "Nature, Published online: 08 October 2025; doi:10.1038/s41586-025-09603-wMetaGraph enables scalable indexing of large sets of DNA, RNA or protein sequences using annotated de Bruijn graphs.",
      "content": "Representing de Bruijn graphs\n\nAccording to the formal definition of a de Bruijn graph, every pair of k-mers (short contiguous subsequences of length k) with an overlap of k − 1 characters are connected with a directed edge. It can be seen that in the de Bruijn graph, the edges can always be derived from the nodes alone, and hence, it is sufficient to store only the k-mer set. These k-mers are used as elementary indexing tokens in MetaGraph.\n\nMetaGraph provides several data structures for storing k-mer sets, which are used as a basis to implement different representations of the de Bruijn graph abstraction. In addition to the simple hash table, the k-mers may be stored in an indicator bitmap61 (a binary vector represented as a succinct bitmap of size \\(|\\,\\varSigma \\,{|}^{k}\\) indicating which k-mers are present in the set) or in the BOSS table1 (a data structure proposed by Bowe, Onodera, Sadakane and Shibuya for storing a set of k-mers succinctly; see Supplementary Fig. 3 and the corresponding Supplementary Table 2 for an example BOSS graph and table). We call our de Bruijn graph implementations based on these data structures HashDBG, BitmapDBG and SuccinctDBG, respectively. For detailed descriptions and the properties of these representations, refer to Supplementary Section A.3.1 and Supplementary Table 1. All these data structures support exact membership queries, and they map k-mers to positive indexes from 1 to n, where n is the number of k-mers in the represented set (or zero if the queried k-mer does not belong to the set). While HashDBG is mostly used internally (for example, for batched sequence search), SuccinctDBG typically exhibits the best compression performance and, therefore, serves as the default compressed representation.\n\nScalable k-mer enumeration and counting\n\nTo count and de-duplicate k-mers, MetaGraph uses the following approach to generate the so-called k-mer spectrum: the input k-mers are appended to a list, which is sorted and de-duplicated every time it reaches the allocated space limit or after all input k-mers have been processed. During de-duplication, the k-mer counts are summed up to maintain the total count of each unique k-mer. We call this approach SortedSet. Moreover, MetaGraph offers the SortedSetDisk approach, which implements a similar algorithm in external memory. Using a pre-allocated fixed-size buffer limits memory usage and allows for constructing virtually arbitrarily large k-mer spectra but requires a larger amount of disk I/O. Lastly, MetaGraph supports passing precomputed outputs from the KMC3 (ref. 62) k-mer counting tool (fork karasikov/KMC commit b163688) as an input to make use of its exceptionally efficient counting algorithm and filters. Once the entire k-mer spectrum is obtained and all k-mers are sorted, they are converted into the final data structure to construct the target graph representation.\n\nExtracting contigs and unitigs from graphs\n\nAll sequences encoded in the graph (or any defined subgraph) can be extracted from it and stored in FASTA format through graph traversal63,64. The graph is fully traversed and its paths, formed by consecutive overlapping k-mers, are converted into sequences (contigs) that are returned as a result of this operation. Each k-mer of the graph (or subgraph) appears in the assembled contigs exactly once. Thus, the resulting set of sequences is a disjoint node cover of the traversed graph.\n\nMetaGraph provides efficient parallel algorithms for sequence extraction and distinguishes two main types of traversal: (1) traversal in contig mode extends a traversed path until no further outgoing edge is present or if all the next outgoing edges have already been traversed; while (2) traversal in unitig mode only extends a path if its last node has a single outgoing edge, and this edge is the single edge incoming to its target node. This definition of a unitig matches the one described previously65.\n\nBasic, canonical and primary graphs\n\nWhen indexing raw reads sequenced from unknown strands, we supplement each sequence with its reverse complement, which is then indexed along with the original sequence. As a result, the de Bruijn graph accumulates each k-mer in both orientations. Such graphs (which we call canonical) can be represented by storing only one orientation of each k-mer and simulating the full canonical graph on-the-fly (for example, for querying outgoing edges, return not only the edges outgoing from the source k-mer, but also all edges incoming to its reverse complement k-mer).\n\nStoring only canonical k-mers (that is, the lexicographically smallest of the k-mer and its reverse complement) effectively reduces the size of the graph by up to two times. However, this cannot be efficiently used with the succinct graph representation based on the BOSS table. The BOSS table, by design, requires that each k-mer in it has other k-mers overlapping its prefix and suffix of length k − 1 (at least one incoming and one outgoing edge in the de Bruijn graph). However, it is often the case that among two consecutive k-mers in a read, only one of them is canonical. Thus, storing only canonical k-mers in the BOSS table would often require adding several extra dummy k-mers for each real k-mer (Supplementary Fig. 4), which makes this approach memory inefficient. We overcome this issue by constructing primary graphs, where the word primary reflects the traversal order, as described in the next paragraph.\n\nWhen traversing a canonical de Bruijn graph, we can additionally apply the constraint that only one of the orientations of a given k-mer is called. More precisely, the traversal algorithm works as usual, but never visits a k-mer if its reverse complement has already been visited. Whichever orientation of the forward or reverse complement k-mer is visited first is considered to be the primary k-mer of the pair (an example illustration is shown in Supplementary Fig. 6). This results in a set of sequences, which we call primary (primary contigs or primary unitigs). Note that the traversal order of the graph may change the set of primary sequences extracted from it, but it may never change the total number of k-mers in these sequences (primary k-mers). This is relevant when extracting primary contigs with multiple threads since the node traversal order may differ between runs. We call graphs constructed from primary sequences primary graphs. In contrast to the common approach in which only canonical k-mers are stored, primary de Bruijn graphs can be efficiently represented succinctly using the BOSS table, and effectively enable us to reduce the size of the graph part of the MetaGraph index by up to two times.\n\nGraph cleaning\n\nWhen a graph is constructed from raw sequencing data, it might contain a considerable number of k-mers resulting from sequencing errors (erroneous k-mers). These k-mers do not occur in the biological sequences and make up spurious paths in the graph, which one may desire to prune off. True-signal k-mers may also originate from contaminant organisms in the biological sample. Pruning the graph to discard either or both of these classes of undesirable k-mers is called graph cleaning.\n\nMetaGraph provides routines for graph cleaning and k-mer filtering based on the assumption that k-mers with relatively low abundance (low k-mer counts) in the input data were probably generated due to sequencing errors or contamination and should therefore be dropped. To identify potentially undesirable k-mers, we use an algorithm proposed previously65. In MetaGraph, we adapted and scaled up this algorithm to work not only for small but also for very large graphs (up to trillions of nodes).\n\nIn brief, the decision to filter out a k-mer is based on the median abundance of the unitig to which this k-mer belongs. That is, k-mers with low abundance are preserved if they are situated in a unitig with sufficiently many (more precisely, at least 50%) highly abundant (solid) k-mers. Then the entire unitig is considered solid and is kept in the graph. All solid unitigs (which may also be concatenated into contigs called clean contigs) are extracted from the graph and output in FASTA format. Connected unitigs (those with non-zero degree) that are discarded due to lack of abundance typically originate from sequencing errors, and their removal is traditionally called bubble popping66. Optionally, all tips (that is, unitigs where the last node has no outgoing edges) that are shorter than a given cut-off (typically 2k) are discarded as well. Afterwards, a new graph can be constructed from these clean contigs, which we call a cleaned graph.\n\nThe abundance threshold for solid unitigs can be set either manually or computed automatically from the full k-mer spectrum. It is assumed that k-mers with an abundance of at most 3 are likely to be generated by sequencing errors and that all erroneous k-mers follow a negative binomial distribution. After fitting a negative binomial distribution to these low-abundance k-mers, the abundance threshold is set to the 99.9th percentile of this distribution65. Finally, in case the chosen threshold leads to preserving less than 20% of the total coverage, the automatic estimation procedure is deemed unsuccessful and a pre-defined value (typically 2) is used as a fallback threshold instead.\n\nConstructing a joint graph from multiple samples\n\nAccording to our workflow, when indexing multiple read sets (especially when indexing vast collections of raw sequencing data), the recommended workflow for constructing a joint de Bruijn graph from the input samples consists of the following three steps. First, we independently construct a de Bruijn graph from each input read set. As each graph is constructed from a single read set (or sample), we call these graphs sample graphs. If desired, these sample graphs are independently cleaned with the graph cleaning procedure described above. Then, each sample graph is decomposed into a set of (clean) contigs, either by extracting the contigs directly or as a result of the graph cleaning procedure. Finally, a new de Bruijn graph is constructed from all these contigs, which is then annotated to represent the relation between the k-mers and the input samples. As this graph represents the result of merging all sample graphs, we refer to it as the joint de Bruijn graph. In practice, the size of the contigs extracted from sample graphs is up to 100 times smaller than the raw input, which makes the construction of the joint de Bruijn graph by this workflow much more efficient compared with constructing it directly from the original raw read sets.\n\nGraph annotations\n\nOnce a de Bruijn graph is constructed, it can already be used to answer k-mer membership queries, that is, to check whether a certain k-mer belongs to the graph or not. However, the de Bruijn graph alone can encode no additional metadata (such as sample ID, organism, chromosome number, expression level or geographical location). Thus, we supplement the de Bruijn graph with another data structure called an annotation matrix. Each column of the annotation matrix A ∈ {0,1}n×m, where n is the number of k-mers in the graph and m is the number of annotation labels, is a bit vector indicating which k-mers possess a particular property:\n\n$${A}_{j}^{i}:= \\{\\begin{array}{cc}1 & k \\mbox{-} {\\rm{m}}{\\rm{e}}{\\rm{r}}\\,i\\,{\\rm{i}}{\\rm{s}}\\,{\\rm{i}}{\\rm{n}}\\,{\\rm{r}}{\\rm{e}}{\\rm{l}}{\\rm{a}}{\\rm{t}}{\\rm{i}}{\\rm{o}}{\\rm{n}}\\,{\\rm{w}}{\\rm{i}}{\\rm{t}}{\\rm{h}}\\,{\\rm{a}}{\\rm{t}}{\\rm{t}}{\\rm{r}}{\\rm{i}}{\\rm{b}}{\\rm{u}}{\\rm{t}}{\\rm{e}}\\,j,\\\\ 0 & {\\rm{o}}{\\rm{t}}{\\rm{h}}{\\rm{e}}{\\rm{r}}{\\rm{w}}{\\rm{i}}{\\rm{s}}{\\rm{e}}\\end{array}$$\n\nWithout loss of generality, we will assume that the annotation matrix encodes the membership of k-mers to different samples, that is, encodes sample IDs. In this case, \\({A}_{j}^{i}=1\\) indicates that k-mer i appears in sample j (note that the same k-mer may also appear in multiple samples).\n\nThe number of rows of the annotation matrix corresponds to the number of k-mers indexed in the de Bruijn graph, and the annotation matrix can therefore be of enormous size, containing up to 1012 rows and 109 columns. However, this matrix is typically extremely sparse and can therefore be efficiently compressed.\n\nRepresenting graph annotations in MetaGraph\n\nIndependent of the choice of graph representation, a variety of methods are provided in MetaGraph for compressing annotation matrices to accommodate different query types. These different matrix representation schemes can be split into row-major and column-major. The row-major representations (such as RowSparse4, RowFlat67,68) enable fast row queries but have a poor performance of column queries. By contrast, the column-major representations (such as ColumnCompressed, Multi-BRWT3) provide fast access to individual columns of the annotation matrix but typically have a poorer row query performance. Despite being mathematically equivalent, up to the transposition of the represented matrix, these schemes are in fact algorithmically different due to the number of rows of the annotation matrix typically being orders of magnitude larger than the number of columns. A detailed description of these and other representation schemes is provided in Supplementary Information A.4. The size and query time benchmarks, respectively, of the compression methods used in our representations are provided in Supplementary Figs. 1 and 2.\n\nThe RowDiff compression technique\n\nDue to the nature of de Bruijn graphs and the fact that adjacent nodes (k-mers) usually originate from the same sequences, it turns out that, in practice, adjacent nodes in the graph are likely to carry identical or similar annotations. The RowDiff compression technique4,5 exploits this regularity by replacing the annotations at nodes with their relative differences. This enables us to substantially sparsify the annotation matrix and, therefore, considerably improve its compressibility. Notably, the transformed annotation matrix can still be represented with any other available scheme, including those described above. MetaGraph provides a scalable implementation of this technique with efficient construction algorithms, which allow applying it to virtually arbitrarily large annotation matrices. The algorithm essentially consists of two parts. First, for each node with at least one outgoing edge, it picks one of the edges and marks its target node as a successor. Second, it replaces the original annotations at nodes with their differences from the annotations at their assigned successor nodes. This delta-like transform is applied to all nodes in the graph except for a small subset of them (called anchors), which keep their original annotation unchanged and serve to end every path composed of successors and break the recursion when reconstructing the original annotations (for example, in cycles).\n\nCounting de Bruijn graphs\n\nFinally, MetaGraph supports generalized graph annotations for representing quantitative information such as k-mer positions and their abundances in input sources, encoded with non-binary matrices5.\n\nAnnotation construction\n\nThe typical workflow for constructing an annotation matrix for a large input set consists of the following steps. After the joint de Bruijn graph has been constructed from the input sequences, we iterate over the different samples (corresponding to the different annotation labels) in parallel and map all k-mers of each sample to the joint graph, generating a single annotation column. To avoid the mapping of identical k-mers multiple times and to prevent the processing of erroneous k-mers (k-mer with sequencing errors), we use the unitigs extracted from the cleaned sample graphs instead of the raw sequences when annotating the graph. This substantially reduces the annotation construction time, especially when the joint graph is represented with SuccinctDBG, for which the traversal to an adjacent k-mer is several times faster than a k-mer lookup performed from scratch (Supplementary Information A.3.2).\n\nOnce an annotation matrix has been constructed (typically in the ColumnCompressed representation), it can be transformed to any other representation to achieve the desired trade-offs between the representation size and the performance of the required operations. In particular, for sequence search, the recommended workflow is to apply the RowDiff transform on the annotation matrix and then convert the sparsified columns to the Multi-BRWT or RowSparse representation, depending on the desired speed versus memory trade-off.\n\nDynamic index augmentation and batch updates\n\nGenerally, there are three strategies for extending a fully constructed MetaGraph index (a joint de Bruijn graph and its corresponding annotation matrix). First, the batch of new sequences can be indexed separately and that second MetaGraph index can be hosted on the same or on a different server. Then, these two indexes can be queried simultaneously, as it is done for a distributed MetaGraph index.\n\nSecond, the graph can be updated directly if it is represented using a dynamic data structure that supports dynamic updates (for example, SuccinctDBG (dynamic)). Then, the annotation matrix needs to be updated accordingly. This approach allows making instant changes. However, it does not enable large updates because of the limited performance of dynamic data structures69.\n\nFinally, for large updates, the existing index can be reconstructed entirely. For the reconstruction, the index is first decomposed into contig buckets, where each bucket stores contigs extracted from the subgraph induced by the respective annotation column. Then, these buckets are augmented with the new data (either by adding the new sequences directly or by pre-constructing sample graphs from the new sequences and adding contigs extracted from them), and a new MetaGraph index is constructed from these augmented buckets. Notably, this approach uses a non-redundant set of contigs and does not require processing raw data from scratch again. Furthermore, instead of extracting contigs from the old index, it is also possible to use the inputs initially used to construct the old index (for example, the contigs extracted from sample graphs), which can substantially simplify the process.\n\nSpeeding up k-mer matching\n\nFor higher k-mer matching throughput, we implemented several techniques to speed up this procedure. First, when mapping k-mers to a primary graph (defined above), each k-mer may generally have to be searched twice (first that k-mer and then its reverse complement). Nevertheless, if a k-mer has been found, there is no need to search for its reverse complement. In fact, it is guaranteed in that case that the reverse complement k-mer would be missing in the graph. However, if a certain k-mer from the query is missing in the graph but its reverse complement is found, it is likely that, for the next k-mer from the query sequence, which is adjacent to the current one, the same applies. Thus, in such cases, we directly start the search for the next k-mer by querying the graph with the reverse complement and checking for the original k-mer only if that reverse complement k-mer is not found in the graph.\n\nWhen the graph is represented with the BOSS table, indexing k-mer ranges in the BOSS table (as described in Supplementary Information A.3.2) greatly speeds up k-mer lookups, especially relevant when querying short sequences or arbitrary sequences against a primary graph.\n\nAnother optimization consists of querying the annotation matrix in batches, which improves cache locality and removes possible row duplications. To go further and speed up k-mer mapping as well, we developed the batch query algorithm described below in detail.\n\nBatched sequence search\n\nTo increase the throughput of sequence search for large queries (for example, sets of sequencing reads or long sequences), we have designed an additional batch query algorithm schematically shown in Extended Data Fig. 1e. The algorithm exploits possible query set redundancy: the presence of k-mers shared between individual queries. More precisely, query sequences are processed in batches and an intermediate batch graph is constructed from each batch. This batch graph is then effectively intersected with the large joint graph from the MetaGraph index. The result of this intersection operation forms a relatively small subgraph of the joint graph, which we call a query graph. It is represented in a fast-to-query uncompressed format (HashDBG). In practice, this intersection is performed as follows. First, the batch graph is traversed (step 2 in Extended Data Fig. 1e) to extract a non-redundant set of contigs that are afterwards mapped against the joint graph through exact k-mer matching (step 3) and the respective annotations are extracted from the compressed index accordingly to construct the query graph with its respective annotations representing the intersection of the batch graph with the full MetaGraph index (step 4). All of the query sequences from the current batch are then queried against this query graph (step 5). Depending on the structure of the query data, this algorithm achieves a 10- to 100-fold speedup compared to unbatched queries.\n\nSequence search with alignment\n\nFor cases in which the sensitivity of sequence search through exact k-mer matching is insufficient, we developed several approaches for aligning sequences to the MetaGraph index, a process known as sequence-to-graph alignment5,6,23,70,71,72,73,74,75,76. Note that each approach has its target use cases and the choice should be made based on the particular application and the problem setting.\n\nEach alignment algorithm takes a classical seed-and-extend approach27,71,72,73,74. Given an input sequence, the seeds are composed by joining consecutive k-mer matches within the graph’s unitigs (called unitig maximal exact matches, or Uni-MEMs72,77). Although, by default, this restricts the seeds to be at least of length k, representing the graph with the BOSS table allows for relaxing this restriction by mapping arbitrarily short sequences to suffixes of the k-mers indexed in the graph (as described in Supplementary Information A.3.2).\n\nEach seed is extended in the graph forwards and backwards to produce a complete local alignment. Similarly to how GraphAligner78,79 builds on Myers’ algorithm80, our extension algorithm is a generalization of the Smith–Waterman–Gotoh local alignment algorithm with affine gap penalties81. The user can choose to report multiple alignments for each query, which may be found if seeds to multiple locations in the graph are discovered.\n\nWe now describe the extension algorithm in more detail. Given a seed, let \\(s={s}_{1}\\cdots {s}_{{k}^{{\\prime} }}\\) denote the suffix of the query sequence starting from the first character of the seed. We use a dynamic programming table to represent the scores of the best partial alignments. More precisely, each node v has three corresponding integer score vectors S v , E v , and F v of size equal to the query length \\({\\ell }\\). S v [i] stores the best alignment score of the prefix \\({s}_{1}\\cdots {s}_{i}\\) ending at node v. E v [i] and F v [i] represent the best alignment scores of \\({s}_{1}\\cdots {s}_{i}\\) ending with an insertion and deletion at node v, respectively.\n\nLet v S be the first node of the seed S. We define an alignment tree T S = (V S ,E S ) rooted at v S encoding all walks traversed during the search starting from v S , where V S ⊂ V × ℕ contains all the nodes of the paths originating at v S and E S ⊂ V S × V S contains all the edges within these paths. T S is constructed on-the-fly during the seed extension process by extending it with new nodes and edges after each graph traversal step.\n\nAs the size of T S can grow exponentially if all paths are explored (and is, in fact, of infinite size if the graph is cyclic), we traverse the graph and update T S in a score-guided manner. For this, we maintain a priority queue graph nodes and corresponding score vectors to be traversed, prioritizing nodes whose traversal led to the best local score update78. We use several heuristics to restrict the alignment search space. First, we use the X-drop criterion82,83, skipping an element if it is more than X units lower than the current best-computed alignment score. Moreover, we maintain an aggregated score column for each graph node storing the element-wise maximum score achieved among the score columns of each node in T S . Using this, we discard nodes in T S from further consideration if their traversal did not update the aggregate score column. Finally, we apply a restriction on the total number of nodes which can be explored as a constant factor of \\({\\ell }\\).\n\nTo find seeds of length k′ < k (by default, we use a seed length of 19) matching the suffixes of nodes in the canonical graph, a three-step approach is taken. First, seeds corresponding to the forward orientation of the query are found, which correspond to contiguous node ranges in the BOSS representation of the graph (a description of the node range matching algorithm is provided in Supplementary Information A.3.2). The next two steps then retrieve suffix matches which are represented in their reverse complement form in the graph. In the second step, the reverse complements of the query k-mers are searched to find node ranges corresponding to suffix matches of length k′. Finally, these ranges are traversed forwards k − k′ steps in the graph to make the prefixes of these nodes correspond to the sequence matched. The reverse complements of these nodes are then returned as the remaining suffix matches.\n\nWhile primary graphs act as an efficient representation of canonical de Bruijn graphs, special considerations need to be made when aligning to these graphs to ensure that all paths that are present in the corresponding canonical graph are still reachable. For this, we introduce a further extension of the alignment algorithm to allow for alignment to an implicit canonical graph while only keeping a primary graph in memory. During seed extension, the children of a given node are determined simply by finding the children of that node in the primary graph, along with the parents of its reverse complement node. Finding exact matching seeds of length k′ ≥ k can be achieved in a similar manner, searching for both the forward and reverse complement of each k-mer in the primary graph.\n\nMetaGraph maintains three different alignment approaches that determine how the graph is traversed during seed extension, called MetaGraph-Align, SCA and TCG-Aligner, each applying different restrictions to traversal.\n\nMetaGraph-Align\n\nIn this approach, the sequences are aligned against the joint de Bruijn graph to compute their respective closest walks in the graph. After computing a set of alignments, they are used in place of the original sequences to fetch their corresponding annotations. This approach allows for aligning to paths representing recombinations of sequences across annotation labels.\n\nLabel-consistent graph alignment\n\nWhen label recombination is not desired, we support an alternative approach in which queries are aligned to subgraphs of the joint graph induced by single annotation labels (columns of the annotation matrix). We call this approach label-consistent graph alignment (or alignment to columns), and it is implemented by the SCA algorithm6. However, instead of aligning to all the subgraphs independently, we perform the alignment with a single search procedure while keeping track of the annotations corresponding to the alignments.\n\nTrace-consistent graph alignment (TCG-Aligner)\n\nFinally, when input sequences are losslessly encoded in a MetaGraph index using the methodology introduced and evaluated previously5, the alignment can be done against those original input sequences of which the respective walks in the graph are called traces. This method is called the trace-consistent graph aligner (TCG-Aligner)5.\n\nColumn transformations\n\nIn addition to the operations mentioned above, MetaGraph supports operations aggregating multiple annotation columns to compute statistics for the k-mers and their counts (abundances). In general, the following formula is used in the aggregation to compute the ith bit of the new annotation column:\n\n$${a}_{\\min }\\le \\mathop{\\sum }\\limits_{j=1}^{m}1[{v}_{\\min }\\le {c}_{{ij}}\\le {v}_{\\max }]\\le {a}_{\\max },$$\n\nwhere c ij is the count (abundance) of the ith k-mer in the jth label, and 1[A] is a Boolean predicate function that evaluates as 1 if the statement A is true and as 0 otherwise. If no counts are associated with the column, we assume that c ij = 1 for every set bit in the jth annotation column and 0 otherwise. If the sum \\({\\sum }_{j=1}^{m}1[{v}_{\\min }\\le {c}_{{ij}}\\le {v}_{\\max }]\\) falls within specified minimum and maximum abundance thresholds a min and a max , the bit in the aggregated column for this k-mer is set to 1, and the value of the sum is written as the count associated with that bit. In other words, the resulting aggregated column is always supplemented with a count vector representing the number of original annotation columns with k-mer counts between v min and v max , which can be used in downstream analyses as an ordinary count vector.\n\nSeamless distribution and interactive use\n\nIn addition to the single-machine use case, where the index is constructed and queried locally, MetaGraph also supports querying indexes provided on a remote server through a client-server architecture. In this approach, a set of graphs and annotations can easily be distributed across multiple machines. Each machine runs MetaGraph in server mode, hosting one or multiple indexes and awaiting queries on a pre-defined port (Extended Data Fig. 1b). This setup makes it straightforward to execute user queries across all indexes hosted on multiple servers. For easy integration of results and coordination of different MetaGraph instances, we provide client interfaces in Python (Extended Data Fig. 1a). Notably, our distribution approach can be used not only for hosting multiple indexes of distinct sources but also when indexing a single dataset of extremely large size, such as SRA. This distribution approach enables virtually unlimited scalability.\n\nMetaGraph API and Python client\n\nFor querying large graph indexes interactively, MetaGraph offers an API that allows clients to send requests to a single or multiple MetaGraph servers. When started in server mode, the MetaGraph index will be persistently present in server memory, which will accept HTTP requests on a pre-defined port. To make the querying more convenient, we have also implemented a Python API client as a Python package available at GitHub (https://github.com/ratschlab/metagraph/tree/master/metagraph/api/python).\n\nMetaGraph Online\n\nThe search engine MetaGraph Online has a clean and intuitive graphical web user interface (UI; Supplementary Fig. 11), enabling the user to paste an arbitrary sequence and search it against a selected index. Restrictions to search multiple sequences at once are only in place to limit hosting costs. By default, the search is performed through basic k-mer matching. For greater sensitivity, it is also possible for all indexes to additionally align the searched sequence to the annotated graph. If k-mer coordinates or counts are represented in the queried index, the web UI allows retrieving them for the query sequence as well. In addition to user interaction with the web interface, MetaGraph Online provides a web API that allows connecting to the respective servers via their endpoints. That is, any of the hosted indexes can be queried through Python API by connecting to the respective endpoint of the server hosting that index (Extended Data Fig. 1a).\n\nIndexing public read sets from the NCBI SRA\n\nWe have split the set of all read sets from SRA (excluding sequencing technologies with high read error rates, see more details in the sections below) into different groups of related samples and constructed a separate MetaGraph index for each group. The groups were defined either using dataset definitions of previous work32 or using the metadata provided by NCBI SRA. As a result, we constructed the following six datasets: SRA-MetaGut, SRA-Microbe, SRA-Fungi, SRA-Plants, SRA-Human and SRA-Metazoa. All of these datasets are listed in the supplementary tables available at GitHub (https://github.com/ratschlab/metagraph_paper_resources) and make up a total of 4.4 Pbp and 2.3 PB of gzip-compressed input sequences, while the indexes make up only 11.6 TB, which corresponds to the overall compression ratio of 193×, or 376 bp per byte.\n\nFor constructing the SRA-Microbe index, we used cleaned contigs downloaded from the European Bioinformatics Institute FTP file server provided as supplementary data to BIGSI32. Thus, no additional data preprocessing was needed for this dataset.\n\nFor all other datasets, each sample was either transferred and decompressed from NCBI’s mirror on the Google Cloud Platform or, if not available on Google Cloud, downloaded from the ENA onto one of our cloud-compute servers and subjected to k-mer counting with KMC3 (ref. 62) to generate the full k-mer spectrum. If the median k-mer count on the spectrum was less than 2, the sample was further processed without any cleaning. Otherwise, the sample was subjected to cleaning with the standard graph cleaning procedure implemented in MetaGraph, with pruning tips shorter than 2k (for all these datasets k was set to 31) and using an automatically computed k-mer abundance threshold for pruning low-coverage unitigs, with a fallback threshold value of 3. This cleaning procedure was applied for SRA-Fungi, SRA-Plants, SRA-Human and SRA-Metazoa.\n\nFor read sets of the SRA-MetaGut dataset, the sequencing depth was typically low, and we therefore applied a more lenient cleaning strategy. Namely, we switched off the singleton filtering (that is, we initially kept all k-mers that appear only once) on the k-mer spectrum and used a constant cleaning threshold of 2 during graph cleaning to remove all unitigs with a median k-mer abundance of 1.\n\nFor each dataset, we first constructed a joint canonical graph with k = 31 (including for each indexed k-mer its reverse complement) from the cleaned contigs and then transformed it into a primary graph (storing only one form of each k-mer and representing the other implicitly). Finally, using the same cleaned contigs, we annotated the joint primary graph with sample IDs to construct the annotation matrix. Each input sample thereby formed an individual column of the annotation matrix. The annotation matrix was then transformed to the RowDiff<Multi-BRWT> representation for higher compression and faster queries. The graph was, in turn, transformed to the small representation. The exact commands and scripts are available at GitHub (https://github.com/ratschlab/metagraph_paper_resources).\n\nSRA subset composition\n\nHere we provide a detailed description of each of the 6 datasets.\n\nSRA-Microbe\n\nThis dataset was first used to construct the BIGSI index32. Consisting of 446,506 microbial genome sequences, this dataset once posed the largest indexed set of raw sequencing data. However, at the time of performing our experiments, it represented only an outdated snapshot of the corresponding part of the SRA. Nevertheless, we decided to keep the same sequence set for this work to enable direct comparison and benchmarking. A complete list of SRA IDs contained in this set is available as file TableS1_SRA_Microbe.tsv.gz (with further information available in TableS10_SRA_Microbe_McCortex_logs.tsv.gz and TableS11_SRA_Microbe_no_logs.tsv) at GitHub (https://github.com/ratschlab/metagraph_paper_resources). For details on how the set of genomes was selected, we refer to the original publication32.\n\nSRA-Fungi\n\nThis dataset contains all samples from the SRA assigned to the taxonomic ID 4751 (Fungi) specifying the library sources GENOMIC and METAGENOMIC and excluding samples using platforms PACBIO_SMRT or OXFORD_NANOPORE. In total, this amounts to 149,607 samples processed for cleaning. Out of these, 138,158 (92.3%) could be successfully cleaned and were used to assemble the final MetaGraph index. All sample metadata were requested from NCBI SRA on 25 September 2020 using the BigQuery tool on the Google Cloud Platform.\n\nSRA-Plants\n\nThis dataset contains all samples from the SRA assigned to the taxonomic ID 33090 (Viridiplantae), specifying the library source GENOMIC and excluding samples using platforms PACBIO_SMRT or OXFORD_NANOPORE. In total, this amounts to 576,226 samples processed for cleaning. Out of these, 531,736 (92.3%) could be successfully cleaned and were used to assemble the final MetaGraph index. All sample metadata were requested from NCBI SRA on 17 August 2020 using the BigQuery tool on the Google Cloud Platform.\n\nSRA-Human\n\nThis dataset contains all samples of assay type WGS, AMPLICON, WXS, WGA, WCS, CLONE, POOLCLONE, or FINISHING from the SRA assigned to the taxonomic ID 9606 (Homo sapiens) specifying the library source GENOMIC and excluding samples using platforms PACBIO_SMRT or OXFORD_NANOPORE. In total, this amounts to 454,252 samples processed for cleaning. Out of these, 436,502 (96.1%) could be successfully cleaned and were used to assemble the final MetaGraph index. All sample metadata were requested from NCBI SRA on 12 December 2020 using the BigQuery tool on the Google Cloud Platform.\n\nSRA-Metazoa\n\nThis dataset contains all samples from the SRA assigned to the taxonomic ID 33208 (Metazoa) specifying the library source GENOMIC and excluding samples using platforms PACBIO_SMRT or OXFORD_NANOPORE. In total, this amounts to 906,401 samples processed for cleaning. Out of these, 805,239 (88.8%) could be successfully cleaned and were used to assemble the final MetaGraph index. All sample metadata were requested from NCBI SRA on 17 September 2020 using the BigQuery tool on the Google Cloud Platform.\n\nSRA-MetaGut (human gut microbiome)\n\nThis group contains all sequencing samples of the assay type WGS and AMPLICON from the SRA assigned to the taxonomic ID 408170 (human gut metagenome), excluding samples using platforms PACBIO_SMRT and OXFORD_NANOPORE. In total, this amounts to 242,619 samples, where 177,759 (73.3%) were AMPLICON and 64,860 (26.7%) were WGS samples. All these samples were successfully cleaned and were used to assemble the final MetaGraph index. All sample metadata were requested from NCBI SRA on 01 October 2020 using the BigQuery tool on the Google Cloud Platform.\n\nThe complete lists of all samples (including the list of successfully cleaned ones) for each subset are available at GitHub (TableS5_SRA_MetaGut.tsv.gz; https://github.com/ratschlab/metagraph_paper_resources).\n\nIndexing GTEx data\n\nThe 9,759 raw RNA-seq samples of the GTEx project have become a de facto reference set for the study of human transcriptomics43. All available RNA-seq samples that were part of the version 7 release of GTEx were downloaded through dbGaP to our compute cluster of ETH Zurich. A list of all of the samples used is available at GitHub (TableS7_GTEX.txt; https://github.com/ratschlab/metagraph_paper_resources).\n\nEach sample was individually transformed into a graph using k = 31 and then cleaned with the standard graph cleaning algorithm implemented in MetaGraph, with trimming tips shorter than 2k and using an automatically computed coverage threshold with the fallback value of 2 for removing unitigs with low median k-mer abundance. All resulting cleaned contigs were assembled into a joint canonical de Bruijn graph and then transformed to the final primary graph. Using the typical workflow, the primary joint graph was annotated using the cleaned contigs extracted from each sample, generating one label per sample. All individual annotation columns were finally collected into one matrix and transformed into the RowDiff<Multi-BRWT> representation.\n\nWhen performing the indexing with k-mer counts (row ‘GTEx with counts’ in Table 1), we applied an additional smoothing of k-mer counts within cleaned unitigs to facilitate the compression. We used a smoothing window of size 60. That is, for each k-mer of a cleaned unitig, its count was replaced with the median abundance of 30 k-mers before it in that unitig and 30 after. This smoothing window is much smaller than the expected transcript length. However, it was sufficient to considerably reduce the annotation size (from 184 GB when indexing the original counts to 76 GB).\n\nIndexing the TCGA RNA-seq cohort\n\nTCGA has collected RNA-seq samples on the same order of magnitude from primary tumours, spanning across more than 30 cancer types, constituting a central resource for cancer research42. We downloaded the data from the Genomic Data Commons Portal of the NCI. A list containing all processed samples is available as file TableS8_TCGA.tsv.gz at GitHub (https://github.com/ratschlab/metagraph_paper_resources). In total, the index contains 11,095 individual records spanning all available TCGA cancer types. We used the same indexing workflow as for GTEx. Similarly to GTEx, we have also constructed MetaGraph indexes with k-mer counts for TCGA (Table 1).\n\nIndexing environmental metagenome samples (MetaSUB)\n\nThis dataset contains 4,220 WMGS samples (the pilot dataset) collected from the environment through the MetaSUB consortium8. The swabs were collected at different locations and from different objects, where we also contributed to data collection by collecting swabs from benches, ticket machines and various other objects at different tram stops and train stations in Zurich. When sampling, each swab was annotated with additional data, including the location of sampling, the type of object from which the swab was collected, the material of that object, the elevation above or below sea level, and the station or line where the sample was collected. The swabs were then sent for further processing to the sequencing team. For more details about DNA extraction and sequencing, refer to the original publication8.\n\nThe raw data (read sets) can be downloaded using the MetaSUB utils84. A list of all sample IDs used in this study is available as file TableS6_MetaSUB.csv.gz at GitHub (https://github.com/ratschlab/metagraph_paper_resources).\n\nAll input samples were directly assembled into canonical de Bruijn graphs (sample graphs) with k = 41. All graphs were then cleaned with the standard graph cleaning procedure implemented in MetaGraph, with pruning tips shorter than 2k and removing unitigs depending on coverage (automatically computed based on k-mer spectrum). If no threshold could be computed by the algorithm, we used 3 as a fallback value (an evaluation of this cleaning strategy is shown in Supplementary Fig. 15). The cleaned graphs were transformed into primary contigs, which were then used to assemble a joint graph and annotate it. We annotated the graph with sample IDs, which is the most fine-grained annotation we could construct. Thus, each sample was transformed into a single annotation column in the final MetaGraph index. All the annotation columns were finally aggregated into a joint annotation matrix compressed with the RowDiff<Multi-BRWT> representation. The additional metadata, such as the location, the object and the surface material, were written to a separate table and could easily be retrieved for any sample ID.\n\nIndexing the RefSeq and UniParc collections\n\nThe NCBI RefSeq database9 contains a non-redundant collection of genomic DNA sequences (all assembled reference genome sequences), transcripts and proteins.\n\nWe indexed all 32,881,422 nucleotide sequences from release 97 of the RefSeq collection, a total of 1.7 Tbp, which takes 483 GB when compressed with gzip -9, using a de Bruijn graph k-mer index with k = 31. We annotated k-mer coordinates within buckets split by Taxonomy IDs (85,375 annotation columns with tuples of k-mer coordinates). The graph was constructed in basic mode (non-canonical, non-primary), as all the sequences of the collection are assemblies and are therefore of a determined orientation.\n\nAs expected, the compression ratio (the ratio between the compressed input and the index size) is lower than the raw sequencing read sets at 3.3 bp per byte (Table 1). Our index forms an alternative to the commonly used BLAST database27,28,85 for competitive high-throughput search5.\n\nFor amino acids, we indexed the UniParc collection of non-redundant sequences (release 2023_04), containing 210 gigaresidues, using a basic-mode graph and k-mer coordinate annotations to ensure lossless encodings. As expected, the compression ratio is low, at 1.7 amino acids/byte.\n\nIndexing global ocean microbiome (Tara Oceans) data\n\nThis collection (v.1.0) contains 34,815 genomes reconstructed from metagenomic datasets from major oceanographical surveys and time-series studies with high coverage of global ocean microbial communities across ocean basins, depth layers and time11. In addition to metagenome-assembled genomes (MAGs) constructed from 1,038 publicly available metagenomes extracted from ocean water samples collected at 215 globally distributed sampling sites, the collection includes a set of single amplified genomes and reference genome sequences of marine bacteria and archaea from other existing databases. For more details on the data composition, refer to the original publication11.\n\nWe constructed an index for this collection (summary information is provided in Table 1) using a de Bruijn graph of order k = 31 constructed in the basic mode. This index encodes the coordinates of the k-mers within individual assembled genomes and therefore losslessly represents the input sequences. Notably, it still achieves a compression ratio of 4.2 bp per byte. We also indexed the raw assembled scaffolded contigs (360 gigabases) in an annotated graph with 318 million annotation labels. Owing to the very large number of annotation columns, in contrast to the annotation matrices in other indexes typically represented in the Multi-BRWT format, for this index, we represent the RowDiff-transformed annotation matrix in the RowFlat format for fast row queries.\n\nIndexing a subset of the Logan dataset\n\nBuilding on the publicly available Logan resource13, we grouped samples available in Logan (v.1.0) into subsets based on their phylogenetic relation. On the basis of the organism field in the metadata, we grouped samples together if at least 10,000 samples shared the same label. For the remainder, we mapped all samples to the NCBI taxonomy. Beginning at the species level, we aggregated samples into the same group, if they shared the same taxonomic assignment and formed a group of at least 1,000 samples. For the remaining samples yet ungrouped, we repeated the taxonomic grouping with the next-higher taxonomic levels (genus, family, order, superclass, kingdom, superkingdom) in the same manner. Yet ungrouped samples were assigned to a special group other. We performed the above procedure separately for DNA and RNA samples. Once a group was formed, we split it in subgroups based on the number of cumulative unique contig-level k-mers based on the Logan metadata. We processed the samples in order of reverse release date and started a new subgroup whenever the cumulative k-mer count of 500 billion was exceeded; except for groups metagenome and other, and groups above the superclass level. We indexed each sub-group independently, directly downloading the Logan contig-level samples from the cloud repository. If the contig-level sample was not available, we used the unitig-level sample. We tried downloading each sample up to ten times and marked the sample as not found if none of the attempts were successful. After download, we built a joint canonical graph from all inputs, without cleaning. We then transformed the canonical graph into its primary representation. We then built a joint annotation on the primary graph from the same input samples and subsequently computed the RowDiff sparsification of all annotation columns. Finally, all annotation columns were compressed with Multi-BRWT compression using the default settings. Optionally, we transformed the primary graph from its default stat representation into small.\n\nIndexing the UHGG\n\nWe built two indexes using the assemblies from v.1.0 of the dataset. The first index, the UHGG catalogue, contains 4,644 reference genomes, while the other index contains 286,997 non-redundant genomes.\n\nExperiments\n\nThis section summarizes the experimental setup for the different results presented in this work.\n\nBenchmarking Mantis, Themisto, Bifrost and Fulgor\n\nFor indexing and querying k-mer sets with these lossless indexing tools, we used Mantis (v.0.2.0), Themisto (v.3.2.2), Bifrost (v.1.3.5) and Fulgor (v.3.0.0).\n\nBenchmarking COBS and kmindex\n\nWhen indexing subsets of the collection of bacterial and viral genomic read sets32 in our evaluation experiments, we used COBS33 (commit 1cd6df2) with four hash functions and the target false-positive rate of 5%. For kmindex (v.0.5.3), we partitioned the samples into groups based on their k-mer counts, with one group per order of magnitude. We then constructed 28-mer indexes with false-positive rates of 5% for each group to leverage the findere algorithm86 for querying 31-mers at a reduced false-positive rate.\n\nExperiment discovery on SRA graphs\n\nWe evaluated each graph using 300 randomly selected samples from their respective input samples. To generate a query file for a graph, we randomly selected 100 reads (or the entire read set if fewer than 100 reads are available) from each of the 300 selected samples, resulting in query files of 30,000 reads per graph. To generate auxiliary reads with errors, we selected subsets of the original query sets such that 10 random reads would be selected from each sample. We then introduced substitution errors to these reads with probabilities 0.1%, 1%, 2%, 5% and 10% and insertions–deletions at 10% of the substitution probability using Mutation-Simulator87 (v.3.0.1). Given these read sets, we then discarded all reads of which the median k-mer multiplicities were below the unitig cleaning thresholds determined by the cleaning procedure (see the ‘Graph cleaning’ section of the Methods).\n\nWe evaluated experiment discovery at two different levels of granularity: (1) mapping to individual sample graphs (Extended Data Fig. 3a); and (2) mapping to joint annotated graphs (Fig. 3a,b). When mapping to joint graphs, we considered only mapping results that retrieved the ground-truth label of each query read. For all granularities, we mapped the reads through both exact k-mer matching and label-consistent sequence-to-graph alignment using SCA6. We measure how well the reads aligned as the percentage of characters in the query that are covered by at least one reported mapping.\n\nHuman gut resistome and phageome exploration\n\nWe queried all AMR genes from the Comprehensive Antibiotic Resistance Database (CARD) database (v.3.2.7)44 and all bacteriophages from RefSeq Release 218 (ref. 45). We selected bacteriophages by selecting all viral sequences with the term phage in their header. We mapped these sequences to all accessions in the SRA-MetaGut index representing WMGS samples. We recorded an accession as a match to a query if at least 80% of the query’s k-mers exactly matched a k-mer in the accession. We then reduced the pools of AMR genes, SRA accessions and RefSeq bacteriophages to those for which at least one match was found.\n\nTo measure the degree of association for each AMR gene family–bacteriophage pair, we computed two binary vectors where each index represents a gut microbiome sample. The first vector indicates the presence of at least one gene match from the gene family and the second vector indicates a match to the phage. We then measure the association using the Matthews correlation coefficient (denoted by Corr MCC ) if both vectors indicate at least 5 present matches (value 1) and at least 5 absent matches (value 0).\n\nWhen measuring the growth of resistance to antibiotics over time in each continent, we normalized the counts in the confusion matrix before computing Corr MCC to correct for differing numbers of samples deposited in each year and from each continent. If we denote the number of accessions from a continent C by n C and the number of accessions from a year Y by n Y , we normalized the counts by letting each accession from continent C and year Y contribute a count of \\(c=\\frac{{n}_{N}}{{n}_{C}\\cdot {n}_{Y}}\\) instead of 1. n N is a scaling factor applied to the four counts in the confusion matrix so that their sum equals the total number of accessions considered. We use the same scaling factor c for each count when fitting a linear regression model to the match counts to determine drug-resistance growth over time for each continent.\n\nWe compute P values for each gene-to-phage correlation Corr MCC and each drug resistance growth linear regression slope s through permutation testing. For each analysis, we compute 100 permutations of the antibiotic and gene family indicator vectors, respectively, and compute P values relative to the resulting null distributions.\n\nIn Fig. 4a, we only plot a gene family or a phage if it has at least one significant correlation with Corr MCC > 0.25. In Fig. 4b, we report all antibiotics for which we measure statistically significant growth in at least one continent (modelled through a binomial GLM using the Python statsmodels package v.0.14.0). All P values are corrected using the Benjamini–Yekutieli procedure to a family-wise error rate of 0.05 and are considered to be significant if they are P < 0.05 after correction (using the Python scipy package v.1.11.3).\n\nSurvey of BSJs\n\nTo generate the list of candidate trans-junctions, we iterated over all genes present in the GENCODE annotation (v.38) and generated for all transcripts a list of hypothetical BSJs. Assuming that a transcript consists of exons e1, e2, e3 and e4, we would connect the donor site for all exons starting from exon e2 with the acceptor site of all previous exons. The example transcript above would generate the following BSJ candidates: e2-e1, e3-e2, e3-e1, e4-e3, e4-e2, e4-e1. We included only exons with a length of at least 30 bp (which equals k − 1 for k = 31). For all junctions, we extracted the genomic sequences in a ±1.5 × k window around the junction, resulting in query sequences of 90 bp in length. We aligned all 4,052,768 candidate queries against the TCGA and GTEx MetaGraph indexes using the default metagraph align regime. Moreover, we also aligned all queries to the GENCODE (v.38) reference transcriptome using bwa-mem (v.0.7.17-r1188)88 and against the hg38 human reference genome (GRCh38.p13, packaged with GENCODE v.38) using STAR (v.2.7.0 f)89. We conservatively retained only full-length matches against the graph, which removed most of our candidates, retaining 10,257 and 10,369 candidates for TCGA and GTEx, respectively. After making the candidate list unique over sequences and removing genome and transcriptome hits from the previous STAR and bwa-mem alignments, we retained 2,536 and 1,248 candidates for TCGA and GTEx, respectively. For our validation set, we downloaded accession GSE141693 from the Gene Expression Omnibus, containing experimentally confirmed circular RNAs48. We lifted over all annotations from hg19 to hg38 using the UCSC LiftOver tool90, which could map all but 70 of the 87,555 circular RNA annotations. For all junctions, we extracted queries as windows ±1.5 × k around the junctions, resulting in 90-bp query sequences. Alignment against MetaGraph indexes, the reference genome and transcriptome, as well as subsequent filtering, was analogous to the in silico generated BSJ queries.\n\nTheoretical model for exact k-mer matching\n\nTo derive a theoretically expected number of random matches to our 100-study subset (Fig. 5c), we assume that a random sequence containing 100 k-mers (the average length in the query set used for Fig. 5a) is matched against a sample containing 252,631,773 k-mers (the average sample size in our 100-study subset). We use a hypergeometric distribution (that is, random sampling without replacement) to compute the probability of the query sequence having at least one matching k-mer in a sample. Using this as a success probability for a Bernoulli random variable, we multiply this probability by the number of samples in our 100-study subset to get the expected number of matches to the subset. We then scale this up to the entire SRA.\n\nCloud-based query experiments\n\nOur index files computed from the Logan resource are available on AWS S3 storage. Analogous to the indexes formed from taxonomic grouping of input samples, we also generated 50 disjoint input sets, each aggregating the raw data of 100 randomly selected SRA studies. For 47 of the groups, we were able to construct a MetaGraph index using the same strategy as for the other Logan-based indexes. For three groupings, no index could be constructed because exceptionally large studies were selected, preventing the assembly of a single joint index. In addition to these sets, we also selected a subset of 21 index groups from the Logan set that contained only metagenome samples, as they represent the largest input diversity. Specifically, we selected metagenome groups 30, 50, 51, 57, 77, 85, 87, 110, 125, 133, 169, 184, 189, 192, 248, 253, 273, 316, 338, 376 and 385. Based on the total number of unique k-mers in each index group, we were able to extrapolate our query results to the entire index set.\n\nWe selected reads from the samples used to construct the 100-studies index described in Table 1 to construct query sets of varying sizes. First, we selected 300 random samples from this set. Then, given an integer N, we selected N random reads from each of these 300 samples and filtered these by selecting reads of length at least 100 bp and at most 250 bp. We generated query sets for N ∈ {1, 50, 500, 5,000, 50,000, 5,000,000}. Moreover, we selected 1 out of every 20 reads from the N = 1 query set to generate our smallest query set.\n\nWe used an AWS Cloud Formation template (https://github.com/ratschlab/metagraph-open-data) to deploy the querying infrastructure, including AWS Batch compute environments and job queues on r6in, hpc7g and hpc7a EC2 instances, a Step Function and Lambdas for job scheduling and cost estimations. On this setup, we searched all query files in all 47 random study indexes. Smaller queries, for which the execution time is bounded by downloading the index, were processed on r6in instances, while the larger queries, for which the execution time is bounded by the actual runtime, were processed on hpc7g instances, or, when an index did not fit in the available RAM, on hpc7a. After processing all indexes, the total execution time of active EC2 instances was computed to estimate the EC2-related costs of the query. ‘Mountpoint for S3’ was used to avoid staging data on disk and load it directly into RAM instead. Below are the exact MetaGraph CLI parameters that were used for running the queries. Exact match queries: --batch-size 8000000 -p $(nproc) --threads-each 1, 32 or 48 --query-mode labels --min-kmers-fraction-label 0.7 --min-kmers-fraction-graph 0.0 --num-top-labels inf. Fast alignment queries: --batch-size 200000 -p $(nproc) --align-min-exact-match 0.85 --align-min-seed-length 27. Sensitive alignment queries: --batch-size 200000 -p $(nproc) --align-min-exact-match 0.0 --align-min-seed-length 27. A --threads-each parameter was chosen based on the instance type for r6in, hpc7g and hpc7a instances, and $(nproc) depends on the specific instance on which the query was processed. The GitHub repository provides further details on a reproducible deployment of the AWS querying infrastructure.\n\nThe architecture and the implementation of MetaGraph Online\n\nThe architecture of the MetaGraph Online service is schematically shown in Supplementary Fig. 10. The backend server (Supplementary Fig. 10 (middle)), which is implemented as a Flask v.2.3.0 application, provides a web interface and generates dynamic web pages for submitting search queries and viewing the query results. The user queries are processed and transformed into a search query, which is then passed to the remote servers hosting the MetaGraph indexes (Supplementary Fig. 10 (right)). Once the query has been executed by the respective MetaGraph server, the results are sent back to the backend server and an aggregated summary is rendered to the user. Each MetaGraph index is hosted on a remote server by running the MetaGraph tool in server mode (Supplementary Fig. 10 (right)). The server applications run independently and are distributed across the available machines. Each MetaGraph server receives HTTP requests formed by the central backend server on user search requests. The communication between the central backend server and the other remote MetaGraph servers happens through the Python v.3.9 API. For seamless compatibility, we also made the backend server redirect user requests and provide the same web API for querying MetaGraph servers (for example, from Python or as a simple HTTP request) as if the MetaGraph tool is locally running in server mode. The backend of MetaGraph Online is implemented as a Flask application. This web application is deployed in a Docker container (v.1.13.1; API v.1.26) using the Nginx (v.1.16.1) server as a backend. For each search query from the user, it forms a request accordingly and sends it through the Python API client to the MetaGraph servers hosting the indexes. We run these MetaGraph servers in Docker containers on the same or other machines in a federated manner. Moreover, the web application emulates the usual MetaGraph API by redirecting all requests to the respective individual MetaGraph servers hosting the indexes.\n\nReporting summary\n\nFurther information on research design is available in the Nature Portfolio Reporting Summary linked to this article.",
      "url": "https://www.nature.com/articles/s41586-025-09603-w",
      "source": "Nature",
      "published": "2025-10-09",
      "sentiment_score": 0.85,
      "reasoning": "The article reports a significant breakthrough in bioinformatics technology, MetaGraph, which enables efficient, scalable, and highly compressed indexing and querying of petabase-scale DNA, RNA, and protein sequence data. This advancement has broad implications for genomics, metagenomics, and biomedical research by facilitating large-scale sequence search, annotation, and analysis across vast public datasets such as SRA, GTEx, TCGA, and environmental metagenomes. The detailed description of algorithms, data structures, and indexing strategies demonstrates substantial substance and technical depth. The technology's scalability and compression enable meaningful real-world impact by improving accessibility and analysis of massive biological sequence repositories, benefiting researchers and society at large.",
      "category": "Technology",
      "personality_title": "MetaGraph enables fast, compressed search of petabyte-scale genetic data",
      "personality_presentation": "**Context**\nScientists study DNA, RNA, and proteins to understand living things. Huge collections of genetic data exist, but searching through all that information is very hard because it takes a lot of computer power and storage space.\n\n**What happened**\nResearchers developed MetaGraph, a new tool that can quickly organize and search through massive genetic databases containing trillions of sequence pieces. It uses special data structures called de Bruijn graphs to store short DNA or protein segments efficiently. MetaGraph compresses the data a lot, making huge datasets smaller and faster to search. It can handle data from many sources, like human genomes, microbes, plants, and environmental samples, and lets users search sequences through a website or programming tools.\n\n**Impact**\nMetaGraph’s ability to index petabases (millions of billions of base pairs) of sequence data with high compression means scientists can explore enormous genetic datasets that were previously too large to use easily. This helps researchers find genetic information across many studies, improving understanding of biology and disease. For example, MetaGraph indexed millions of human RNA samples and environmental DNA, making it easier to detect genes, viruses, or bacteria across diverse samples.\n\n**What's next step**\nThe MetaGraph team will continue improving the tool’s speed and compression, add new ways to analyze genetic data, and expand its use to more datasets. They also offer an online search engine and software so researchers worldwide can access and query huge sequence collections without needing powerful local computers.\n\n**One-sentence takeaway**\nMetaGraph is a powerful new tool that makes searching vast genetic databases faster and more efficient by compressing and indexing petabyte-scale DNA and protein data.\n",
      "personality_title_fr": "MetaGraph permet une recherche rapide et compressée de données génétiques à l'échelle du pétaoctet",
      "personality_presentation_fr": "**Contexte**\nLes scientifiques étudient l'ADN, l'ARN et les protéines pour comprendre les êtres vivants. Il existe d'énormes collections de données génétiques, mais les parcourir est difficile car cela demande beaucoup de puissance informatique et d'espace de stockage.\n\n**Ce qui s'est passé**\nDes chercheurs ont créé MetaGraph, un nouvel outil capable d'organiser et de rechercher rapidement dans d'immenses bases de données génétiques contenant des billions de segments de séquences. Il utilise des structures de données spéciales appelées graphes de de Bruijn pour stocker efficacement de courts segments d'ADN ou de protéines. MetaGraph compresse beaucoup les données, rendant ces énormes ensembles plus petits et plus rapides à explorer. Il peut gérer des données de nombreuses sources, comme les génomes humains, les microbes, les plantes et les échantillons environnementaux, et permet aux utilisateurs de rechercher des séquences via un site web ou des outils de programmation.\n\n**Impact**\nLa capacité de MetaGraph à indexer des pétaoctets (millions de milliards de paires de bases) de données de séquences avec une forte compression signifie que les chercheurs peuvent explorer d'énormes bases génétiques auparavant trop volumineuses pour être utilisées facilement. Cela aide à trouver des informations génétiques à travers de nombreuses études, améliorant la compréhension de la biologie et des maladies. Par exemple, MetaGraph a indexé des millions d'échantillons d'ARN humain et d'ADN environnemental, facilitant la détection de gènes, virus ou bactéries dans divers échantillons.\n\n**Prochaine étape**\nL'équipe MetaGraph continuera à améliorer la vitesse et la compression de l'outil, ajoutera de nouvelles méthodes d'analyse des données génétiques et étendra son utilisation à plus de jeux de données. Ils proposent aussi un moteur de recherche en ligne et des logiciels pour que les chercheurs du monde entier puissent accéder et interroger d'immenses collections de séquences sans avoir besoin d'ordinateurs puissants localement.\n\n**Message clé en une phrase**\nMetaGraph est un outil puissant qui rend la recherche dans d'immenses bases de données génétiques plus rapide et plus efficace grâce à la compression et à l'indexation de données ADN et protéiques à l'échelle du pétaoctet.\n",
      "personality_title_es": "MetaGraph permite búsquedas rápidas y comprimidas en datos genéticos a escala petabyte",
      "personality_presentation_es": "**Contexto**\nLos científicos estudian el ADN, ARN y proteínas para entender los seres vivos. Existen enormes colecciones de datos genéticos, pero buscar en ellas es difícil porque requiere mucha potencia informática y espacio de almacenamiento.\n\n**Qué pasó**\nInvestigadores desarrollaron MetaGraph, una nueva herramienta que puede organizar y buscar rápidamente en enormes bases de datos genéticas con billones de fragmentos de secuencias. Usa estructuras especiales llamadas grafos de de Bruijn para almacenar segmentos cortos de ADN o proteínas de manera eficiente. MetaGraph comprime mucho los datos, haciendo que estos conjuntos enormes sean más pequeños y rápidos de buscar. Puede manejar datos de muchas fuentes, como genomas humanos, microbios, plantas y muestras ambientales, y permite a los usuarios buscar secuencias a través de un sitio web o herramientas de programación.\n\n**Impacto**\nLa capacidad de MetaGraph para indexar petabytes (millones de miles de millones de pares de bases) de datos de secuencias con alta compresión significa que los científicos pueden explorar enormes bases genéticas que antes eran demasiado grandes para usarse fácilmente. Esto ayuda a encontrar información genética en muchos estudios, mejorando la comprensión de la biología y las enfermedades. Por ejemplo, MetaGraph indexó millones de muestras de ARN humano y ADN ambiental, facilitando la detección de genes, virus o bacterias en muestras diversas.\n\n**Próximo paso**\nEl equipo de MetaGraph seguirá mejorando la velocidad y la compresión de la herramienta, añadirá nuevas formas de analizar datos genéticos y ampliará su uso a más conjuntos de datos. También ofrecen un motor de búsqueda en línea y software para que investigadores de todo el mundo puedan acceder y consultar grandes colecciones de secuencias sin necesitar computadoras potentes localmente.\n\n**Resumen en una frase**\nMetaGraph es una herramienta poderosa que hace que la búsqueda en enormes bases de datos genéticas sea más rápida y eficiente al comprimir e indexar datos de ADN y proteínas a escala petabyte.\n",
      "image_url": "public/images/news_image_Efficient-and-accurate-search-in-petabase-scale-se.png",
      "image_prompt": "A detailed, warm-toned painting of an intricate, glowing network of interconnected loops and strands symbolizing overlapping sequences, woven like a vast tapestry of luminous threads converging into compact, organized clusters that represent efficient data structures—set against a soft, natural background of muted earth tones emphasizing harmony and complexity without any human figures."
    }
  ]
}