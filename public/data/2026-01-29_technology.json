{
  "personality": null,
  "timestamp": "2026-01-29T05:20:20.490480",
  "category": "Technology",
  "news_summary": "Advancements in AI self-communication, quantum sensing for dark matter, multimodal learning techniques, and precise 3D atomic structure analysis highlight significant progress in intelligent systems and material science.",
  "news_summary_fr": "Les progrès réalisés dans les domaines de l'auto-communication par IA, de la détection quantique de la matière noire, des techniques d'apprentissage multimodal et de l'analyse précise de la structure atomique en 3D soulignent les avancées significatives accomplies dans les systèmes intelligents et la science des matériaux.",
  "news_summary_es": "Los avances en la autocomunicación de la IA, la detección cuántica de la materia oscura, las técnicas de aprendizaje multimodal y el análisis preciso de la estructura atómica en 3D ponen de relieve los importantes progresos realizados en los sistemas inteligentes y la ciencia de los materiales.",
  "articles": [
    {
      "title": "AI that talks to itself learns faster and smarter",
      "summary": "AI may learn better when it’s allowed to talk to itself. Researchers showed that internal “mumbling,” combined with short-term memory, helps AI adapt to new tasks, switch goals, and handle complex challenges more easily. This approach boosts learning efficiency while using far less training data. It could pave the way for more flexible, human-like AI systems.",
      "content": "Talking to yourself may feel uniquely human, but it turns out this habit can also help machines learn. Internal dialogue helps people organize ideas, weigh choices, and make sense of emotions. New research shows that a similar process can improve how artificial intelligence learns and adapts. In a study published in Neural Computation, researchers from the Okinawa Institute of Science and Technology (OIST) found that AI systems perform better across many tasks when they are trained to use inner speech alongside short-term memory.\n\nThe findings suggest that learning is shaped not only by the structure of an AI system, but also by how it interacts with itself during training. As first author Dr. Jeffrey Queißer, Staff Scientist in OIST's Cognitive Neurorobotics Research Unit, explains, \"This study highlights the importance of self-interactions in how we learn. By structuring training data in a way that teaches our system to talk to itself, we show that learning is shaped not only by the architecture of our AI systems, but by the interaction dynamics embedded within our training procedures.\"\n\nHow Self Talk Improves AI Performance\n\nTo test this idea, the researchers combined self-directed internal speech, described as quiet \"mumbling,\" with a specialized working memory system. This approach allowed their AI models to learn more efficiently, adjust to unfamiliar situations, and handle multiple tasks at once. The results showed clear gains in flexibility and overall performance compared with systems that relied on memory alone.\n\nBuilding AI That Can Generalize\n\nA central goal of the team's work is content agnostic information processing. This refers to the ability to apply learned skills beyond the exact situations encountered during training, using general rules rather than memorized examples.\n\n\"Rapid task switching and solving unfamiliar problems is something we humans do easily every day. But for AI, it's much more challenging,\" says Dr. Queißer. \"That's why we take an interdisciplinary approach, blending developmental neuroscience and psychology with machine learning and robotics amongst other fields, to find new ways to think about learning and inform the future of AI.\"\n\nWhy Working Memory Matters\n\nThe researchers began by examining memory design in AI models, focusing on working memory and its role in generalization. Working memory is the short-term ability to hold and use information, whether that means following instructions or doing quick mental calculations. By testing tasks with different levels of difficulty, the team compared various memory structures.\n\nThey found that models with multiple working memory slots (temporary containers for pieces of information) performed better on challenging problems, such as reversing sequences or recreating patterns. These tasks require holding several pieces of information at once and manipulating them in the correct order.\n\nWhen the team added targets that encouraged the system to talk to itself a specific number of times, performance improved even further. The biggest gains appeared during multitasking and in tasks that required many steps.\n\n\"Our combined system is particularly exciting because it can work with sparse data instead of the extensive data sets usually required to train such models for generalization. It provides a complementary, lightweight alternative,\" Dr. Queißer says.\n\nLearning to Learn in the Real World\n\nThe researchers now plan to move beyond clean, controlled tests and explore more realistic conditions. \"In the real world, we're making decisions and solving problems in complex, noisy, dynamic environments. To better mirror human developmental learning, we need to account for these external factors,\" says Dr. Queißer.\n\nThis direction supports the team's broader aim of understanding how human learning works at a neural level. \"By exploring phenomena like inner speech, and understanding the mechanisms of such processes, we gain fundamental new insights into human biology and behavior,\" Dr. Queißer concludes. \"We can also apply this knowledge, for example in developing household or agricultural robots which can function in our complex, dynamic worlds.\"",
      "url": "https://www.sciencedaily.com/releases/2026/01/260127112130.htm",
      "source": "Latest Science News -- ScienceDaily",
      "published": "2026-01-28",
      "sentiment_score": 0.85,
      "reasoning": "The article reports a significant breakthrough in AI learning methods by incorporating internal self-talk and working memory, leading to more efficient, flexible, and human-like AI systems. This advancement has broad implications for AI applications in real-world dynamic environments, benefiting society by enabling smarter, adaptable technologies.",
      "category": "Technology",
      "personality_title": "AI learns faster by talking to itself like humans do",
      "personality_presentation": "**Context** – People often talk to themselves to think clearly and solve problems. Researchers wondered if teaching artificial intelligence (AI) to 'talk to itself' could help it learn better.\n\n**What happened** – Scientists at the Okinawa Institute of Science and Technology trained AI systems to use inner speech, or quiet self-talk, combined with short-term memory. This method helped AI quickly adapt to new tasks, switch between goals, and solve complex problems. The study showed these AI systems learned more efficiently and needed less training data.\n\n**Impact** – This is important because it makes AI more flexible and human-like. Instead of just memorizing examples, the AI can apply what it learns to new situations. This could improve how robots or smart machines work in real-life, changing tasks like household chores or farming.\n\n**What's next step** – Researchers plan to test their AI in more realistic and noisy environments, similar to the real world. They want to better understand how human learning works and use that knowledge to build smarter machines that handle everyday challenges.\n\n**One-sentence takeaway** – Teaching AI to quietly talk to itself helps it learn faster, adapt better, and work more like a human in changing situations.",
      "personality_title_fr": "L’IA apprend plus vite en se parlant à elle-même comme les humains",
      "personality_presentation_fr": "**Contexte** – Les gens se parlent souvent à eux-mêmes pour mieux réfléchir et résoudre des problèmes. Des chercheurs ont voulu savoir si apprendre à l’intelligence artificielle (IA) à « se parler » pouvait l’aider à mieux apprendre.\n\n**Ce qui s’est passé** – Des scientifiques de l’Institut des sciences et technologies d’Okinawa ont entraîné des systèmes d’IA à utiliser un discours intérieur, ou un murmure silencieux, combiné à une mémoire à court terme. Cette méthode a permis à l’IA de s’adapter rapidement à de nouvelles tâches, de changer d’objectifs et de résoudre des problèmes complexes. L’étude montre que ces IA apprennent plus efficacement et ont besoin de moins de données.\n\n**Impact** – C’est important car cela rend l’IA plus flexible et proche de l’humain. Au lieu de simplement mémoriser des exemples, l’IA peut appliquer ce qu’elle apprend à de nouvelles situations. Cela pourrait améliorer le travail des robots ou des machines intelligentes dans la vie réelle, comme pour les tâches ménagères ou l’agriculture.\n\n**Prochaine étape** – Les chercheurs prévoient de tester leur IA dans des environnements plus réalistes et bruyants, proches du monde réel. Ils veulent mieux comprendre comment les humains apprennent et utiliser ces connaissances pour construire des machines plus intelligentes qui gèrent les défis quotidiens.\n\n**Résumé en une phrase** – Apprendre à l’IA à se parler en silence l’aide à apprendre plus vite, à mieux s’adapter et à fonctionner davantage comme un humain dans des situations changeantes.",
      "personality_title_es": "La IA aprende más rápido hablándose a sí misma como los humanos",
      "personality_presentation_es": "**Contexto** – Las personas a menudo se hablan a sí mismas para pensar mejor y resolver problemas. Los investigadores se preguntaron si enseñar a la inteligencia artificial (IA) a « hablar consigo misma » podría ayudarla a aprender mejor.\n\n**Qué pasó** – Científicos del Instituto de Ciencia y Tecnología de Okinawa entrenaron sistemas de IA para usar un discurso interno, o murmullo silencioso, junto con memoria a corto plazo. Este método ayudó a la IA a adaptarse rápido a nuevas tareas, cambiar objetivos y resolver problemas complejos. El estudio mostró que estas IA aprendieron de manera más eficiente y con menos datos.\n\n**Impacto** – Esto es importante porque hace que la IA sea más flexible y parecida a los humanos. En lugar de solo memorizar ejemplos, la IA puede aplicar lo que aprende a situaciones nuevas. Esto podría mejorar cómo trabajan los robots o máquinas inteligentes en la vida real, en tareas como las del hogar o la agricultura.\n\n**Próximo paso** – Los investigadores planean probar su IA en ambientes más realistas y ruidosos, similares al mundo real. Quieren entender mejor cómo aprenden los humanos y usar ese conocimiento para crear máquinas más inteligentes que enfrenten los desafíos diarios.\n\n**Frase clave** – Enseñar a la IA a hablarse en silencio le ayuda a aprender más rápido, adaptarse mejor y funcionar más como un humano en situaciones cambiantes.",
      "image_url": "public/images/news_image_AI-that-talks-to-itself-learns-faster-and-smarter.png",
      "image_prompt": "A warm, detailed painting of a glowing, abstract neural network softly conversing with itself through intertwined, luminous threads resembling gentle whispers, surrounded by floating, translucent memory slots like softly glowing bubbles, all rendered in natural, muted earth tones and soft blues against a calm, minimal background."
    },
    {
      "title": "Constraints on axion dark matter by distributed intercity quantum sensors",
      "summary": "Nature, Published online: 28 January 2026; doi:10.1038/s41586-025-10034-wAmplification and optimal noise filtering in hyperpolarized noble-gas spins of observations from distributed intercity quantum sensors monitoring for unexpected transient rotations of polarized spins set parameter range constraints in the search for axion dark matter.",
      "content": "Bertone, G. & Hooper, D. History of dark matter. Rev. Mod. Phys. 90, 045002 (2018).\n\nPospelov, M. et al. Detecting domain walls of axionlike models using terrestrial experiments. Phys. Rev. Lett. 110, 021803 (2013).\n\nAfach, S. et al. Search for topological defect dark matter with a global network of optical magnetometers. Nat. Phys. 17, 1396–1401 (2021).\n\nDerevianko, A. & Pospelov, M. Hunting for topological dark matter with atomic clocks. Nat. Phys. 10, 933–936 (2014).\n\nBuschmann, M., Dessert, C., Foster, J. W., Long, A. J. & Safdi, B. R. Upper limit on the QCD axion mass from isolated neutron star cooling. Phys. Rev. Lett. 128, 091102 (2022).\n\nGorenstein, P. & Tucker, W. Astronomical signatures of dark matter. Adv. High Energy Phys. 2014, 878203 (2014).\n\nDeMille, D., Doyle, J. M. & Sushkov, A. O. Probing the frontiers of particle physics with tabletop-scale experiments. Science 357, 990–994 (2017).\n\nSafronova, M. et al. Search for new physics with atoms and molecules. Rev. Mod. Phys. 90, 025008 (2018).\n\nLiu, J., Chen, X. & Ji, X. Current status of direct dark matter detection experiments. Nat. Phys. 13, 212–216 (2017).\n\nFerreira, E. G. Ultra-light dark matter. Astron. Astrophys. Rev. 29, 7 (2021).\n\nArcadi, G. et al. The waning of the WIMP? A review of models, searches, and constraints. Eur. Phys. J. C 78, 203 (2018).\n\nO’Hare, C. A. New definition of the neutrino floor for direct dark matter searches. Phys. Rev. Lett. 127, 251802 (2021).\n\nChadha-Day, F., Ellis, J. & Marsh, D. J. Axion dark matter: what is it and why now? Sci. Adv. 8, eabj3618 (2022).\n\nWilczek, F. Problem of strong P and T invariance in the presence of instantons. Phys. Rev. Lett. 40, 279 (1978).\n\nWeinberg, S. A new light boson? Phys. Rev. Lett. 40, 223 (1978).\n\nKim, J. E. & Carosi, G. Axions and the strong CP problem. Rev. Mod. Phys. 82, 557 (2010).\n\nIrastorza, I. G. & Redondo, J. New experimental approaches in the search for axion-like particles. Prog. Part. Nucl. Phys. 102, 89–159 (2018).\n\nSvrcek, P. & Witten, E. Axions in string theory. J. High Energy Phys. 2006, 051 (2006).\n\nKawasaki, M., Saikawa, K. & Sekiguchi, T. Axion dark matter from topological defects. Phys. Rev. D 91, 065014 (2015).\n\nRaffelt, G. G. Astrophysical axion bounds. Lect. Notes Phys. 741, 51–71 (2008).\n\nJiang, M., Su, H., Garcon, A., Peng, X. & Budker, D. Search for axion-like dark matter with spin-based amplifiers. Nat. Phys. 17, 1402–1407 (2021).\n\nWang, Y. et al. Limits on axions and axionlike particles within the axion window using a spin-based amplifier. Phys. Rev. Lett. 129, 051801 (2022).\n\nBudker, D., Graham, P. W., Ledbetter, M., Rajendran, S. & Sushkov, A. O. Proposal for a cosmic axion spin precession experiment (CASPEr). Phys. Rev. X 4, 021030 (2014).\n\nGarcon, A. et al. Constraints on bosonic dark matter from ultralow-field nuclear magnetic resonance. Sci. Adv. 5, eaax4539 (2019).\n\nBhusal, A., Houston, N. & Li, T. Searching for solar axions using data from the Sudbury Neutrino Observatory. Phys. Rev. Lett. 126, 091601 (2021).\n\nCarenza, P. et al. Improved axion emissivity from a supernova via nucleon-nucleon bremsstrahlung. J. Cosmol. Astropart. Phys. 2019, 016 (2019).\n\nDeRocco, W., Graham, P. W. & Rajendran, S. Exploring the robustness of stellar cooling constraints on light particles. Phys. Rev. D 102, 075015 (2020).\n\nBar, N., Blum, K. & D’amico, G. Is there a supernova bound on axions? Phys. Rev. D 101, 123025 (2020).\n\nvan den Bergh, S. How rare are supernovae? Comments Astrophys. 17, 125–130 (1993).\n\nAfach, S. et al. What can a GNOME do? Search targets for the Global Network of Optical Magnetometers for Exotic physics searches. Ann. Phys. 536, 2300083 (2024).\n\nYang, Y., Wu, T., Zhang, J. & Guo, H. Search for topological defect of axionlike model with cesium atomic comagnetometer. Chin. Phys. B 30, 050704 (2021).\n\nKhamis, S. S. et al. Multimessenger search for exotic field emission with a global magnetometer network. Phys. Rev. X 15, 031048 (2025).\n\nGavilan-Martin, D. et al. Searching for dark matter with a spin-based interferometer. Nat. Commun. 16, 4953 (2025).\n\nWang, Y. et al. Search for exotic parity-violation interactions with quantum spin amplifiers. Sci. Adv. 9, eade0353 (2023).\n\nRoberts, B. M. et al. Search for domain wall dark matter with atomic clocks on board global positioning system satellites. Nat. Commun. 8, 1195 (2017).\n\nWalker, T. G. & Happer, W. Spin-exchange optical pumping of noble-gas nuclei. Rev. Mod. Phys. 69, 629 (1997).\n\nOwen, B. J. & Sathyaprakash, B. S. Matched filtering of gravitational waves from inspiraling compact binaries: computational cost and template placement. Phys. Rev. D 60, 022002 (1999).\n\nWainstein, L. A. & Zubakov, V. Extraction of Signals from Noise (Dover, 1970).\n\nAybas, D. et al. Search for axionlike dark matter using solid-state nuclear magnetic resonance. Phys. Rev. Lett. 126, 141802 (2021).\n\nBrubaker, B., Zhong, L., Lamoreaux, S., Lehnert, K. & van Bibber, K. HAYSTAC axion search analysis procedure. Phys. Rev. D 96, 123008 (2017).\n\nKimball, D. J. Nuclear spin content and constraints on exotic spin-dependent couplings. New J. Phys. 17, 073008 (2015).\n\nCatena, R. & Ullio, P. A novel determination of the local dark matter density. J. Cosmol. Astropart. Phys. 2010, 004 (2010).\n\nBovy, J. & Tremaine, S. On the local dark matter density. Astrophys. J. 756, 89 (2012).\n\nSivertsson, S., Silverwood, H., Read, J. I., Bertone, G. & Steger, P. The local dark matter density from SDSS-SEGUE G-dwarfs. Mon. Not. R. Astron. Soc. 478, 1677–1693 (2018).\n\nHuang, X. et al. Hunting for exotic bosons with flying quantum sensors in space. Phys. Rev. D 112, 095015 (2025).\n\nKornack, T., Ghosh, R. & Romalis, M. Nuclear spin gyroscope based on an atomic comagnetometer. Phys. Rev. Lett. 95, 230801 (2005).\n\nShaham, R., Katz, O. & Firstenberg, O. Strong coupling of alkali-metal spins to noble-gas spins with an hour-long coherence time. Nat. Phys. 18, 506–510 (2022).\n\nBraaten, E. & Zhang, H. Colloquium: The physics of axion stars. Rev. Mod. Phys. 91, 041002 (2019).\n\nBuschmann, M. et al. Dark matter from axion strings with adaptive mesh refinement. Nat. Commun. 13, 1049 (2022).\n\nKimball, D. J. et al. Searching for axion stars and Q-balls with a terrestrial magnetometer network. Phys. Rev. D 97, 043002 (2018).\n\nKusenko, A. & Steinhardt, P. J. Q-ball candidates for self-interacting dark matter. Phys. Rev. Lett. 87, 141301 (2001).\n\nCenters, G. P. et al. Stochastic fluctuations of bosonic dark matter. Nat. Commun. 12, 7321 (2021).\n\nBanerjee, A., Budker, D., Eby, J., Kim, H. & Perez, G. Relaxion stars and their detection via atomic physics. Commun. Phys. 3, 1 (2020).\n\nWu, T. et al. Search for axionlike dark matter with a liquid-state nuclear spin comagnetometer. Phys. Rev. Lett. 122, 191302 (2019).\n\nAbel, C. et al. Search for axionlike dark matter through nuclear spin precession in electric and magnetic fields. Phys. Rev. X 7, 041034 (2017).\n\nDailey, C. et al. Quantum sensor networks as exotic field telescopes for multi-messenger astronomy. Nat. Astron. 5, 150–158 (2021).\n\nAbbott, B. P. et al. GW170817: observation of gravitational waves from a binary neutron star inspiral. Phys. Rev. Lett. 119, 161101 (2017).",
      "url": "https://www.nature.com/articles/s41586-025-10034-w",
      "source": "Nature",
      "published": "2026-01-29",
      "sentiment_score": 0.85,
      "reasoning": "The article reports on the use of distributed intercity quantum sensors to set new constraints on axion dark matter, representing a significant scientific advancement in the search for dark matter. This breakthrough has broad implications for fundamental physics and cosmology, potentially advancing our understanding of the universe. The article provides sufficient context about the methodology and references prior foundational work, indicating substance and significance beyond a niche audience.",
      "category": "Technology",
      "personality_title": "New quantum sensors tighten the search for mysterious axion dark matter",
      "personality_presentation": "**Context** – Scientists have long searched for dark matter, an invisible substance that makes up most of the universe’s mass. One possible form of dark matter is called the axion, a tiny particle that is very hard to detect. Researchers use advanced tools like quantum sensors to try to find signs of axions.\n\n**What happened** – A recent study used a network of quantum sensors placed in different cities to listen for tiny changes in magnetic fields caused by axions. These sensors measure the spins of special gases very precisely and look for unusual movements that axions might cause. By combining data from several locations, the scientists improved their ability to detect or rule out axions in certain ranges.\n\n**Impact** – This approach set new limits on where axions could be hiding, narrowing down the possibilities for future searches. It is important because it uses a clever way to reduce noise and improve detection by spreading sensors far apart. This method is more sensitive than previous experiments and helps focus efforts on the most promising areas, saving time and resources.\n\n**What's next step** – Scientists plan to expand these sensor networks and improve their precision. They hope this will either discover axions or rule out even more possibilities. This progress may also inspire new technologies for detecting other hard-to-find particles in the universe.\n\n**One-sentence takeaway** – Distributed quantum sensors have advanced the search for axion dark matter by setting tighter limits on where these elusive particles might be found.",
      "personality_title_fr": "De nouveaux capteurs quantiques renforcent la recherche des mystérieux axions de la matière noire",
      "personality_presentation_fr": "**Contexte** – Les scientifiques cherchent depuis longtemps la matière noire, une substance invisible qui compose la majeure partie de la masse de l’univers. Une forme possible de matière noire est l’axion, une particule très petite et difficile à détecter. Pour cela, ils utilisent des outils avancés comme les capteurs quantiques.\n\n**Ce qui s’est passé** – Une étude récente a utilisé un réseau de capteurs quantiques répartis dans plusieurs villes pour détecter de très faibles changements dans les champs magnétiques causés par les axions. Ces capteurs mesurent avec précision les spins de gaz spéciaux et recherchent des mouvements inhabituels que les axions pourraient provoquer. En combinant les données de plusieurs lieux, les chercheurs ont amélioré leur capacité à détecter ou exclure les axions dans certaines plages.\n\n**Impact** – Cette méthode a permis de mieux définir où les axions pourraient se trouver, réduisant ainsi les possibilités pour les futures recherches. C’est important car elle utilise une technique intelligente pour réduire le bruit et augmenter la sensibilité en répartissant les capteurs loin les uns des autres. Cette approche est plus précise que les expériences précédentes et aide à concentrer les efforts sur les zones les plus prometteuses.\n\n**Étape suivante** – Les scientifiques prévoient d’étendre ces réseaux de capteurs et d’améliorer leur précision. Ils espèrent ainsi découvrir des axions ou exclure encore plus de possibilités. Cette avancée pourrait aussi inspirer de nouvelles technologies pour détecter d’autres particules difficiles à trouver dans l’univers.\n\n**Résumé en une phrase** – Des capteurs quantiques distribués ont fait progresser la recherche des axions de la matière noire en limitant plus précisément les endroits où ces particules insaisissables pourraient se cacher.",
      "personality_title_es": "Nuevos sensores cuánticos mejoran la búsqueda de los misteriosos axiones de la materia oscura",
      "personality_presentation_es": "**Contexto** – Los científicos han buscado durante mucho tiempo la materia oscura, una sustancia invisible que forma la mayor parte de la masa del universo. Una posible forma de materia oscura es el axión, una partícula muy pequeña y difícil de detectar. Para esto, usan herramientas avanzadas como sensores cuánticos.\n\n**Qué pasó** – Un estudio reciente usó una red de sensores cuánticos ubicados en diferentes ciudades para detectar cambios muy pequeños en campos magnéticos causados por axiones. Estos sensores miden con mucha precisión los giros de gases especiales y buscan movimientos extraños que los axiones podrían provocar. Al combinar datos de varios lugares, los científicos mejoraron su capacidad para detectar o descartar axiones en ciertos rangos.\n\n**Impacto** – Este método estableció nuevos límites sobre dónde podrían estar los axiones, reduciendo las posibilidades para búsquedas futuras. Es importante porque usa una forma inteligente de reducir el ruido y aumentar la sensibilidad al tener sensores distribuidos lejos unos de otros. Este método es más sensible que experimentos anteriores y ayuda a enfocar esfuerzos en las áreas más prometedoras.\n\n**Próximo paso** – Los científicos planean expandir estas redes de sensores y mejorar su precisión. Esperan que esto permita descubrir axiones o descartar aún más posibilidades. Este avance también podría inspirar nuevas tecnologías para detectar otras partículas difíciles de encontrar en el universo.\n\n**Frase clave** – Sensores cuánticos distribuidos han avanzado la búsqueda del axión de materia oscura al establecer límites más estrictos sobre dónde podrían encontrarse estas partículas esquivas.",
      "image_url": "public/images/news_image_Constraints-on-axion-dark-matter-by-distributed-in.png",
      "image_prompt": "A warm, detailed painting of a network of glowing, interconnected quantum sensors shaped like stylized atomic clocks and magnetometers, spread across a gently rolling landscape under a soft twilight sky, with subtle swirling patterns representing elusive axion dark matter particles weaving through and around the sensors like delicate threads of light."
    },
    {
      "title": "Multimodal learning with next-token prediction for large multimodal models",
      "summary": "Nature, Published online: 28 January 2026; doi:10.1038/s41586-025-10041-xEmu3 enables large-scale text, image and video learning based solely on next-token prediction, matching the generation and perception performance of task-specific methods, with implications for the development of scalable and unified multimodal intelligence systems.",
      "content": "Tokenizer design\n\nA unified tokenizer discretizes texts, images and videos into compact token sequences using shared codebooks. This enables text and vision information to reside in a common discrete space, facilitating autoregressive modelling. For text tokens and control tokens, we leveraged a byte pair encoding (BPE)-based text tokenizer for tokenization, whereas a vector quantization (VQ)-based visual tokenizer was used to discretize images and videos into compact token sequences.\n\nText tokenizer\n\nFor text tokenization, we adopted Qwen’s tokenizer49, which uses byte-level byte-pair encoding with a vocabulary encompassing 151,643 regular text tokens. To reserve sufficient capacity for template control, we also incorporated 211 special tokens into the tokenizer’s vocabulary.\n\nVision tokenizer\n\nWe trained the vision tokenizer using SBER-MoVQGAN14, which can encode a 4 × 512 × 512 video clip or a 512 × 512 image into 4,096 discrete tokens from a codebook of size 32,768. Our tokenizer achieved 4× compression in the temporal dimension and 8 × 8 compression in the spatial dimension and is applicable to any temporal and spatial resolution. Building on the MoVQGAN architecture50, we incorporated two temporal residual layers with three-dimensional convolution kernels into both the encoder and decoder modules to perform temporal downsampling and enhance video tokenization capabilities. The tokenizer was trained end-to-end on the LAION high-resolution image dataset and the InternVid51 video dataset using combined objective functions of Euclidean norm (L2) loss, learned perceptual image patch similarity (LPIPS) perceptual loss52, generative adversarial network (GAN) loss and commitment loss. Further details on video compression metrics, the impact of codebook size, and comparisons between the unified and standalone image tokenizers are provided in section 1 of the Supplementary Information.\n\nArchitecture design\n\nEmu3 uses a decoder-only Transformer with modality-shared embeddings. We used RMSNorm53 for normalization and GQA54 for attention mechanisms, as well as the SwiGLU55 activation function and rotary positional embeddings56. Biases in the qkv and linear projection layers were removed. In addition, a dropout rate of 0.1 was implemented to improve training stability. Overall, the model contains 8.49 billion parameters, including 32 layers with a hidden size of 4,096, intermediate size of 14,336 and 32 attention heads (8 key-value heads). The shared multimodal vocabulary comprises 184,622 tokens, enabling consistent representation across language and vision domains.\n\nArchitectural comparisons with diffusion models\n\nTo fairly compare the next-token prediction paradigm with diffusion models for visual generation tasks, we used Flan-T5-XL57 as the text encoder and trained both a 1.5B diffusion transformer58,59 and a 1.5B decoder-only transformer60 on the OpenImages61 dataset. The diffusion model leverages the variational autoencoder from SDXL20, whereas the decoder-only transformer uses the video tokenizer in Emu3 to encode images into latent tokens. Both models were trained with identical configurations, including a linear warm-up of 2,235 steps, a constant learning rate of 1 × 10−4 and a global batch size of 1,024. As shown in Fig. 3c, the next-token prediction model consistently converged faster than its diffusion counterpart for equal training samples, challenging the prevailing belief that diffusion architectures are inherently superior for visual generation.\n\nArchitectural comparisons with encoder + LLM compositional paradigm\n\nTo fairly evaluate different vision–language architectures, we compared three model variants (trained without any pretrained LLM initialization) on the I2T validation set (an image-understanding task), as shown in Fig. 3b. All models were trained on the EVE-33M multimodal corpus35, using a global batch size of 1,024, a base learning rate of 1 × 10−4 with cosine decay scheduling and 12,000 training steps, and evaluated on a held-out validation set of 1,024 samples with comparable parameters. The models compared were: (1) a decoder-only model that consumes discrete image tokens as input (Emu3 variant, 1.22B parameters); (2) a late-fusion architecture comprising a vision encoder and decoder (LLaVA-style variant, 1.22B = 1.05B decoder + 0.17B vision encoder); and (3) a late-fusion architecture initialized with a CLIP-based vision encoder (LLaVA-style variant, 1.35B = 1.05B + 0.30B). The late-fusion LLaVA-style model initialized with a pretrained CLIP vision encoder showed substantially lower validation loss. Notably, when that pretraining advantage was removed, the apparent superiority of the encoder-based compositional architecture was largely diminished. The decoder-only next-token prediction model showed comparable performance, challenging the prevailing belief that encoder + LLM architectures are inherently superior for multimodal understanding. When evaluated under equal scratch training conditions, without prior initialization from LLMs and CLIP, it matched compositional encoder + LLM paradigms in terms of learning efficiency. Further architectural analyses are provided in section 2.1 of the Supplementary Information.\n\nData collection\n\nEmu3 was pretrained from scratch on a mix of language, image and video data. Details of data construction, including sources, filtering and preprocessing, are provided in Extended Data Table 7. Further information on dataset composition, collection pipelines and filtering details is provided in section 3.1 of the Supplementary Information.\n\nPretraining details\n\nData format\n\nImages and videos were resized to areas near 512 × 512 while preserving the aspect ratio during pretraining. We inserted special tokens [SOV], [SOT] and [EOV] to delimit multimodal segments:\n\n$$[\\text{BOS}]\\{\\text{caption text}\\}[\\text{SOV}]\\{\\text{meta text}\\}[\\text{SOT}]\\{\\text{vision tokens}\\}[\\text{EOV}][\\text{EOS}],$$\n\nwhere [BOS] and [EOS] mark the start and end of the whole sample, [SOV] marks the start of the vision input, [SOT] marks the start of vision tokens, and [EOV] indicates the end of the vision input. In addition, [EOL] and [EOF] were inserted into the vision tokens to denote line breaks and frame breaks, respectively. The ‘meta text’ contains information about the resolution for images; for videos, it includes resolution, frame rate and duration, all presented in plain text format. We also moved the ‘caption text’ field in a portion of the dataset to follow the [EOV] token, thereby constructing data aimed at vision understanding tasks.\n\nTraining recipe\n\nPretraining followed a three-stage curriculum designed to balance training efficiency and optimization stability. Stage 1 used a learning rate of 1 × 10−4 with cosine decay, no dropout and a sequence length of 5,120. This configuration enabled rapid early convergence; however, the absence of dropout eventually led to optimization instability and model collapse in late training. Stage 2 therefore introduced a dropout rate of 0.1, which stabilized optimization while retaining the warm-start benefits established in stage 1. Stage 3 extended the context length to 65,536 tokens to accommodate video–text data. The sampling ratio gradually shifted from image–text pairs towards video–text pairs. This curriculum substantially improved overall efficiency: the first two stages focused on image data for stable and cost-effective initialization, whereas the third stage expanded the context window and incorporated video data for full multimodal training. Tensor and pipeline parallelism remained constant across stages, with context parallelism scaling from 1 to 4 only in stage 3 to support the extended sequence length. Further implementation details including multimodal dropout for stability, token-level loss weighting, LLM-based initialization and mixture-of-experts configuration are provided in section 3.2.3 of the Supplementary Information.\n\nPost-training details\n\nT2I generation\n\nQFT. After pretraining, Emu3 underwent post-training to enhance visual generation quality. We applied QFT to high-quality image data while continuing next-token prediction with supervision restricted to vision tokens. Training data were filtered by the average of three preference scores: HPSv2.1 (ref. 62), MPS63 and the LAION-Aesthetics score64, and the image resolution was increased from 512 to 720 pixels. We set the batch size to 240 with a context length of 9,216, with the learning rate cosine decaying from 1 × 10−5 to 1 × 10−6 over 15,000 training steps. Subsequently, a linear annealing strategy was used to gradually decay the learning rate to zero over the final 5,000 steps of QFT training.\n\nDPO. We further aligned generation quality with human preference using DPO13. For each prompt, the model generated 8–10 candidate images that were evaluated by three annotators on visual appeal and alignment. The highest and lowest scoring samples formed preference triplets \\(({p}_{i},{x}_{i}^{{\\rm{chosen}}},{x}_{i}^{{\\rm{rejected}}})\\) for optimization. Tokenized data from this process were reused directly during training to avoid retokenization inconsistencies. Emu3-DPO jointly minimizes the DPO loss and the next-token prediction loss, with a weighting factor of 0.2 applied to the supervised fine-tuning loss for stable optimization. During DPO training, we use a dataset of 5,120 prompts and train for one epoch with a global batch size of 128. The learning rate follows a cosine decay schedule with a brief 5-step warm-up and then decays to a constant value of 7 × 10−7. A KL penalty of 0.5 is applied to the reference policy to balance alignment strength and generation diversity.\n\nWe present the performance of Emu3 through automated metric evaluation on popular T2I benchmarks: MSCOCO-30K23, GenEval24, T2I-CompBench25, and DPG-Bench26. Evaluation details are provided in the Supplementary Information, section 4.1.2.\n\nT2V generation\n\nEmu3 was extended to T2V generation by applying QFT to high-quality video data (each sample was 5 s long, 24 fps), with strict resolution and motion filters to ensure visual fidelity. We set the batch size to 720 with a context length of 131,072, with the learning rate set to 5 × 10−5 over 5,000 training steps. We evaluated video generation using VBench27, which assesses 16 dimensions including temporal consistency, appearance quality, semantic fidelity and subject–background coherence. Evaluation details are provided in section 4.2.2 of the Supplementary Information.\n\nVision–language understanding\n\nEmu3 was further adapted to vision–language understanding through a two-stage post-training procedure. In the first stage, the model was trained on 10 million image–text pairs using a batch size of 512, mixing image-understanding data with pure language data while masking losses on vision tokens for text-only prediction. All images were resized to approximately 512 × 512 while preserving the aspect ratio. In the second stage, we performed instruction tuning on 3.5 million question–answer pairs sampled from ref. 65, also using a batch size of 512; images with shorter or longer resolution were clipped to the 512–1,024 pixel range. For both stages, we used a cosine learning rate schedule with a peak learning rate of 1 × 10−5. Evaluation details are provided in section 4.3 of the Supplementary Information.\n\nInterleaved image–text generation\n\nWe further extended Emu3 to interleaved image–text generation, in which structured textual steps are accompanied by corresponding illustrative images within a single output sequence. The model was fine-tuned end-to-end to autoregressively generate such multimodal sequences, leveraging the flexibility of the unified framework. Training was performed for 10,000 steps with a global batch size of 128 and a maximum sequence length of 33,792 tokens. Each sequence included up to 8 images, each resized to a maximum area of 5122 pixels while preserving the aspect ratio. We used the Adam optimizer with a cosine learning rate schedule and a base learning rate of 7 × 10−6 and applied a dropout rate of 0.1 with equal weighting between image and text losses. Further details on data formatting and visualization results are provided in section 4.4 of the Supplementary Information.\n\nVision–language–action models\n\nWe further extended Emu3 to vision–language–action tasks by fine-tuning it on the CALVIN66 benchmark, a simulated environment designed for long-horizon, language-conditioned robotic manipulation.\n\nThe model was initialized from Emu3 pretrained weights, whereas the action encoder used the FAST tokenizer31 with a 1,024-size vocabulary, replacing the last 1,024 token IDs of the language tokenizer. RGB observations from third-person (200 × 200) and wrist (80 × 80) views were discretized using the Emu3 vision tokenizer with a spatial compression factor of 8. Training used a time window of 20 and an action chunk size of 10, forming input sequences of two consecutive vision–action–vision–action frames. Loss weights were set to 0.5 for visual tokens and 1.0 for action tokens. The model was trained for 8,000 steps with a batch size of 192 and a cosine learning rate schedule starting at 8 × 10−5. During inference, it predicted actions online by means of a sliding two-frame window. Visualizations are shown in Extended Data Fig. 3. Although the CALVIN benchmark is simulation-based, Emu3’s vision–language–action formulation was designed with real-world deployment challenges in mind. The next-token prediction paradigm naturally conditions on arbitrary-length histories, allowing the model to integrate feedback over time and recover from partial or imperfect sensor inputs, thereby accommodating noisy sensors or delayed feedback. In practice, real-world robotic validation requires substantial data collection (for instance, time-consuming tele-operation or on-hardware rollouts) and system-level engineering efforts to ensure safety, latency guarantees and reliable actuation, which made large-scale evaluation on physical robots difficult within the scope of this work. Although large-scale physical-robot validation will be part of our future work, the simulation results show that Emu3 can model complex, interleaved perception–action sequences without task-specific components, indicating strong potential for transfer to real robotic systems.",
      "url": "https://www.nature.com/articles/s41586-025-10041-x",
      "source": "Nature",
      "published": "2026-01-29",
      "sentiment_score": 0.85,
      "reasoning": "The article reports a significant breakthrough in large-scale multimodal AI models that unify text, image, and video understanding and generation through next-token prediction. This advancement challenges existing paradigms, improves training efficiency, and has broad implications for AI applications including vision-language understanding and robotic manipulation. The work is detailed and focused, describing a scalable, unified approach with potential for real-world impact across multiple domains.",
      "category": "Technology",
      "personality_title": "New AI model learns text, images, and videos together with improved efficiency",
      "personality_presentation": "**Context** – Artificial intelligence (AI) models often focus on one type of data at a time, like text or images. Combining different types—text, images, and videos—into one model is hard but useful for many tasks, such as understanding pictures or controlling robots.\n\n**What happened** – Researchers created a new AI model called Emu3 that learns from text, images, and videos all at once by predicting the next piece of information, called next-token prediction. The model uses a special system to turn words, pictures, and video frames into a shared language of tokens. It was trained on millions of examples and tested on tasks like creating images from text, understanding pictures with captions, and even controlling robots in simulations.\n\n**Impact** – Emu3 challenges the usual belief that separate models or complex methods are better for different types of data. Instead, it shows that one big model can learn all these tasks efficiently and with similar or better results. This means AI can become simpler and faster to train while handling multiple types of information. It also opens the door to better AI that can understand and create content across text, images, and videos, and even guide robots using language and vision.\n\n**What's next step** – The team plans to test Emu3 on real robots to see how well it can help with physical tasks. They also want to improve the model further and explore more ways to combine different types of data for even smarter AI systems.\n\n**One-sentence takeaway** – Emu3 is a new AI model that learns text, images, and videos together efficiently, showing a simpler way to build powerful systems for understanding and creating multimedia content.\n",
      "personality_title_fr": "Nouveau modèle d’IA apprend texte, images et vidéos ensemble avec plus d’efficacité",
      "personality_presentation_fr": "**Contexte** – Les modèles d’intelligence artificielle (IA) se concentrent souvent sur un seul type de données, comme le texte ou les images. Combiner différents types—texte, images et vidéos—dans un seul modèle est difficile mais utile pour de nombreuses tâches, comme comprendre des images ou contrôler des robots.\n\n**Ce qui s’est passé** – Des chercheurs ont créé un nouveau modèle d’IA appelé Emu3 qui apprend à partir de texte, d’images et de vidéos en même temps en prédisant la prochaine information, appelée prédiction du prochain jeton. Le modèle utilise un système spécial pour transformer mots, images et vidéos en un langage commun de jetons. Il a été entraîné sur des millions d’exemples et testé sur des tâches comme créer des images à partir de texte, comprendre des images avec des légendes, et même contrôler des robots en simulation.\n\n**Impact** – Emu3 remet en question la croyance habituelle que des modèles séparés ou des méthodes complexes sont meilleurs pour différents types de données. Il montre qu’un grand modèle peut apprendre toutes ces tâches efficacement et avec des résultats similaires ou meilleurs. Cela signifie que l’IA peut devenir plus simple et plus rapide à entraîner tout en traitant plusieurs types d’informations. Cela ouvre aussi la voie à une IA capable de comprendre et de créer du contenu en texte, images et vidéos, et même de guider des robots avec la langue et la vision.\n\n**Prochaine étape** – L’équipe prévoit de tester Emu3 sur de vrais robots pour voir comment il peut aider dans des tâches physiques. Ils souhaitent aussi améliorer le modèle et explorer plus de façons de combiner différents types de données pour des systèmes d’IA encore plus intelligents.\n\n**Résumé en une phrase** – Emu3 est un nouveau modèle d’IA qui apprend texte, images et vidéos ensemble efficacement, montrant une façon plus simple de construire des systèmes puissants pour comprendre et créer du contenu multimédia.\n",
      "personality_title_es": "Nuevo modelo de IA aprende texto, imágenes y videos juntos con mayor eficiencia",
      "personality_presentation_es": "**Contexto** – Los modelos de inteligencia artificial (IA) suelen centrarse en un tipo de dato, como texto o imágenes. Combinar diferentes tipos—texto, imágenes y videos—en un solo modelo es difícil pero útil para muchas tareas, como entender imágenes o controlar robots.\n\n**Qué pasó** – Investigadores crearon un nuevo modelo de IA llamado Emu3 que aprende de texto, imágenes y videos al mismo tiempo, prediciendo la siguiente pieza de información, llamada predicción del siguiente token. El modelo usa un sistema especial para convertir palabras, imágenes y videos en un lenguaje común de tokens. Fue entrenado con millones de ejemplos y probado en tareas como crear imágenes desde texto, entender imágenes con leyendas, e incluso controlar robots en simulaciones.\n\n**Impacto** – Emu3 desafía la creencia común de que modelos separados o métodos complejos son mejores para diferentes tipos de datos. En cambio, muestra que un solo modelo grande puede aprender todas estas tareas de forma eficiente y con resultados similares o mejores. Esto significa que la IA puede ser más simple y rápida de entrenar, manejando varios tipos de información. También abre la puerta a una IA mejor que puede entender y crear contenido en texto, imágenes y videos, y guiar robots usando lenguaje y visión.\n\n**Próximo paso** – El equipo planea probar Emu3 en robots reales para ver qué tan bien ayuda en tareas físicas. También quieren mejorar el modelo y explorar más formas de combinar datos para sistemas de IA aún más inteligentes.\n\n**Resumen en una frase** – Emu3 es un nuevo modelo de IA que aprende texto, imágenes y videos juntos de manera eficiente, mostrando una forma más sencilla de construir sistemas potentes para entender y crear contenido multimedia.\n",
      "image_url": "public/images/news_image_Multimodal-learning-with-next-token-prediction-for.png",
      "image_prompt": "A detailed, warm painting of an intricate, glowing neural network interwoven with delicate, translucent film strips and flowing text ribbons, symbolizing the harmonious fusion of language, images, and video into a unified, dynamic stream of tokens, rendered in soft natural hues and abstract shapes without human figures."
    },
    {
      "title": "Accurate determination of the 3D atomic structure of amorphous materials",
      "summary": "Nature, Published online: 28 January 2026; doi:10.1038/s41586-025-09857-4A quantitative framework for atomic electron tomography enables reliable determination of three-dimensional (3D) atomic coordinates and elemental identities in amorphous materials.",
      "content": "Nomura, K. et al. Room-temperature fabrication of transparent flexible thin-film transistors using amorphous oxide semiconductors. Nature 432, 488–492 (2004).\n\nCarlson, D. E. & Wronski, C. R. Amorphous silicon solar cell. Appl. Phys. Lett. 28, 671–673 (1976).\n\nWuttig, M., Bhaskaran, H. & Taubner, T. Phase-change materials for non-volatile photonic applications. Nat. Photon. 11, 465–476 (2017).\n\nWang, W. H., Dong, C. & Shek, C. H. Bulk metallic glasses. Mater. Sci. Eng. R Rep. 44, 45–89 (2004).\n\nLi, H. F. & Zheng, Y. F. Recent advances in bulk metallic glasses for biomedical applications. Acta Biomater. 36, 1–20 (2016).\n\nEsmaeil Zadeh, I. et al. Superconducting nanowire single-photon detectors: a perspective on evolution, state-of-the-art, future developments, and applications. Appl. Phys. Lett. 118, 190502 (2021).\n\nLi, B. et al. Down-converted photon pairs in a high-Q silicon nitride microresonator. Nature 639, 922–927 (2025).\n\nde Leon, N. P. et al. Materials challenges and opportunities for quantum computing hardware. Science 372, eabb2823 (2021).\n\nZachariasen, W. H. The atomic arrangement in glass. J. Am. Chem. Soc. 54, 3841–3851 (1932).\n\nWarren, B. E. & Biscob, J. Fourier analysis of X-ray patterns of soda-silica glass. J. Am. Ceram. Soc. 21, 259–265 (1938).\n\nFrank, F. C. Supercooling of liquids. Proc. R. Soc. Lond. A 215, 43–46 (1952).\n\nBernal, J. D. Geometry of the structure of monatomic liquids. Nature 185, 68–70 (1960).\n\nFinney, J. L. Random packings and structure of simple liquids. I. The geometry of random close packing. Proc. R. Soc. Lond. A 319, 479–493 (1970).\n\nMcGreevy, R. L. & Pusztai, L. Reverse Monte Carlo simulation: a new technique for the determination of disordered structures. Mol. Simul. 1, 359–367 (1988).\n\nElliott, S. R. Medium-range structural order in covalent amorphous solids. Nature 354, 445–452 (1991).\n\nKresse, G. & Hafner, J. Ab initio molecular-dynamics simulation of the liquid-metal–amorphous-semiconductor transition in germanium. Phys. Rev. B 49, 14251–14269 (1994).\n\nKelton, K. F. et al. First X-ray scattering studies on electrostatically levitated metallic liquids: demonstrated influence of local icosahedral order on the nucleation barrier. Phys. Rev. Lett. 90, 195504 (2003).\n\nMiracle, D. B. A structural model for metallic glasses. Nat. Mater. 3, 697–702 (2004).\n\nSheng, H. W., Luo, W. K., Alamgir, F. M., Bai, J. M. & Ma, E. Atomic packing and short-to-medium-range order in metallic glasses. Nature 439, 419–425 (2006).\n\nCheng, Y. Q. & Ma, E. Atomic-level structure and structure-property relationship in metallic glasses. Prog. Mater Sci. 56, 379–473 (2011).\n\nHwang, J. et al. Nanoscale structure and structural relaxation in Zr 50 Cu 45 A 15 bulk metallic glass. Phys. Rev. Lett. 108, 195505 (2012).\n\nTreacy, M. M. J. & Borisenko, K. B. The local structure of amorphous silicon. Science 335, 950–953 (2012).\n\nHirata, A. et al. Geometric frustration of icosahedron in metallic glasses. Science 341, 376–379 (2013).\n\nLan, S. et al. A medium-range structure motif linking amorphous and crystalline states. Nat. Mater. 20, 1347–1352 (2021).\n\nYang, Y. Determining the three-dimensional atomic structure of an amorphous solid. Nature 592, 60–64 (2021).\n\nYuan, Y. Three-dimensional atomic packing in amorphous solids with liquid-like structure. Nat. Mater. 21, 95–102 (2022).\n\nMiao, J. Computational microscopy with coherent diffractive imaging and ptychography. Nature 637, 281–295 (2025).\n\nBusch, R., Rez, P., Treacy, M. M. J. & Zuo, J.-M. Limit of atomic resolution tomography reconstruction of amorphous nanoparticles. Nature https://doi.org/10.1038/s41586-025-09924-w (2026).\n\nMiao, J., Ercius, P. & Billinge, S. J. L. Atomic electron tomography: 3D structures without crystals. Science 353, aaf2157 (2016).\n\nScott, M. C. et al. Electron tomography at 2.4-ångström resolution. Nature 483, 444–447 (2012).\n\nChen, C.-C. et al. Three-dimensional imaging of dislocations in a nanoparticle at atomic resolution. Nature 496, 74–77 (2013).\n\nGoris, B. et al. Measuring lattice strain in three dimensions through electron microscopy. Nano Lett. 15, 6996–7001 (2015).\n\nYang, Y. Deciphering chemical order/disorder and material properties at the single-atom level. Nature 542, 75–79 (2017).\n\nZhou, J. Observing crystal nucleation in four dimensions using atomic electron tomography. Nature 570, 500–503 (2019).\n\nXu, R. et al. Three-dimensional coordinates of individual atoms in materials revealed by electron tomography. Nat. Mater. 14, 1099–1103 (2015).\n\nTian, X. Correlating the three-dimensional atomic defects and electronic properties of two-dimensional transition metal dichalcogenides. Nat. Mater. 19, 867–873 (2020).\n\nTian, X. Capturing 3D atomic defects and phonon localization at the 2D heterostructure interface. Sci. Adv. 7, eabi6699 (2021).\n\nMoniri, S. Three-dimensional atomic structure and local chemical order of medium and high-entropy nanoalloys. Nature 624, 564–569 (2023).\n\nJo, H. et al. Direct strain correlations at the single-atom level in three-dimensional core–shell interface structures. Nat. Commun. 13, 5957 (2022).\n\nLi, Z. et al. Probing the atomically diffuse interfaces in Pd@Pt core–shell nanoparticles in three dimensions. Nat. Commun. 14, 2934 (2023).\n\nYang, Y. et al. Atomic-scale identification of the active sites of oxygen reduction nanocatalysts. Nat. Catal. 7, 796–806 (2024).\n\nMadsen, J. & Susi, T. The abTEM code: transmission electron microscopy from first principles. Open Res. Eur. 1, 24 (2021).\n\nDabov, K., Foi, A., Katkovnik, V. & Egiazarian, K. Image Denoising by Sparse 3-D Transform-Domain Collaborative Filtering. IEEE Trans. Image Process. 16, 2080–2095 (2007).\n\nPham, M., Yuan, Y., Rana, A., Osher, S. & Miao, J. Accurate real space iterative reconstruction (RESIRE) algorithm for tomography. Sci Rep. 13, 5624 (2023).\n\nWeyland, M. & Midgley, P. A. Electron tomography. Mater. Today 7, 32–40 (2004).\n\nMiao, J., Charalambous, P., Kirz, J. & Sayre, D. Extending the methodology of X-ray crystallography to allow imaging of micrometre-sized non-crystalline specimens. Nature 400, 342–344 (1999).\n\nMiao, J., Förster, F. & Levi, O. Equally sloped tomography with oversampling reconstruction. Phys. Rev. B 72, 052103 (2005).\n\nPryor, A. Jr et al. GENFIRE: a generalized Fourier iterative reconstruction algorithm for high-resolution 3D imaging. Sci Rep. 7, 10409 (2017).\n\nGilbert, P. Iterative methods for the three-dimensional reconstruction of an object from projections. J. Theor. Biol. 36, 105–117 (1972).\n\nChang, D. J. et al. Ptychographic atomic electron tomography: Towards three-dimensional imaging of individual light atoms in materials. Phys. Rev. B 102, 174101 (2020).\n\nPelz, P. M. et al. Solving complex nanostructures with ptychographic atomic electron tomography. Nat. Commun. 14, 7906 (2023).\n\nMaiden, A. M., Humphry, M. J. & Rodenburg, J. M. Ptychographic transmission microscopy in three dimensions using a multi-slice approach. J. Opt. Soc. Am. A 29, 1606–1614 (2012).\n\nChen, Z. Electron ptychography achieves atomic-resolution limits set by lattice vibrations. Science 372, 826–831 (2021).\n\nO’Leary, C. M. et al. Three-dimensional structure of buried heterointerfaces revealed by multislice ptychography. Phys. Rev. Appl. 22, 014016 (2024).\n\nRaines, K. S. et al. Three-dimensional structure determination from a single view. Nature 463, 214–217 (2010).\n\nRogers, S. S., Waigh, T. A., Zhao, X. & Lu, J. R. Precise particle tracking against a complicated background: polynomial fitting with Gaussian weight. Phys. Biol. 4, 220 (2007).\n\nLloyd, S. Least squares quantization in PCM. IEEE Trans. Inf. Theory 28, 129–137 (1982).\n\nCowley, J. M. & Moodie, A. F. The scattering of electrons by atoms and crystals. I. A new theoretical approach. Acta Crystallogr. 10, 609–619 (1957).\n\nGoodman, P. & Moodie, A. F. Numerical evaluations of N-beam wave functions in electron scattering by the multi-slice method. Acta Crystallogr. A 30, 280–290 (1974).\n\nMiao, J., Sayre, D. & Chapman, H. N. Phase retrieval from the magnitude of the Fourier transform of nonperiodic objects. J. Opt. Soc. Am. A 15, 1662–1669 (1998).\n\nWakonig, K. et al. PtychoShelves, a versatile high-level framework for high-performance analysis of ptychographic data. J. Appl. Crystallogr. 53, 574–586 (2020).\n\nTsai, E. H. R., Usov, I., Diaz, A., Menzel, A. & Guizar-Sicairos, M. X-ray ptychography with extended depth of field. Opt. Express 24, 29089–29108 (2016).",
      "url": "https://www.nature.com/articles/s41586-025-09857-4",
      "source": "Nature",
      "published": "2026-01-29",
      "sentiment_score": 0.85,
      "reasoning": "The article reports a significant scientific breakthrough in accurately determining the 3D atomic structure of amorphous materials using atomic electron tomography. This advancement enables reliable identification of atomic coordinates and elemental identities in materials that lack crystalline order, which has broad implications for material science, electronics, and biomedical applications. The detailed references and context demonstrate the substance and significance of this technological innovation, which can lead to improved material design and novel applications impacting many sectors.",
      "category": "Technology",
      "personality_title": "New method reveals 3D atomic structure of non-crystalline materials",
      "personality_presentation": "**Context** – Scientists have long struggled to understand the exact arrangement of atoms in amorphous materials. Unlike crystals, these materials do not have a regular atomic pattern, making it hard to see where each atom sits and what type it is.\n\n**What happened** – Researchers developed a new technique called atomic electron tomography (AET) that can accurately map the three-dimensional positions and types of atoms in amorphous materials. This method uses advanced electron microscopes combined with special algorithms to create detailed 3D images at the atomic level.\n\n**Impact** – This breakthrough is important because it allows scientists to study materials that were previously too complex to analyze in detail. Knowing the exact atomic structure helps improve materials used in electronics, solar cells, and medical devices. For example, better understanding could lead to stronger flexible screens or more efficient solar panels.\n\n**What's next step** – The research team plans to apply this method to a wider range of materials and explore how atomic arrangements affect their properties. This could guide the design of new materials with specific features, such as better durability or conductivity.\n\n**One-sentence takeaway** – A new imaging technique now lets scientists see the 3D atomic makeup of amorphous materials, opening the door to improved material design and new technologies.",
      "personality_title_fr": "Nouvelle méthode révèle la structure atomique 3D des matériaux non cristallins",
      "personality_presentation_fr": "**Contexte** – Les scientifiques ont longtemps eu du mal à comprendre la disposition exacte des atomes dans les matériaux amorphes. Contrairement aux cristaux, ces matériaux n’ont pas de motif atomique régulier, ce qui rend difficile de savoir où se trouve chaque atome et de quel type il est.\n\n**Ce qui s’est passé** – Des chercheurs ont développé une nouvelle technique appelée tomographie électronique atomique (TEA) qui peut cartographier avec précision les positions tridimensionnelles et les types d’atomes dans les matériaux amorphes. Cette méthode utilise des microscopes électroniques avancés combinés à des algorithmes spéciaux pour créer des images 3D détaillées au niveau atomique.\n\n**Impact** – Cette avancée est importante car elle permet aux scientifiques d’étudier des matériaux trop complexes à analyser auparavant. Connaître la structure atomique exacte aide à améliorer les matériaux utilisés en électronique, cellules solaires et dispositifs médicaux. Par exemple, une meilleure compréhension pourrait conduire à des écrans flexibles plus solides ou à des panneaux solaires plus efficaces.\n\n**Prochaine étape** – L’équipe de recherche prévoit d’appliquer cette méthode à un plus grand nombre de matériaux et d’explorer comment les arrangements atomiques influencent leurs propriétés. Cela pourrait guider la conception de nouveaux matériaux avec des caractéristiques spécifiques, comme une meilleure durabilité ou conductivité.\n\n**Résumé en une phrase** – Une nouvelle technique d’imagerie permet désormais de voir la composition atomique 3D des matériaux amorphes, ouvrant la voie à une meilleure conception des matériaux et de nouvelles technologies.",
      "personality_title_es": "Nuevo método revela la estructura atómica 3D de materiales no cristalinos",
      "personality_presentation_es": "**Contexto** – Los científicos han tenido dificultades para entender la disposición exacta de los átomos en materiales amorfos. A diferencia de los cristales, estos materiales no tienen un patrón atómico regular, lo que hace difícil saber dónde está cada átomo y qué tipo es.\n\n**Qué pasó** – Investigadores desarrollaron una nueva técnica llamada tomografía electrónica atómica (TEA) que puede mapear con precisión las posiciones tridimensionales y los tipos de átomos en materiales amorfos. Este método usa microscopios electrónicos avanzados combinados con algoritmos especiales para crear imágenes 3D detalladas a nivel atómico.\n\n**Impacto** – Este avance es importante porque permite a los científicos estudiar materiales que antes eran demasiado complejos para analizar en detalle. Conocer la estructura atómica exacta ayuda a mejorar materiales usados en electrónica, paneles solares y dispositivos médicos. Por ejemplo, un mejor conocimiento podría llevar a pantallas flexibles más resistentes o paneles solares más eficientes.\n\n**Próximo paso** – El equipo de investigación planea aplicar este método a una mayor variedad de materiales y explorar cómo las disposiciones atómicas afectan sus propiedades. Esto podría guiar el diseño de nuevos materiales con características específicas, como mejor durabilidad o conductividad.\n\n**Resumen en una frase** – Una nueva técnica de imagen permite ahora ver la composición atómica 3D de materiales amorfos, abriendo la puerta a un mejor diseño de materiales y nuevas tecnologías.",
      "image_url": "public/images/news_image_Accurate-determination-of-the-3D-atomic-structure-.png",
      "image_prompt": "A warm, detailed painting of a translucent, three-dimensional lattice of interconnected glowing spheres and irregular shapes symbolizing atoms in an amorphous material, gently illuminated to reveal their complex, non-crystalline arrangement, floating serenely against a soft, natural-toned background that evokes scientific discovery and clarity."
    }
  ]
}