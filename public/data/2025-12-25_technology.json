{
  "personality": null,
  "timestamp": "2025-12-25T04:48:58.647344",
  "category": "Technology",
  "news_summary": "Advancements in AI and physics are driving breakthroughs with innovative 3D chip technology enhancing AI capabilities and novel research exploring consciousness and dark photons, boosting scientific output despite quality challenges.",
  "news_summary_fr": "Les avancées dans les domaines de l'IA et de la physique sont à l'origine de percées, avec une technologie innovante de puce 3D qui renforce les capacités d'IA et une recherche novatrice qui explore la conscience et les photons sombres, stimulant ainsi la production scientifique malgré les défis en matière de qualité.",
  "news_summary_es": "Los avances en inteligencia artificial y física están impulsando grandes progresos, con una innovadora tecnología de chip 3D que mejora las capacidades de inteligencia artificial y una novedosa investigación que explora la conciencia y los fotones oscuros, impulsando la producción científica a pesar de los retos de calidad.",
  "articles": [
    {
      "title": "This new 3D chip could break AI’s biggest bottleneck",
      "summary": "Researchers have created a new kind of 3D computer chip that stacks memory and computing elements vertically, dramatically speeding up how data moves inside the chip. Unlike traditional flat designs, this approach avoids the traffic jams that limit today’s AI hardware. The prototype already beats comparable chips by several times, with future versions expected to go much further. Just as important, it was manufactured entirely in a U.S. foundry, showing the technology is ready for real-world production.",
      "content": "Engineers from Stanford University, Carnegie Mellon University, University of Pennsylvania, and the Massachusetts Institute of Technology worked with SkyWater Technology, the largest exclusively U.S. based pure play semiconductor foundry, to create a new multilayer computer chip. The team says its architecture could mark a major shift in AI hardware and strengthen domestic semiconductor innovation.\n\nUnlike most of today's chips, which are mostly flat and 2D, this prototype is built to rise upward. Ultra thin parts are stacked like floors in a tall building, and vertical wiring works like many fast elevators that move huge amounts of data quickly. With a record setting number of vertical connections and a tightly woven layout that places memory and computing units close together, the design avoids slowdowns that have limited progress in flat chips. In hardware tests and simulations, the 3D chip beats 2D chips by roughly an order of magnitude.\n\nResearchers have made experimental 3D chips in academic labs before, but the team says this is the first time one has delivered clear performance improvements and been produced in a commercial foundry. \"This opens the door to a new era of chip production and innovation,\" said Subhasish Mitra, the William E. Ayer Professor in Electrical Engineering and professor of computer science at Stanford University, and principal investigator of a new paper describing the chip presented at the 71st Annual IEEE International Electron Devices Meeting (IEDM). \"Breakthroughs like this are how we get to the 1,000-fold hardware performance improvements future AI systems will demand.\"\n\nWhy Flat Chips Struggle With Modern AI\n\nLarge AI models such as ChatGPT and Claude constantly shuttle enormous volumes of data between memory, which holds information, and the computing units that process it.\n\nOn conventional 2D chips, everything sits on one surface and memory is limited and spread out, so data is forced through a small number of long, crowded paths. The computing parts can run far faster than data can be delivered, and the chip cannot keep enough memory nearby. The result is frequent waiting. Engineers call this problem the \"memory wall,\" where processing speed outruns the chip's ability to feed it data.\n\nFor years, chipmakers pushed back against the memory wall by shrinking transistors, the tiny switches that handle computations and store data, and packing more of them onto each chip. But researchers say that approach is nearing hard physical limits, known as the \"miniaturization wall.\"\n\nThe new design aims to get past both limits by building upward. \"By integrating memory and computation vertically, we can move a lot more information much quicker, just as the elevator banks in a high-rise let many residents travel between floors at once,\" said Tathagata Srimani, assistant professor of electrical and computer engineering at Carnegie Mellon University, the paper's senior author, who began the work as a postdoctoral fellow advised by Mitra.\n\n\"The memory wall and the miniaturization wall form a deadly combination,\" said Robert M. Radway, assistant professor of electrical and systems engineering at the University of Pennsylvania and a co-author of the study. \"We attacked it head-on by tightly integrating memory and logic and then building upward at extremely high density. It's like the Manhattan of computing -- we can fit more people in less space.\"\n\nHow the Monolithic 3D Chip Is Manufactured\n\nMany earlier 3D chip efforts have taken a simpler route by stacking separate chips. That can help, but the links between layers are often relatively rough, limited in number, and can become bottlenecks.\n\nThis team used a different approach. Instead of making separate chips and bonding them together, they build each new layer directly on top of the previous one in a single continuous flow. This method, known as \"monolithic\" 3D integration, uses temperatures low enough to avoid harming the circuitry already built below. That makes it possible to pack layers more tightly and create far more dense connections between them.\n\nA key point, the researchers say, is that the entire process was carried out in a domestic commercial silicon foundry. \"Turning a cutting-edge academic concept into something a commercial fab can build is an enormous challenge,\" said co-author Mark Nelson, vice president of technology development operations at SkyWater Technology. \"This shows that these advanced architectures aren't just possible in the lab -- they can be produced domestically, at scale, which is what America needs to stay at the forefront of semiconductor innovation.\"\n\nPerformance Gains and What Comes Next for AI Hardware\n\nIn early hardware tests, the prototype outperformed comparable 2D chips by about four times. The team's simulations suggest even bigger gains as the design grows taller with more stacked layers of memory and compute. With additional tiers, the models show up to a twelve fold improvement on real AI workloads, including workloads derived from Meta's open source LLaMA model.\n\nThe researchers also highlight a longer range payoff. They say the architecture offers a practical route to 100 to 1,000 fold improvements in energy delay product (EDP), a metric that combines speed and energy efficiency. By shortening how far data needs to travel and adding many more vertical routes for movement, the chip can increase throughput while reducing energy per operation, a combination that has been difficult to achieve with conventional flat designs.\n\nThe team says the importance of the work is not only about speed. By demonstrating that monolithic 3D chips can be made in the United States, they argue it provides a blueprint for a new period of domestic hardware innovation where the most advanced chips can be designed and manufactured on U.S. soil.\n\nThey also say the shift to vertical, monolithic 3D integration will require a new generation of engineers trained in these methods, similar to how the integrated circuit boom of the 1980s was fueled by students learning chip design and fabrication in U.S. labs. Through collaborations and funding efforts including the Microelectronics Commons California-Pacific-Northwest AI Hardware Hub (Northwest-AI-Hub), students and researchers are already being prepared to push American semiconductor innovation forward.\n\n\"Breakthroughs like this are of course about performance,\" said H.-S. Philip Wong, the Willard R. and Inez Kerr Bell Professor in the Stanford School of Engineering and principal investigator of the Northwest-AI-Hub. \"But they're also about capability. If we can build advanced 3D chips, we can innovate faster, respond faster, and shape the future of AI hardware.\"\n\nThis study took place at Stanford University School of Engineering, Carnegie Mellon University College of Engineering, the University of Pennsylvania School of Engineering and Applied Science, and the Massachusetts Institute of Technology, and all fabrications were completed at SkyWater Technology's Bloomington, Minnesota, Foundry. Support came from the Defense Advanced Research Projects Agency, the U.S. National Science Foundation Graduate Research Fellowship Program, Samsung, the Stanford Precourt Institute for Energy, the Stanford SystemX Alliance, the Department of War's Microelectronics Commons AI Hardware Hub, the U.S. Department of Energy, and the National Science Foundation's Future of Semiconductors Program (2425218).\n\nAdditional Stanford co-authors include Suhyeong Choi, Samuel Dayo, Andrew Bechdolt, Shengman Li, Dennis T. Rich, and R.H. Yang. Additional authors are from CMU and MIT.",
      "url": "https://www.sciencedaily.com/releases/2025/12/251223084857.htm",
      "source": "Latest Science News -- ScienceDaily",
      "published": "2025-12-24",
      "sentiment_score": 0.9,
      "reasoning": "The article reports a significant technological breakthrough in 3D chip design that dramatically improves AI hardware performance and energy efficiency. The innovation has broad implications for AI advancement and domestic semiconductor manufacturing, impacting society at large. The article provides detailed context on the technology, its advantages over traditional chips, and its potential future impact, fulfilling the criteria for inspiring good news.",
      "category": "Technology",
      "personality_title": "New 3D chip design speeds up AI and boosts U.S. chip manufacturing",
      "personality_presentation": "**Context** – Today's AI systems need chips that can quickly handle huge amounts of data. Most computer chips are flat and spread out, which causes slowdowns because data has to travel long paths between memory and computing parts. This problem, called the \"memory wall,\" limits how fast AI can work. \n\n**What happened** – Researchers from Stanford, Carnegie Mellon, University of Pennsylvania, and MIT teamed up with SkyWater Technology, a U.S. chip factory, to build a new kind of chip. Instead of being flat, this chip stacks memory and computing layers vertically, like floors in a tall building, with many tiny connections between them. This design moves data much faster inside the chip. They made a working prototype that runs about four times faster than similar flat chips and was made entirely in the United States.\n\n**Impact** – This new 3D chip design helps solve two big problems: the memory wall and the limit to how small chip parts can get. By stacking layers tightly and connecting them with many vertical links, the chip can handle more data quickly and use energy more efficiently. This could lead to chips that are up to twelve times faster on real AI tasks. Also, making the chip in a U.S. factory shows that advanced technology can be produced domestically, which is important for future innovation and independence in chip manufacturing.\n\n**What's next step** – The researchers plan to build chips with more stacked layers to increase speed and efficiency even further. They are also training new engineers in this 3D chip technology to keep advancing the field. This work could inspire new AI hardware designs and help the U.S. stay competitive in making the most advanced computer chips.\n\n**One-sentence takeaway** – A new 3D chip made in the U.S. stacks memory and computing layers to speed up AI processing and could transform future chip technology and manufacturing.",
      "personality_title_fr": "Une nouvelle puce 3D accélère l’IA et renforce la fabrication américaine",
      "personality_presentation_fr": "**Contexte** – Les systèmes d’IA actuels ont besoin de puces capables de gérer rapidement de grandes quantités de données. La plupart des puces sont plates, ce qui ralentit le transfert des données entre la mémoire et les unités de calcul. Ce problème, appelé « mur de mémoire », limite la vitesse de l’IA.\n\n**Ce qui s’est passé** – Des chercheurs de Stanford, Carnegie Mellon, l’Université de Pennsylvanie et du MIT ont collaboré avec SkyWater Technology, une usine américaine, pour créer une nouvelle puce. Au lieu d’être plate, cette puce empile des couches de mémoire et de calcul verticalement, comme les étages d’un immeuble, avec beaucoup de connexions fines entre elles. Ce design accélère fortement le transfert de données. Ils ont fabriqué un prototype fonctionnel, environ quatre fois plus rapide que les puces plates comparables, entièrement produit aux États-Unis.\n\n**Impact** – Ce nouveau design 3D résout deux grands problèmes : le mur de mémoire et la limite à la miniaturisation des composants. En empilant les couches et en multipliant les connexions verticales, la puce peut traiter plus de données rapidement et consommer moins d’énergie. Elle pourrait être jusqu’à douze fois plus rapide sur des tâches d’IA réelles. De plus, la fabrication aux États-Unis montre que cette technologie avancée peut être produite localement, ce qui est important pour l’innovation et l’indépendance.\n\n**Prochaine étape** – Les chercheurs veulent construire des puces avec encore plus de couches pour augmenter vitesse et efficacité. Ils forment aussi une nouvelle génération d’ingénieurs à cette technologie 3D pour continuer les progrès. Ce travail pourrait inspirer de nouveaux designs d’IA et aider les États-Unis à rester compétitifs dans la fabrication des puces les plus avancées.\n\n**Résumé en une phrase** – Une nouvelle puce 3D fabriquée aux États-Unis empile mémoire et calcul pour accélérer l’IA et pourrait transformer la technologie et la production des puces à l’avenir.",
      "personality_title_es": "Nuevo chip 3D acelera la IA y fortalece la fabricación de chips en EE.UU.",
      "personality_presentation_es": "**Contexto** – Los sistemas de IA actuales necesitan chips que puedan manejar grandes cantidades de datos rápidamente. La mayoría de los chips son planos, lo que causa lentitud porque los datos deben viajar largas distancias entre la memoria y las partes de cálculo. Este problema, llamado \"muro de memoria\", limita la velocidad de la IA.\n\n**Qué pasó** – Investigadores de Stanford, Carnegie Mellon, la Universidad de Pensilvania y el MIT trabajaron con SkyWater Technology, una fábrica de chips en EE.UU., para crear un nuevo tipo de chip. En lugar de ser plano, este chip apila capas de memoria y cálculo verticalmente, como pisos en un edificio alto, con muchas conexiones pequeñas entre ellos. Este diseño mueve los datos mucho más rápido dentro del chip. Hicieron un prototipo que funciona aproximadamente cuatro veces más rápido que chips planos similares y fue fabricado completamente en Estados Unidos.\n\n**Impacto** – Este nuevo diseño 3D ayuda a resolver dos grandes problemas: el muro de memoria y el límite para hacer las partes del chip más pequeñas. Al apilar capas y conectar muchas vías verticales, el chip puede manejar más datos rápidamente y usar energía de forma más eficiente. Esto podría llevar a chips hasta doce veces más rápidos en tareas reales de IA. Además, fabricar el chip en EE.UU. muestra que esta tecnología avanzada puede producirse localmente, lo que es importante para la innovación y la independencia.\n\n**Próximo paso** – Los investigadores planean construir chips con más capas apiladas para aumentar aún más la velocidad y eficiencia. También están formando a nuevos ingenieros en esta tecnología 3D para seguir avanzando. Este trabajo podría inspirar nuevos diseños de hardware para IA y ayudar a EE.UU. a mantenerse competitivo en la fabricación de chips avanzados.\n\n**Conclusión en una frase** – Un nuevo chip 3D hecho en EE.UU. apila memoria y cálculo para acelerar el procesamiento de IA y podría transformar la tecnología y fabricación futura de chips.",
      "image_url": "public/images/news_image_This-new-3D-chip-could-break-AIs-biggest-bottlenec.png",
      "image_prompt": "A warmly lit, intricately layered cityscape made of glowing circuit-like towers stacked vertically with bright connecting pathways resembling high-speed elevators weaving between floors, symbolizing a breakthrough 3D computer chip architecture that integrates memory and computing units closely to power advanced AI innovation."
    },
    {
      "title": "Why consciousness can’t be reduced to code",
      "summary": "The familiar fight between “mind as software” and “mind as biology” may be a false choice. This work proposes biological computationalism: the idea that brains compute, but not in the abstract, symbol-shuffling way we usually imagine. Instead, computation is inseparable from the brain’s physical structure, energy constraints, and continuous dynamics. That reframes consciousness as something that emerges from a special kind of computing matter, not from running the right program.",
      "content": "Today's arguments about consciousness often get stuck between two firm camps. One is computational functionalism, which says thinking can be fully described as abstract information processing. If a system has the right functional organization (regardless of the material it runs on), it should produce consciousness. The other is biological naturalism, which argues the opposite. It says consciousness cannot be separated from the special features of living brains and bodies because biology is not just a container for cognition, it is part of cognition itself. Both views capture real insights, but the deadlock suggests an important piece is still missing.\n\nIn our new paper, we propose a different approach: biological computationalism. The label is meant to be provocative, but also to sharpen the conversation. Our main argument is that the standard computational framework is broken, or at least poorly suited to how brains actually work. For a long time, it has been tempting to picture the mind as software running on neural hardware, with the brain \"computing\" in roughly the way a conventional computer does. But real brains are not von Neumann machines, and forcing that comparison leads to shaky metaphors and fragile explanations. If we want a serious account of how brains compute, and what it would take to build minds in other substrates, we first need a broader definition of what \"computation\" can be.\n\nBiological computation, as we describe it, has three core features.\n\nHybrid Brain Computation in Real Time\n\nFirst, biological computation is hybrid. It mixes discrete events with continuous dynamics. Neurons fire spikes, synapses release neurotransmitters, and networks shift through event-like states. At the same time, these events unfold within constantly changing physical conditions such as voltage fields, chemical gradients, ionic diffusion, and time-varying conductances. The brain is not purely digital, and it is not simply an analog machine either. Instead, it works as a multi-layered system where continuous processes influence discrete events, and discrete events reshape the continuous background, over and over, in an ongoing feedback loop.\n\nWhy Brain Computation Cannot Be Separated by Scale\n\nSecond, biological computation is scale-inseparable. In conventional computing, it is often possible to cleanly separate software from hardware, or a \"functional level\" from an \"implementation level.\" In the brain, that kind of separation breaks down. There is no neat dividing line where you can point to the algorithm on one side and the physical mechanism on the other. Cause and effect run across many scales at once, from ion channels to dendrites to circuits to whole-brain dynamics, and these levels do not behave like independent modules stacked in layers. In biological systems, changing the \"implementation\" changes the \"computation,\" because the two are tightly intertwined.\n\nMetabolism and Energy Constraints Shape Intelligence\n\nThird, biological computation is metabolically grounded. The brain operates under strict energy limits, and those limits shape its structure and function everywhere. This is not just an engineering detail. Energy constraints influence what the brain can represent, how it learns, which patterns remain stable, and how information is coordinated and routed. From this perspective, the tight coupling across levels is not accidental complexity. It is an energy optimization strategy that supports robust, flexible intelligence under severe metabolic limits.\n\nThe Algorithm Is the Substrate\n\nTaken together, these three features point to a conclusion that can feel strange if you are used to classical computing ideas. Computation in the brain is not abstract symbol manipulation. It is not simply about moving representations around according to formal rules while the physical medium is treated as \"mere implementation.\" In biological computation, the algorithm is the substrate. The physical organization does not just enable the computation, it is what the computation consists of. Brains do not merely run a program. They are a specific kind of physical process that computes by unfolding through time.\n\nWhat This Means for AI and Synthetic Minds\n\nThis view also exposes a limitation in how people often describe modern AI. Even powerful systems mostly simulate functions. They learn mappings from inputs to outputs, sometimes with impressive generalization, but the computation is still a digital procedure running on hardware built for a very different style of computing. Brains, by contrast, carry out computation in physical time. Continuous fields, ion flows, dendritic integration, local oscillatory coupling, and emergent electromagnetic interactions are not just biological \"details\" that can be ignored while extracting an abstract algorithm. In our view, these are the computational primitives of the system. They are the mechanisms that enable real-time integration, resilience, and adaptive control.\n\nNot Biology Only, But Biology Like Computation\n\nThis does not mean we think consciousness is somehow restricted to carbon-based life. We are not arguing \"biology or nothing.\" Our claim is narrower and more practical. If consciousness (or mind-like cognition) depends on this kind of computation, then it may require biological-style computational organization, even if it is built in new substrates. The key issue is not whether the substrate is literally biological, but whether the system instantiates the right kind of hybrid, scale-inseparable, metabolically (or more generally energetically) grounded computation.\n\nA Different Target for Building Conscious Machines\n\nThat reframes the goal for anyone trying to build synthetic minds. If brain computation cannot be separated from how it is physically realized, then scaling digital AI alone may not be enough. This is not because digital systems cannot become more capable, but because capability is only part of the puzzle. The deeper risk is that we may be optimizing the wrong thing by improving algorithms while leaving the underlying computational ontology unchanged. Biological computationalism suggests that building truly mind-like systems may require new kinds of physical machines whose computation is not organized as software on hardware, but spread across levels, dynamically linked, and shaped by the constraints of real-time physics and energy.\n\nSo if we want something like synthetic consciousness, the central question may not be, \"What algorithm should we run?\" It may be, \"What kind of physical system must exist for that algorithm to be inseparable from its own dynamics?\" What features are required, including hybrid event-field interactions, multi-scale coupling without clean interfaces, and energetic constraints that shape inference and learning, so that computation is not an abstract description layered on top but an intrinsic property of the system itself?\n\nThat is the shift biological computationalism calls for. It moves the challenge from finding the right program to finding the right kind of computing matter.",
      "url": "https://www.sciencedaily.com/releases/2025/12/251224032351.htm",
      "source": "Latest Science News -- ScienceDaily",
      "published": "2025-12-24",
      "sentiment_score": 0.8,
      "reasoning": "The article presents a significant conceptual breakthrough in understanding consciousness and brain computation, proposing a new framework—biological computationalism—that could reshape AI development and cognitive science. This has broad implications for technology and synthetic minds, offering a hopeful and inspiring perspective on future innovations in AI and consciousness research.",
      "category": "Technology",
      "personality_title": "New idea shows consciousness depends on how the brain physically works",
      "personality_presentation": "**Context** – Scientists often debate whether the mind is like software running on a computer or something unique to living brains. The usual views either see thinking as abstract information processing or as something tied closely to biology.\n\n**What happened** – Researchers proposed a new idea called biological computationalism. They say brains compute, but not like regular computers. Instead, brain computation mixes quick events with ongoing physical changes, works across many levels at once, and depends on energy limits. This means the brain’s physical makeup and energy use are part of how it thinks, not just a platform for running programs.\n\n**Impact** – This idea changes how we understand consciousness and brain function. It shows that thinking isn’t just about running the right program but involves the physical brain itself acting as a special kind of computer. It also suggests that current AI, which runs on digital hardware, might miss key parts of how minds work.\n\n**What's next step** – Scientists and engineers may need to explore new kinds of machines that compute like brains do—combining events, continuous changes, and energy use tightly together. This could lead to better AI that thinks more like living minds and deeper understanding of consciousness.\n\n**One-sentence takeaway** – Consciousness arises from the brain’s unique physical and energy-based computing, not just from running software-like programs.\n",
      "personality_title_fr": "Une nouvelle idée montre que la conscience dépend du fonctionnement physique du cerveau",
      "personality_presentation_fr": "**Contexte** – Les scientifiques débattent souvent pour savoir si l’esprit est comme un logiciel fonctionnant sur un ordinateur ou quelque chose d’unique aux cerveaux vivants. Les points de vue habituels considèrent la pensée soit comme un traitement abstrait de l’information, soit comme liée étroitement à la biologie.\n\n**Ce qui s’est passé** – Des chercheurs ont proposé une nouvelle idée appelée le computationnisme biologique. Ils expliquent que le cerveau calcule, mais pas comme un ordinateur ordinaire. Au lieu de cela, le calcul cérébral mélange des événements rapides avec des changements physiques continus, fonctionne à plusieurs niveaux en même temps, et dépend des limites énergétiques. Cela signifie que la structure physique du cerveau et son utilisation d’énergie font partie de la pensée, pas seulement une plateforme pour exécuter des programmes.\n\n**Impact** – Cette idée change notre compréhension de la conscience et du fonctionnement du cerveau. Elle montre que penser ne se résume pas à exécuter le bon programme, mais implique que le cerveau lui-même agit comme un type spécial d’ordinateur. Elle suggère aussi que l’IA actuelle, qui tourne sur du matériel numérique, pourrait manquer des éléments clés du fonctionnement des esprits.\n\n**Prochaine étape** – Les scientifiques et ingénieurs devront peut-être explorer de nouveaux types de machines qui calculent comme le cerveau, combinant événements, changements continus et utilisation d’énergie de manière étroite. Cela pourrait mener à une IA meilleure, plus proche des esprits vivants, et à une compréhension plus profonde de la conscience.\n\n**Résumé en une phrase** – La conscience vient du calcul unique du cerveau, basé sur sa structure physique et son énergie, et non seulement de l’exécution de programmes comme un logiciel.\n",
      "personality_title_es": "Nueva idea muestra que la conciencia depende de cómo funciona físicamente el cerebro",
      "personality_presentation_es": "**Contexto** – Los científicos suelen debatir si la mente es como un software que corre en una computadora o algo único de los cerebros vivos. Las opiniones comunes ven el pensamiento como procesamiento abstracto de información o algo estrechamente ligado a la biología.\n\n**Qué pasó** – Investigadores propusieron una nueva idea llamada computacionalismo biológico. Dicen que el cerebro calcula, pero no como una computadora común. En cambio, la computación cerebral mezcla eventos rápidos con cambios físicos continuos, funciona en muchos niveles al mismo tiempo y depende de límites energéticos. Esto significa que la estructura física del cerebro y su uso de energía son parte del pensamiento, no solo una plataforma para ejecutar programas.\n\n**Impacto** – Esta idea cambia cómo entendemos la conciencia y el funcionamiento del cerebro. Muestra que pensar no es solo ejecutar el programa correcto, sino que el cerebro mismo actúa como un tipo especial de computadora. También sugiere que la IA actual, que funciona en hardware digital, podría estar perdiendo partes clave del funcionamiento de la mente.\n\n**Próximo paso** – Científicos e ingenieros podrían necesitar explorar nuevos tipos de máquinas que computen como lo hace el cerebro, combinando eventos, cambios continuos y uso de energía estrechamente. Esto podría llevar a una mejor IA que piense más como mentes vivas y a un entendimiento más profundo de la conciencia.\n\n**Resumen en una frase** – La conciencia surge del cómputo único del cerebro basado en su estructura física y energía, no solo de ejecutar programas tipo software.\n",
      "image_url": "public/images/news_image_Why-consciousness-cant-be-reduced-to-code.png",
      "image_prompt": "An intricate, warm-toned painting of a glowing neural network seamlessly blending into flowing organic shapes that merge discrete, spark-like events with smooth, continuous waves, all interwoven with softly glowing energy currents symbolizing metabolism and multi-scale connectivity, set against a calm, abstract background evoking dynamic, living computation as an inseparable physical process."
    },
    {
      "title": "Physicists used 'dark photons' in an effort to rewrite physics in 2025",
      "summary": "A new theory of \"dark photons\" attempted to explain a centuries-old experiment in a new way this year, in an effort to change our understanding of the nature of light",
      "content": "A new theory of \"dark photons\" attempted to explain a centuries-old experiment in a new way this year, in an effort to change our understanding of the nature of light\n\nDark photons offer a new explanation for the double-slit experiment RUSSELL KIGHTLEY/SCIENCE PHOTO LIBRARY\n\nA core tenet of quantum theory was imperilled this year when a team of researchers put forward a radical new interpretation of an experiment about the nature of light.\n\nAt the centre of the new work was the double-slit experiment, which was first conducted in 1801 by physicist Thomas Young, who used it to confirm that light acts like a wave. Classically, something that is a particle can never also be a wave, and vice versa, but in the quantum realm, the two aren’t mutually exclusive. In fact, all quantum objects exhibit so-called wave-particle duality.\n\nFor decades, light seemed to be a prime example of this: experiments showed that it sometimes behaves as a particle called a photon and sometimes as a wave that produces effects like those that Young saw. But earlier this year, Celso Villas-Boas at the Federal University of São Carlos in Brazil and his colleagues proposed an interpretation of the double-slit experiment that only involves photons, effectively eliminating the need for the wavy part of light’s duality.\n\nAfter New Scientist reported on the study, the team behind it was contacted by many colleagues who were interested in the work, which has since been cited very widely, says Villas-Boas. One YouTube video about it has been viewed more than 700,000 times. “I was invited to deliver talks about this in Japan, Spain, here in Brazil, so many places,” he says.\n\nIn the classic double-slit experiment, an opaque barrier with two narrow, adjacent slits is placed between a screen and a source of light. The light passes through the slits and falls onto the screen, which consequently shows a pattern of bright and dark vertical stripes, known as classical interference. This is usually explained as a result of light waves spilling through the two slits and crashing into each other at the screen.\n\nSubscriber-only newsletter Sign up to Lost in Space-Time Untangle mind-bending physics, maths and the weirdness of reality with our monthly, special-guest-written newsletter. Sign up to newsletter\n\nThe researchers ditched this picture and turned to so-called dark states of photons, special quantum states that don’t light up the screen because they are unable to interact with any other particle. With these states explaining the dark stripes, there was now no need to invoke light waves.\n\nThis is a notable departure from the most common view of light in quantum physics. “Many professors were saying to me: ‘You are touching one of the most fundamental things in my life, I have been teaching interference by the book since the beginning, and now you’re saying that everything that I taught is wrong’,” says Villas-Boas. He says that some of his colleagues did accept the new view. Others remained if not outright sceptical, then cautiously intrigued, as New Scientist’s reporting bore out when the study first became public.\n\nAnd Villas-Boas has kept busy since, examining several new implications of photons’ dark states. For instance, his and his colleagues’ mathematical analysis revealed that thermal radiation, such as light coming from the sun or the stars, can have dark states that carry a significant portion of its energy, but as they don’t interact with other objects, that energy is, in some sense, hidden. This could be tested in experiments that place atoms in cavities where their interactions with light can be precisely monitored, says Villas-Boas.\n\nHe says that his team’s reinterpretation of interference also makes it possible to understand seemingly impossible phenomena, such as waves interfering even when they don’t directly overlap or interference between mechanical and electromagnetic waves. In either case, leaving behind the wave model in favour of bright and dark photon states opens up new possibilities. Villas-Boas can even imagine using some of these findings to build new types of light-driven switches or devices that are only transparent to certain types of light.\n\nIn his view, all this work relates to a fundamental truth about quantum physics: it is impossible to discuss quantum objects without describing how they interact with detectors and other measuring devices, including being dark. “This is not new, in my opinion. This is what quantum mechanics already says to us,” says Villas-Boas.",
      "url": "https://www.newscientist.com/article/2507992-physicists-used-dark-photons-in-an-effort-to-rewrite-physics-in-2025/?utm_campaign=RSS%7CNSNS&utm_source=NSNS&utm_medium=RSS&utm_content=home",
      "source": "New Scientist - Home",
      "published": "2025-12-24",
      "sentiment_score": 0.8,
      "reasoning": "The article reports a significant scientific breakthrough in quantum physics with the new theory of 'dark photons' offering a novel explanation for the double-slit experiment. This challenges a fundamental concept in physics, potentially reshaping our understanding of light and quantum mechanics. The discovery has broad implications for science and technology, including new experimental possibilities and future applications in light-driven devices, thus having a meaningful and inspiring impact on society's knowledge and technological progress.",
      "category": "Technology",
      "personality_title": "New theory uses 'dark photons' to rethink how light works",
      "personality_presentation": "**Context** – For over 200 years, scientists have studied light using the double-slit experiment. This test showed that light behaves both like a wave and a particle, a strange idea called wave-particle duality that is a key part of quantum physics.\n\n**What happened** – In 2025, a team led by Celso Villas-Boas from Brazil suggested a new way to understand this experiment. Instead of light acting like waves, they said light is made of photons that can be in 'dark states.' These dark photons don’t shine or interact with other particles, explaining the dark stripes seen in the experiment without needing waves.\n\n**Impact** – This is important because it challenges a basic idea about light that many scientists have accepted for a long time. If light does not need to behave like a wave, it could change how we think about many quantum effects. The idea has already sparked interest worldwide, with many researchers and students discussing and testing this new view.\n\n**What's next step** – The team plans to test their theory further by studying how dark photons might carry hidden energy in sunlight and other sources. They also want to explore new technologies, like light switches or devices that only let certain light pass through, based on these dark states.\n\n**One-sentence takeaway** – A new theory using dark photons offers a fresh explanation for light’s behavior, potentially changing fundamental physics and opening doors to new technologies.",
      "personality_title_fr": "Une nouvelle théorie utilise les 'photons sombres' pour repenser la lumière",
      "personality_presentation_fr": "**Contexte** – Depuis plus de 200 ans, les scientifiques étudient la lumière grâce à l'expérience de la double fente. Cette expérience a montré que la lumière se comporte à la fois comme une onde et comme une particule, une idée étrange appelée dualité onde-particule, essentielle en physique quantique.\n\n**Ce qui s'est passé** – En 2025, une équipe dirigée par Celso Villas-Boas au Brésil a proposé une nouvelle façon de comprendre cette expérience. Au lieu que la lumière agisse comme une onde, ils ont expliqué que la lumière est composée de photons pouvant être dans des « états sombres ». Ces photons sombres ne brillent pas et n'interagissent pas avec d'autres particules, ce qui explique les bandes sombres de l'expérience sans avoir besoin d'ondes.\n\n**Impact** – Cela est important car cela remet en cause une idée de base sur la lumière que beaucoup de scientifiques acceptent depuis longtemps. Si la lumière ne doit pas forcément se comporter comme une onde, cela pourrait changer notre compréhension de nombreux effets quantiques. Cette idée a déjà suscité un grand intérêt dans le monde entier.\n\n**Prochaines étapes** – L'équipe prévoit de tester davantage leur théorie en étudiant comment les photons sombres pourraient transporter de l'énergie cachée dans la lumière du soleil et d'autres sources. Ils veulent aussi explorer de nouvelles technologies, comme des interrupteurs lumineux ou des dispositifs qui laissent passer seulement certains types de lumière.\n\n**Conclusion en une phrase** – Une nouvelle théorie utilisant les photons sombres offre une explication inédite du comportement de la lumière, pouvant changer la physique fondamentale et ouvrir la voie à de nouvelles technologies.",
      "personality_title_es": "Nueva teoría usa 'fotones oscuros' para replantear cómo funciona la luz",
      "personality_presentation_es": "**Contexto** – Durante más de 200 años, los científicos han estudiado la luz con el experimento de la doble rendija. Este experimento mostró que la luz se comporta como onda y partícula a la vez, una idea extraña llamada dualidad onda-partícula, clave en la física cuántica.\n\n**Qué pasó** – En 2025, un equipo liderado por Celso Villas-Boas de Brasil propuso una nueva forma de entender este experimento. En lugar de que la luz actúe como ondas, dijeron que la luz está hecha de fotones que pueden estar en 'estados oscuros'. Estos fotones oscuros no brillan ni interactúan con otras partículas, explicando las franjas oscuras sin necesidad de ondas.\n\n**Impacto** – Esto es importante porque desafía una idea básica sobre la luz que muchos científicos han aceptado durante mucho tiempo. Si la luz no necesita comportarse como onda, podría cambiar cómo entendemos muchos efectos cuánticos. La idea ya ha generado interés mundial.\n\n**Próximos pasos** – El equipo planea probar su teoría estudiando cómo los fotones oscuros podrían llevar energía oculta en la luz del sol y otras fuentes. También quieren explorar nuevas tecnologías, como interruptores de luz o dispositivos que solo dejan pasar cierto tipo de luz.\n\n**Conclusión en una frase** – Una nueva teoría con fotones oscuros ofrece una explicación fresca del comportamiento de la luz, que podría cambiar la física fundamental y abrir camino a nuevas tecnologías.",
      "image_url": "public/images/news_image_Physicists-used-dark-photons-in-an-effort-to-rewri.png",
      "image_prompt": "A warm, detailed painting of an abstract double-slit barrier made of softly glowing, translucent panels with streams of luminous photons passing through, some depicted as bright glowing orbs and others as subtle dark or shadowy orbs representing dark photon states, all converging onto a gently illuminated screen showing alternating bright and dark vertical stripes, set against a simple, natural-toned background evoking a sense of quantum mystery and discovery."
    },
    {
      "title": "AI supercharges scientific output while quality slips",
      "summary": "AI writing tools are supercharging scientific productivity, with researchers posting up to 50% more papers after adopting them. The biggest beneficiaries are scientists who don’t speak English as a first language, potentially shifting global centers of research power. But there’s a downside: many AI-polished papers fail to deliver real scientific value. This growing gap between slick writing and meaningful results is complicating peer review, funding decisions, and research oversight.",
      "content": "After ChatGPT became widely available in late 2022, many researchers started telling colleagues they could get more done with these new artificial intelligence tools. At the same time, journal editors reported a surge of smoothly written submissions that did not seem to add much scientific value.\n\nA new Cornell study suggests those informal reports point to a broader change in how scientists are preparing manuscripts. The researchers found that large language models (LLMs) such as ChatGPT can increase paper output, with especially strong benefits for scientists who are not native English speakers. But the growing volume of AI written text is also making it harder for key decision makers to tell meaningful work apart from low value content.\n\n\"It is a very widespread pattern, across different fields of science -- from physical and computer sciences to biological and social sciences,\" said Yian Yin, assistant professor of information science in the Cornell Ann S. Bowers College of Computing and Information Science. \"There's a big shift in our current ecosystem that warrants a very serious look, especially for those who make decisions about what science we should support and fund.\"\n\nThe findings appear in a paper titled \"Scientific Production in the Era of Large Language Models,\" published Dec. 18 in Science.\n\nHow the Cornell Team Measured AI Use in Research Papers\n\nTo examine how LLMs are influencing scientific publishing, Yin's team compiled more than 2 million papers posted from January 2018 through June 2024 across three major preprint platforms. Those sites are arXiv, bioRxiv and Social Science Research Network (SSRN). Together, they represent the physical sciences, life sciences and social sciences, and they host studies that have not yet been through peer review.\n\nThe researchers used papers posted before 2023 that were presumed to be written by humans and compared them with AI generated text. From that comparison, they built a model designed to flag papers that were likely written with help from LLMs. Using this detector, they estimated which authors were probably using LLMs for writing, tracked how many papers those scientists posted before and after adopting the tools, and then checked whether the papers were later accepted by scientific journals.\n\nBig Productivity Gains, Especially for Non Native English Speakers\n\nThe results showed a clear productivity jump linked to apparent LLM use. On arXiv, scientists flagged as using LLMs posted roughly one third more papers than those who did not appear to use AI. On bioRxiv and SSRN, the increase exceeded 50%.\n\nThe boost was largest for scientists who write in English as a second language and face extra hurdles when communicating technical work in a foreign language. For example, researchers affiliated with Asian institutions posted between 43.0% and 89.3% more papers after the detector suggested they began using LLMs, compared with similar researchers who did not appear to adopt the technology, depending on the preprint site. Yin expects the advantage could eventually shift global patterns of scientific productivity toward regions that have been held back by the language barrier.\n\nAI Search May Broaden What Scientists Cite\n\nThe study also pointed to a potential benefit during literature searches and citation building. When researchers look for related work to cite, Bing Chat -- described as the first widely adopted AI powered search tool -- performed better at surfacing newer papers and relevant books than traditional search tools. Traditional tools, by contrast, were more likely to return older and more heavily cited sources.\n\n\"People using LLMs are connecting to more diverse knowledge, which might be driving more creative ideas,\" said first author Keigo Kusumegi, a doctoral student in the field of information science. He plans future research to test whether AI use is associated with more innovative and interdisciplinary science.\n\nA New Problem for Peer Review and Research Evaluation\n\nEven as LLMs help individuals produce more manuscripts, the same tools can make it harder for others to judge what is truly strong science. In human written papers, clearer yet more complex writing, including longer sentences and bigger words, has often been a useful signal of higher quality research. Across arXiv, bioRxiv and SSRN, papers likely written by humans that scored highly on a writing complexity test were also the most likely to be accepted by journals.\n\nThat pattern looked different for papers likely written with LLM assistance. Even when those AI flagged papers scored high on writing complexity, they were less likely to be accepted by journals. The researchers interpret this as a sign that polished language may no longer reliably reflect scientific value, and that reviewers may be rejecting some of these papers despite strong sounding writing.\n\nYin said this gap between writing quality and research quality could have serious consequences. Editors and reviewers may struggle more to identify the most valuable submissions, while universities and funding agencies may find that raw publication counts no longer reflect scientific contribution.\n\nWhat Comes Next for Research on Generative AI\n\nThe researchers emphasize that these findings are observational. As a next step, they hope to test cause and effect using approaches such as controlled experiments, including designs where some scientists are randomly assigned to use LLMs and others are not.\n\nYin is also organizing a symposium on the Ithaca campus scheduled for March 3-5, 2026. The event will explore how generative AI is changing research and how scientists and policymakers can guide those changes.\n\nAs AI becomes more common for writing, coding and even generating ideas, Yin expects its influence to expand, effectively turning these systems into a kind of co scientist. He argues that policymakers should update rules to keep pace with the fast moving technology.\n\n\"Already now, the question is not, have you used AI? The question is, how exactly have you used AI and whether it's helpful or not.\"\n\nStudy Authors and Funding\n\nCo authors include Xinyu Yang, a doctoral student in the field of computer science; Paul Ginsparg, professor of information science in Cornell Bowers and of physics in the College of Arts and Sciences, and founder of arXiv; and Mathijs de Vaan and Toby Stuart of the University of California, Berkeley.\n\nThe research was supported by the National Science Foundation.",
      "url": "https://www.sciencedaily.com/releases/2025/12/251224032347.htm",
      "source": "Latest Science News -- ScienceDaily",
      "published": "2025-12-24",
      "sentiment_score": 0.75,
      "reasoning": "The article reports a significant real-world impact of AI tools boosting scientific productivity, especially benefiting non-native English speakers, which could shift global research dynamics. It also highlights challenges in research evaluation, providing a substantive, focused study with broad implications for the scientific community and policy makers.",
      "category": "Technology",
      "personality_title": "AI tools boost scientific papers, especially for non-native English speakers",
      "personality_presentation": "**Context** – Since late 2022, many scientists have started using artificial intelligence (AI) writing tools like ChatGPT to help write research papers. These tools can create smooth and clear text, changing how scientific papers are made and shared.\n\n**What happened** – A study by Cornell University looked at over 2 million research papers from 2018 to 2024. They found that scientists using AI tools published about 30% to 50% more papers than those who did not. The biggest increase was seen among researchers who don’t speak English as their first language, especially those from Asian countries. The study also found that AI-powered search tools help scientists find newer and more diverse research to cite.\n\n**Impact** – This shows that AI can help scientists write more papers and share their work faster, especially helping those who struggle with English. However, the study also found a problem: papers written with AI often look well-written but sometimes lack strong scientific value. This makes it harder for journal editors and reviewers to decide which research is truly important. The usual signs of good science, like complex writing, don’t always work anymore.\n\n**What's next step** – The researchers want to do experiments to better understand how AI affects science quality. They are also planning a meeting in 2026 to discuss how AI is changing research and what rules might be needed. Policymakers will need to update guidelines to handle AI’s growing role in science.\n\n**One-sentence takeaway** – AI writing tools are helping scientists publish more papers, especially non-native English speakers, but also create challenges in judging the true quality of research.",
      "personality_title_fr": "Les outils d'IA augmentent la production scientifique, surtout pour les non-anglophones",
      "personality_presentation_fr": "**Contexte** – Depuis fin 2022, de nombreux scientifiques utilisent des outils d'intelligence artificielle (IA) comme ChatGPT pour aider à rédiger leurs articles de recherche. Ces outils produisent des textes clairs et fluides, changeant la façon dont les articles sont écrits et partagés.\n\n**Ce qui s'est passé** – Une étude de l'université Cornell a analysé plus de 2 millions d'articles publiés entre 2018 et 2024. Elle a montré que les chercheurs utilisant l'IA publiaient entre 30 % et 50 % d'articles en plus que ceux qui ne l'utilisaient pas. L'augmentation la plus forte concernait les chercheurs dont l'anglais n'est pas la langue maternelle, notamment en Asie. L'étude a aussi montré que les outils de recherche avec IA aident à trouver des travaux plus récents et variés à citer.\n\n**Impact** – Cela montre que l'IA aide les scientifiques à publier plus rapidement, surtout ceux qui ont des difficultés avec l'anglais. Mais un problème apparaît : les articles écrits avec l'IA sont souvent bien rédigés, mais manquent parfois de valeur scientifique réelle. Cela complique le travail des éditeurs et relecteurs qui doivent décider quels travaux sont importants. Les signes habituels de qualité scientifique, comme un style d'écriture complexe, ne sont plus toujours fiables.\n\n**Prochaine étape** – Les chercheurs veulent réaliser des expériences pour mieux comprendre l'impact de l'IA sur la qualité scientifique. Ils prévoient aussi un colloque en 2026 pour discuter des changements liés à l'IA et des règles à adapter. Les décideurs devront mettre à jour les règles pour gérer ce rôle grandissant de l'IA en science.\n\n**Résumé en une phrase** – Les outils d’écriture par IA aident les chercheurs à publier davantage, surtout les non-anglophones, mais rendent plus difficile l’évaluation de la qualité réelle des recherches.",
      "personality_title_es": "Las herramientas de IA aumentan la producción científica, especialmente para quienes no son angloparlantes",
      "personality_presentation_es": "**Contexto** – Desde finales de 2022, muchos científicos han comenzado a usar herramientas de inteligencia artificial (IA) como ChatGPT para ayudar a escribir sus artículos de investigación. Estas herramientas crean textos claros y bien escritos, cambiando la forma en que se preparan y comparten los trabajos científicos.\n\n**Qué pasó** – Un estudio de la Universidad de Cornell analizó más de 2 millones de artículos publicados entre 2018 y 2024. Descubrieron que los científicos que usan IA publican entre un 30 % y un 50 % más de artículos que quienes no la usan. El mayor aumento se vio en investigadores cuyo primer idioma no es el inglés, especialmente en Asia. El estudio también encontró que las búsquedas con IA ayudan a encontrar trabajos más recientes y variados para citar.\n\n**Impacto** – Esto muestra que la IA ayuda a los científicos a publicar más rápido, en especial a quienes tienen dificultades con el inglés. Pero también hay un problema: los artículos escritos con IA suelen estar bien redactados pero a veces carecen de valor científico real. Esto dificulta que los editores y revisores decidan qué investigaciones son importantes. Las señales habituales de calidad, como un lenguaje complejo, ya no siempre funcionan.\n\n**Próximo paso** – Los investigadores quieren hacer experimentos para entender mejor cómo la IA afecta la calidad científica. También planean un encuentro en 2026 para discutir cómo la IA está cambiando la investigación y qué reglas se deben actualizar. Los responsables deberán adaptar las normas para manejar el papel creciente de la IA en la ciencia.\n\n**Resumen en una frase** – Las herramientas de escritura con IA ayudan a los científicos a publicar más, especialmente a los no angloparlantes, pero complican evaluar la calidad real de la investigación.",
      "image_url": "public/images/news_image_AI-supercharges-scientific-output-while-quality-sl.png",
      "image_prompt": "A detailed, warm painting of an open book glowing with flowing streams of colorful, interconnected digital text and symbols rising from its pages, merging into a network of glowing nodes shaped like scientific icons (microscopes, atoms, and charts) that gently illuminate a diverse globe beneath, symbolizing AI-enhanced scientific collaboration and the complex balance between productivity and quality."
    }
  ]
}